{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting ensemble code of Kamlesh 2023\n",
    "*Requires cuda\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/9brv6cdn1b97f82knqyjcb4m0000gn/T/ipykernel_19220/2989364366.py:12: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n",
      "Setting seed 0.\n",
      "dPLHBV_dyn\n",
      "SACSMA_snow\n",
      "marrmot_PRMS\n"
     ]
    }
   ],
   "source": [
    "from config.read_configurations import config_hbv as hbvArgs\n",
    "from config.read_configurations import config_prms as prmsArgs\n",
    "from config.read_configurations import config_sacsma as sacsmaArgs\n",
    "from config.read_configurations import config_sacsma_snow as sacsmaSnowArgs\n",
    "from config.read_configurations import config_hbv_hydrodl as hbvhyArgs_d\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.stats\n",
    "# from post import plot\n",
    "\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import create_output_dirs\n",
    "from MODELS.loss_functions.get_loss_function import get_lossFun\n",
    "from MODELS.test_dp_HBV_dynamic import test_dp_hbv\n",
    "from core.data_processing.data_loading import loadData\n",
    "from core.data_processing.normalization import transNorm\n",
    "from core.data_processing.model import (\n",
    "    take_sample_test,\n",
    "    converting_flow_from_ft3_per_sec_to_mm_per_day\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "## GPU setting\n",
    "# which GPU to use when having multiple\n",
    "# traingpuid = 0\n",
    "# torch.cuda.set_device(traingpuid)\n",
    "\n",
    "\n",
    "\n",
    "# fix the random seeds for reproducibility\n",
    "def randomseed_config(seed):\n",
    "    if seed == None:  # args['randomseed'] is None:\n",
    "        # generate random seed\n",
    "        randomseed = int(np.random.uniform(low=0, high=1e6))\n",
    "        print(\"random seed updated!\")\n",
    "    else:\n",
    "        print(\"Setting seed 0.\")\n",
    "        # randomseed = args['randomseed']\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "randomseed_config(0)\n",
    "\n",
    "\n",
    "\n",
    "# Set path to `hydro_multimodel_results` directory.\n",
    "if platform.system() == 'Darwin':\n",
    "    # For mac os\n",
    "    out_dir = '/Users/leoglonz/Desktop/water/data/model_runs/hydro_multimodel_results'\n",
    "    # Some operations are not yet working with MPS, so we might need to set some environment variables to use CPU fall instead\n",
    "    # %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "elif platform.system() == 'Windows':\n",
    "    # For windows\n",
    "    out_dir = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n",
    "\n",
    "elif platform.system() == 'Linux':\n",
    "    # For Colab\n",
    "    out_dir = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results'\n",
    "\n",
    "else:\n",
    "    raise ValueError('Unsupported operating system.')\n",
    "\n",
    "\n",
    "##-----## Multi-model Parameters ##-----##\n",
    "##--------------------------------------##\n",
    "# Setting dictionaries to separately manage each diff model's attributes.\n",
    "models = {'dPLHBV_dyn': None,'SACSMA_snow':None, 'marrmot_PRMS':None}  # 'HBV':None, 'hbvhy': None, 'SACSMA_snow':None, 'SACSMA':None,\n",
    "args_list = {'dPLHBV_dyn': hbvhyArgs_d,'SACSMA_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\n",
    "ENSEMBLE_TYPE = 'max'  # 'median', 'avg', 'max', 'softmax'\n",
    "\n",
    "# Load test observations and predictions from a prior run.\n",
    "pred_path = os.path.join(out_dir, 'multimodels', '671_sites_dp', 'hydro_preds_obs', 'preds_671_dPLHBVd_SACSMASnow_PRMS.npy')\n",
    "obs_path = os.path.join(out_dir, 'multimodels', '671_sites_dp', 'hydro_preds_obs', 'obs_671_dPLHBVd_SACSMASnow_PRMS.npy')\n",
    "preds = np.load(pred_path, allow_pickle=True).item()\n",
    "obs = np.load(obs_path, allow_pickle=True).item()\n",
    "\n",
    "model_output = preds\n",
    "y_obs = obs\n",
    "\n",
    "# Initialize\n",
    "flow_preds = []\n",
    "flow_obs = None\n",
    "obs_trig = False\n",
    "\n",
    "# Concatenate individual model predictions, and observation data.\n",
    "for i, mod in enumerate(args_list):\n",
    "    args = args_list[mod]\n",
    "    mod_out = model_output[mod]\n",
    "    y_ob = y_obs[mod]\n",
    "\n",
    "    print(mod)\n",
    "\n",
    "    if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n",
    "        # Hydro models are tested in batches, so we concatenate them and select\n",
    "        # the desired flow.\n",
    "        # Note: modified HBV already has this preparation done during testing.\n",
    "\n",
    "        # Get flow predictions and swap axes to get shape [basins, days]\n",
    "        pred = np.swapaxes(torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1).squeeze().numpy(), 0, 1)\n",
    "\n",
    "        if obs_trig == False:\n",
    "            # dPLHBV uses GAGES while the other hydro models use CAMELS data. This means small\n",
    "            # e-5 variation in observation data between the two. This is averaged if both models\n",
    "            # are used, but to avoid double-counting data from multiply hydro models, use a trigger.\n",
    "            obs = np.swapaxes(y_ob[:, :, args[\"target\"].index(\"00060_Mean\")].numpy(), 0, 1)\n",
    "            obs_trig = True\n",
    "            dup = False\n",
    "        else:\n",
    "            dup = True\n",
    "\n",
    "    elif mod in ['dPLHBV_dyn']:\n",
    "        pred = mod_out[:,:,0][:,365:] # Set dim2 = 0 to get streamflow Qr\n",
    "        obs = y_ob.squeeze()[:,365:]\n",
    "        dup = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type in `models`.\")\n",
    "\n",
    "    if i == 0:\n",
    "        tmp_pred = pred\n",
    "        tmp_obs = obs\n",
    "    elif i == 1:\n",
    "        tmp_pred = np.stack((tmp_pred, pred), axis=2)\n",
    "        if not dup:\n",
    "            # Avoid double-counting GAGES obs.\n",
    "            tmp_obs = np.stack((tmp_obs, obs), axis=2)\n",
    "    else:\n",
    "        # Combine outputs of >3 models.\n",
    "        tmp_pred = np.concatenate((tmp_pred,np.expand_dims(pred, 2)), axis=2)\n",
    "        if not dup:\n",
    "            # Avoid double-counting GAGES obs.\n",
    "            tmp_obs = np.concatenate((tmp_obs,np.expand_dims(obs, 2)), axis=2)\n",
    "\n",
    "preds = tmp_pred\n",
    "obs = tmp_obs\n",
    "\n",
    "# Merge observation data.\n",
    "if len(obs.shape) == 3:\n",
    "    comp_obs = np.mean(obs, axis = 2)\n",
    "elif len(obs.shape) == 2:\n",
    "    comp_obs = obs\n",
    "else:\n",
    "    raise ValueError(\"Error reading prediction data: incorrect formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model dependencies:\n",
    "##########################\n",
    "\n",
    "\n",
    "#### DESTINATION: train.py\n",
    "\n",
    "\n",
    "import time\n",
    "from hydroDL.model import rnn, cnn, crit\n",
    "\n",
    "\n",
    "\n",
    "def randomIndex(ngrid, nt, dimSubset, bufftime=0):\n",
    "    batchSize, rho = dimSubset\n",
    "    iGrid = np.random.randint(0, ngrid, [batchSize])\n",
    "    iT = np.random.randint(0+bufftime, nt - rho, [batchSize])\n",
    "    return iGrid, iT\n",
    "\n",
    "\n",
    "def selectSubset(x, iGrid, iT, rho, *, c=None, tupleOut=False, LCopt=False, bufftime=0):\n",
    "    nx = x.shape[-1]\n",
    "    nt = x.shape[1]\n",
    "    if x.shape[0] == len(iGrid):   #hack\n",
    "        iGrid = np.arange(0,len(iGrid))  # hack\n",
    "    if nt <= rho:\n",
    "        iT.fill(0)\n",
    "\n",
    "    batchSize = iGrid.shape[0]\n",
    "    if iT is not None:\n",
    "        # batchSize = iGrid.shape[0]\n",
    "        xTensor = torch.zeros([rho+bufftime, batchSize, nx], requires_grad=False)\n",
    "        for k in range(batchSize):\n",
    "            temp = x[iGrid[k]:iGrid[k] + 1, np.arange(iT[k]-bufftime, iT[k] + rho), :]\n",
    "            xTensor[:, k:k + 1, :] = torch.from_numpy(np.swapaxes(temp, 1, 0))\n",
    "    else:\n",
    "        if LCopt is True:\n",
    "            # used for local calibration kernel: FDC, SMAP...\n",
    "            if len(x.shape) == 2:\n",
    "                # Used for local calibration kernel as FDC\n",
    "                # x = Ngrid * Ntime\n",
    "                xTensor = torch.from_numpy(x[iGrid, :]).float()\n",
    "            elif len(x.shape) == 3:\n",
    "                # used for LC-SMAP x=Ngrid*Ntime*Nvar\n",
    "                xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 2)).float()\n",
    "        else:\n",
    "            # Used for rho equal to the whole length of time series\n",
    "            xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 0)).float()\n",
    "            rho = xTensor.shape[0]\n",
    "    if c is not None:\n",
    "        nc = c.shape[-1]\n",
    "        temp = np.repeat(\n",
    "            np.reshape(c[iGrid, :], [batchSize, 1, nc]), rho+bufftime, axis=1)\n",
    "        cTensor = torch.from_numpy(np.swapaxes(temp, 1, 0)).float()\n",
    "\n",
    "        if (tupleOut):\n",
    "            if torch.cuda.is_available():\n",
    "                xTensor = xTensor.cuda()\n",
    "                cTensor = cTensor.cuda()\n",
    "            out = (xTensor, cTensor)\n",
    "        else:\n",
    "            out = torch.cat((xTensor, cTensor), 2)\n",
    "    else:\n",
    "        out = xTensor\n",
    "\n",
    "    if torch.cuda.is_available() and type(out) is not tuple:\n",
    "        out = out.cuda()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "##########################\n",
    "\n",
    "\n",
    "#### DESTINATION: rnn.py\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from hydroDL.model.dropout import DropMask, createMask\n",
    "\n",
    "\n",
    "\n",
    "class CudnnLstm(torch.nn.Module):\n",
    "    def __init__(self, *, inputSize, hiddenSize, dr=0.5, drMethod='drW',\n",
    "                 gpu=0):\n",
    "        super(CudnnLstm, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.dr = dr\n",
    "        self.w_ih = Parameter(torch.Tensor(hiddenSize * 4, inputSize))\n",
    "        self.w_hh = Parameter(torch.Tensor(hiddenSize * 4, hiddenSize))\n",
    "        self.b_ih = Parameter(torch.Tensor(hiddenSize * 4))\n",
    "        self.b_hh = Parameter(torch.Tensor(hiddenSize * 4))\n",
    "        self._all_weights = [['w_ih', 'w_hh', 'b_ih', 'b_hh']]\n",
    "        self.cuda()\n",
    "\n",
    "        self.reset_mask()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(CudnnLstm, self)._apply(fn)\n",
    "        return ret\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        super(CudnnLstm, self).__setstate__(d)\n",
    "        self.__dict__.setdefault('_data_ptrs', [])\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        self._all_weights = [['w_ih', 'w_hh', 'b_ih', 'b_hh']]\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.maskW_ih = createMask(self.w_ih, self.dr)\n",
    "        self.maskW_hh = createMask(self.w_hh, self.dr)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hiddenSize)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, hx=None, cx=None, doDropMC=False, dropoutFalse=False):\n",
    "        # dropoutFalse: it will ensure doDrop is false, unless doDropMC is true\n",
    "        if dropoutFalse and (not doDropMC):\n",
    "            doDrop = False\n",
    "        elif self.dr > 0 and (doDropMC is True or self.training is True):\n",
    "            doDrop = True\n",
    "        else:\n",
    "            doDrop = False\n",
    "\n",
    "        batchSize = input.size(1)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(\n",
    "                1, batchSize, self.hiddenSize, requires_grad=False)\n",
    "        if cx is None:\n",
    "            cx = input.new_zeros(\n",
    "                1, batchSize, self.hiddenSize, requires_grad=False)\n",
    "\n",
    "        # cuDNN backend - disabled flat weight\n",
    "        # handle = torch.backends.cudnn.get_handle()\n",
    "        if doDrop is True:\n",
    "            self.reset_mask()\n",
    "            weight = [\n",
    "                DropMask.apply(self.w_ih, self.maskW_ih, True),\n",
    "                DropMask.apply(self.w_hh, self.maskW_hh, True), self.b_ih,\n",
    "                self.b_hh\n",
    "            ]\n",
    "        else:\n",
    "            weight = [self.w_ih, self.w_hh, self.b_ih, self.b_hh]\n",
    "\n",
    "        # output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "            # input, weight, 4, None, hx, cx, torch.backends.cudnn.CUDNN_LSTM,\n",
    "            # self.hiddenSize, 1, False, 0, self.training, False, (), None)\n",
    "        if torch.__version__ < \"1.8\":\n",
    "            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "                input, weight, 4, None, hx, cx, 2,  # 2 means LSTM\n",
    "                self.hiddenSize, 1, False, 0, self.training, False, (), None)\n",
    "        else:\n",
    "            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "                input, weight, 4, None, hx, cx, 2,  # 2 means LSTM\n",
    "                self.hiddenSize, 0, 1, False, 0, self.training, False, (), None)\n",
    "        return output, (hy, cy)\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights]\n",
    "                for weights in self._all_weights]\n",
    "\n",
    "\n",
    "class CudnnLstmModel(torch.nn.Module):\n",
    "    def __init__(self, *, nx, ny, hiddenSize, dr=0.5):\n",
    "        super(CudnnLstmModel, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.ct = 0\n",
    "        self.nLayer = 1\n",
    "        self.linearIn = torch.nn.Linear(nx, hiddenSize).cuda()\n",
    "        self.lstm = CudnnLstm(\n",
    "            inputSize=hiddenSize, hiddenSize=hiddenSize, dr=dr)\n",
    "        self.linearOut = torch.nn.Linear(hiddenSize, ny)\n",
    "        self.gpu = 1\n",
    "        # self.drtest = torch.nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x, doDropMC=False, dropoutFalse=False):\n",
    "        x0 = F.relu(self.linearIn(x))\n",
    "        outLSTM, (hn, cn) = self.lstm(x0, doDropMC=doDropMC, dropoutFalse=dropoutFalse)\n",
    "        # outLSTMdr = self.drtest(outLSTM)\n",
    "        out = self.linearOut(outLSTM)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RangeBoundLoss(nn.Module):\n",
    "    \"\"\"limit parameters from going out of range\"\"\"\n",
    "    def __init__(self, lb, ub):\n",
    "        super(RangeBoundLoss, self).__init__()\n",
    "        self.lb = torch.tensor(lb).cuda()\n",
    "        self.ub = torch.tensor(ub).cuda()\n",
    "        # self.factor = torch.tensor(factor).cuda()\n",
    "\n",
    "    def forward(self, params, factor):\n",
    "        factor = torch.tensor(factor).cuda()\n",
    "        loss = 0\n",
    "        for i in range(len(params)):\n",
    "            lb = self.lb[i]\n",
    "            ub = self.ub[i]\n",
    "            upper_bound_loss = factor * torch.relu(params[i] - ub).mean()\n",
    "            lower_bound_loss = factor * torch.relu(lb - params[i]).mean()\n",
    "            loss = loss + upper_bound_loss + lower_bound_loss\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper:\n",
    "##########################\n",
    "\n",
    "\n",
    "#### DESTINATION: rnn.py\n",
    "\n",
    "# Modified prcp_weights\n",
    "\n",
    "class EnsembleWeights(torch.nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, prcp_datatypes=1):\n",
    "        super(EnsembleWeights, self).__init__()\n",
    "        self.ninv = ninv\n",
    "        self.prcp_datatypes = prcp_datatypes\n",
    "\n",
    "        self.ntp = prcp_datatypes*3\n",
    "        self.hiddeninv = hiddeninv\n",
    "\n",
    "        self.lstminv = CudnnLstmModel(\n",
    "            nx=ninv, ny=self.ntp, hiddenSize=hiddeninv, dr=drinv).cuda()\n",
    "\n",
    "        # Adjust the range for acceptable sum of weights for loss.\n",
    "        # Potentially worth testing different combinations.\n",
    "        lb_prcp = [0.95]\n",
    "        ub_prcp = [1.05]\n",
    "        self.RangeBoundLoss = RangeBoundLoss(lb=lb_prcp, ub=ub_prcp)\n",
    "\n",
    "    def forward(self, x, prcp_loss_factor):\n",
    "        # x.requires_grad = True\n",
    "\n",
    "        wghts = self.lstminv(x)\n",
    "        wghts_scaled = torch.sigmoid(wghts)\n",
    "\n",
    "        # prcp_wghts_sum = torch.sum(wghts_scaled, dim=2)\n",
    "        # prcp_wghts_sum = torch.sum(wghts_scaled[:,:,:3], dim=2)\n",
    "        prcp_wghts_sum = torch.sum(wghts_scaled[:,:,:self.ntp], dim=2)\n",
    "\n",
    "\n",
    "        # range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)+self.RangeBoundLoss([temp_wghts_sum], factor=prcp_loss_factor)+self.RangeBoundLoss([pet_wghts_sum], factor=prcp_loss_factor)\n",
    "        range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)\n",
    "\n",
    "        # range_bound_loss_prcp = 0\n",
    "\n",
    "        # Use if the Dr. Shen requests gradient analysis.\n",
    "        # grad_daymet = autograd.grad(outputs=wghts_scaled[:, :, 0], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 0]), retain_graph=True)[0]\n",
    "        # grad_maurer = autograd.grad(outputs=wghts_scaled[:, :, 1], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 1]), retain_graph=True)[0]\n",
    "        # grad_nldas = autograd.grad(outputs=wghts_scaled[:, :, 2], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 2]), retain_graph=True)[0]\n",
    "\n",
    "        # return x_new, range_bound_loss_prcp, wghts_scaled, grad_daymet, grad_maurer, grad_nldas\n",
    "        return range_bound_loss_prcp, wghts_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import ValuesView\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def trainEnsemble(model,\n",
    "                  x,\n",
    "                  y,\n",
    "                  lossFun,\n",
    "                  *,\n",
    "                  nEpoch=500,\n",
    "                  startEpoch=1,\n",
    "                  miniBatch=[100, 30],\n",
    "                  saveEpoch=100,\n",
    "                  saveFolder=None,\n",
    "                  mode='seq2seq',\n",
    "                  bufftime=0,\n",
    "                  prcp_loss_factor = 15,\n",
    "                  smooth_loss_factor = 0,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    x- input;\n",
    "    y - target;\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        lossFun = lossFun.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    batchSize, rho = miniBatch\n",
    "    ngrid, nt, nx = x.shape\n",
    "\n",
    "    if batchSize >= ngrid:\n",
    "        # batchsize larger than total grids\n",
    "        batchSize = ngrid\n",
    "\n",
    "    nIterEp = int(\n",
    "        np.ceil(np.log(0.01) / np.log(1 - batchSize * rho / ngrid / (nt-bufftime))))\n",
    "    if hasattr(model, 'ctRm'):\n",
    "        if model.ctRm is True:\n",
    "            nIterEp = int(\n",
    "                np.ceil(\n",
    "                    np.log(0.01) / np.log(1 - batchSize *\n",
    "                                          (rho - model.ct) / ngrid / (nt-bufftime))))\n",
    "\n",
    "    optim = torch.optim.Adadelta(list(model.parameters()))\n",
    "    model.zero_grad()\n",
    "\n",
    "    if saveFolder != None:\n",
    "        os.makedirs(saveFolder, exist_ok=True)\n",
    "    #     runFile = os.path.join(saveFolder, 'run.csv')\n",
    "\n",
    "    #     rf = open(runFile, 'w+')\n",
    "\n",
    "    for iEpoch in range(startEpoch, nEpoch + 1):\n",
    "        lossEp = 0\n",
    "        loss_prcp_Ep = 0\n",
    "        loss_sf_Ep = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        prog_str = \"Epoch \" + str(iEpoch) + \"/\" + str(nEpoch)\n",
    "\n",
    "        for iIter in tqdm(range(0, nIterEp), desc=prog_str, leave=False):\n",
    "            # training iterations\n",
    "            iGrid, iT = randomIndex(ngrid, nt, [batchSize, rho], bufftime=bufftime)\n",
    "\n",
    "            if type(model) == EnsembleWeights:\n",
    "                # leave here to allow additional model types for future.\n",
    "                xTrain = selectSubset(x, iGrid, iT, rho, bufftime=bufftime)\n",
    "            else:\n",
    "                raise ValueError(\"Model must be of type `Ensemble_weights\")\n",
    "\n",
    "            yTrain = selectSubset(y, iGrid, iT, rho)\n",
    "\n",
    "            # calculate loss and weights `wt`.\n",
    "            prcp_loss, prcp_wt = model(xTrain, prcp_loss_factor)\n",
    "            yP = xTrain * prcp_wt\n",
    "\n",
    "\n",
    "            ## temporary test for NSE loss\n",
    "            if type(lossFun) in [crit.NSELossBatch, crit.NSESqrtLossBatch]:\n",
    "                loss_sf = lossFun(yP, yTrain, iGrid)\n",
    "                loss = loss_sf + prcp_loss\n",
    "            else:\n",
    "                loss_sf = lossFun(yP, yTrain)\n",
    "                loss = loss_sf + prcp_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            lossEp = lossEp + loss.item()\n",
    "\n",
    "            try:\n",
    "                loss_prcp_Ep = loss_prcp_Ep + prcp_loss.item()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            loss_sf_Ep = loss_sf_Ep + loss_sf.item()\n",
    "\n",
    "            # if iIter % 100 == 0:\n",
    "            #     # print loss\n",
    "            #     print('Iter {} of {}: Loss {:.3f}'.format(iIter, nIterEp, loss.item()))\n",
    "\n",
    "        lossEp = lossEp / nIterEp\n",
    "        loss_sf_Ep = loss_sf_Ep / nIterEp\n",
    "        loss_prcp_Ep = loss_prcp_Ep / nIterEp\n",
    "\n",
    "        logStr = 'Epoch {} Loss {:.3f}, Streamflow Loss {:.3f}, Precipitation Loss {:.3f}, time {:.2f}'.format(\n",
    "            iEpoch, lossEp, loss_sf_Ep, loss_prcp_Ep,\n",
    "            time.time() - t0)\n",
    "        print(logStr)\n",
    "\n",
    "        # save model and loss\n",
    "        if saveFolder != None:\n",
    "            runFile = os.path.join(saveFolder, 'run.csv')\n",
    "\n",
    "            with open(runFile, 'w+') as rf:\n",
    "              rf.write(logStr + '\\n')\n",
    "\n",
    "            if iEpoch % saveEpoch == 0:\n",
    "                # save model\n",
    "                modelFile = os.path.join(saveFolder,\n",
    "                                         'model_Ep' + str(iEpoch) + '.pt')\n",
    "                torch.save(model, modelFile)\n",
    "\n",
    "    if saveFolder != None:\n",
    "        rf.close()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing Ensemble\n",
    "\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\\\\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCHSIZE = 100\n",
    "LOSS_FACTOR = 15\n",
    "SMOOTH_LOSS_FACTOR = 0\n",
    "ALPHA = 0.25  # A weight for RMSE loss to balance low and peak flow.\n",
    "LOSSFUNC = crit.RmseLossComb(alpha=ALPHA)\n",
    "\n",
    "# model = EnsembleWeights(ninv=3, hiddeninv=256, prcp_datatypes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = loadModel(save_path, epoch=train_epoch)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m trainEnsemble(\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     10\u001b[0m                       preds,\n\u001b[1;32m     11\u001b[0m                       obs,\n\u001b[1;32m     12\u001b[0m                       LOSSFUNC,\n\u001b[1;32m     13\u001b[0m                       nEpoch\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m     14\u001b[0m                       startEpoch \u001b[38;5;241m=\u001b[39m train_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m                       miniBatch\u001b[38;5;241m=\u001b[39m[BATCHSIZE, \u001b[38;5;241m30\u001b[39m],\n\u001b[1;32m     16\u001b[0m                       saveEpoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m                       saveFolder\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m     18\u001b[0m                       mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2seq\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m                       bufftime\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     20\u001b[0m                       prcp_loss_factor \u001b[38;5;241m=\u001b[39m LOSS_FACTOR,\n\u001b[1;32m     21\u001b[0m                       smooth_loss_factor \u001b[38;5;241m=\u001b[39m SMOOTH_LOSS_FACTOR\n\u001b[1;32m     22\u001b[0m                       )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting save path for model.\n",
    "rootdir = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results/multimodels/671_sites_dp/ensemble/'\n",
    "save_path = rootdir + 'HBV_SAC_wSnow_PRMS' + '_E' + str(EPOCHS) + '_B' + str(BATCHSIZE) + '_L' + str(LOSS_FACTOR) + '_SmL' + str(SMOOTH_LOSS_FACTOR)\n",
    "\n",
    "# Load previous model to continue training:\n",
    "train_epoch = 0\n",
    "# model = loadModel(save_path, epoch=train_epoch)\n",
    "\n",
    "model = trainEnsemble(model,\n",
    "                      preds,\n",
    "                      obs,\n",
    "                      LOSSFUNC,\n",
    "                      nEpoch=EPOCHS,\n",
    "                      startEpoch = train_epoch+1,\n",
    "                      miniBatch=[BATCHSIZE, 30],\n",
    "                      saveEpoch=1,\n",
    "                      saveFolder=save_path,\n",
    "                      mode='seq2seq',\n",
    "                      bufftime=0,\n",
    "                      prcp_loss_factor = LOSS_FACTOR,\n",
    "                      smooth_loss_factor = SMOOTH_LOSS_FACTOR\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/Camels/basin_timeseries_v1p2_metForcing_obsFlow/basin_dataset_public_v1p2/basin_metadata/gauge_information.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m rootDatabase \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscratch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCamels\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# CAMELS dataset root directory\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# rootDatabase = os.path.join(os.path.sep, 'data', 'kas7897', 'dPLHBVrelease')  # CAMELS dataset root directory\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43mcamels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitcamels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrootDatabase\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# initialize camels module-scope variables in camels.py (dirDB, gageDict) to read basin info\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# rootOut = os.path.join(os.path.sep, 'data', 'rnnStreamflow')  # Model output root directory\u001b[39;00m\n\u001b[1;32m    119\u001b[0m rootOut \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkas7897\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdPLHBVrelease\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Model output root directory\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/hydroDL/data/camels.py:498\u001b[0m, in \u001b[0;36minitcamels\u001b[0;34m(rootDB)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m dirDB, gageDict, statDict\n\u001b[1;32m    497\u001b[0m dirDB \u001b[38;5;241m=\u001b[39m rootDB\n\u001b[0;32m--> 498\u001b[0m gageDict \u001b[38;5;241m=\u001b[39m \u001b[43mreadGageInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirDB\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/hydroDL/data/camels.py:35\u001b[0m, in \u001b[0;36mreadGageInfo\u001b[0;34m(dirDB)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadGageInfo\u001b[39m(dirDB):\n\u001b[1;32m     31\u001b[0m     gageFile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirDB, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasin_timeseries_v1p2_metForcing_obsFlow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasin_dataset_public_v1p2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasin_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgauge_information.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgageFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# header gives some troubles. Skip and hardcode\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     fieldLst \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/PGML_STemp/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/PGML_STemp/lib/python3.12/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/PGML_STemp/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/PGML_STemp/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/PGML_STemp/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/Camels/basin_timeseries_v1p2_metForcing_obsFlow/basin_dataset_public_v1p2/basin_metadata/gauge_information.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from hydroDL import master, utils\n",
    "from hydroDL.data import camels\n",
    "from hydroDL.master import default\n",
    "from hydroDL.model import rnn, crit, train\n",
    "from hydroDL.master import loadModel\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fix the random seeds for reproducibility\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## GPU setting\n",
    "# which GPU to use when having multiple\n",
    "# traingpuid = 6\n",
    "# torch.cuda.set_device(traingpuid)\n",
    "\n",
    "\n",
    "# my_list = json.loads(sys.argv[1])\n",
    "## Setting training options here\n",
    "PUOpt = 0\n",
    "# PUOpt values and explanations:\n",
    "# 0: train and test on ALL basins;\n",
    "# 1 for PUB spatial test, randomly hold out basins;\n",
    "# 2 for PUR spatial test, hold out a continuous region;\n",
    "buffOpt = 0\n",
    "# buffOpt defines the warm-up option for the first year of training forcing data\n",
    "# 0: do nothing, the first year forcing would only be used to warm up the next year;\n",
    "# 1: repeat first year forcing to warm up the first year;3.988196838920481\n",
    "# 2: load one more year forcing to warm up the first year\n",
    "TDOpt = True\n",
    "# TDOpt, True as using dynamic parameters and False as using static parameters\n",
    "\n",
    "\n",
    "multiforcing = True # set True if you want to use multiple forcings\n",
    "if multiforcing == False:\n",
    "    forType = 'nldas'\n",
    "    # for Type defines which forcing in CAMELS to use: 'daymet', 'nldas', 'maurer'\n",
    "else:\n",
    "    # forType = ['daymet']\n",
    "    forType = ['daymet', 'maurer_extended', 'nldas_extended']\n",
    "    # forType = ['nldas_extended', 'maurer_extended']\n",
    "\n",
    "#used only when multiforcing is True; else does not matter\n",
    "prcp_loss_factor = 23\n",
    "smooth_loss_factor = 0\n",
    "\n",
    "## Set hyperparameters\n",
    "EPOCH = 50 # total epoches to train the mode\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "saveEPOCH = 10\n",
    "Ttrain = [19801001, 19951001] # Training period\n",
    "# Ttrain = [19891001, 19991001] # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # Inversion period for historical forcings\n",
    "# Tinv = [19891001, 19991001] # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters. 12:original HBV; 13:includes the added dynamic ET para when setting ETMod=True\n",
    "BUFFTIME = 365 # for each training sample, to use BUFFTIME days to warm up the states.\n",
    "routing = True # Whether to use the routing module for simulated runoff\n",
    "Nmul = 16 # Multi-component model. How many parallel HBV components to use. 1 means the original HBV.\n",
    "comprout = False # True is doing routing for each component\n",
    "compwts = False # True is using weighted average for components; False is the simple mean\n",
    "pcorr = None # or a list to give the range of precip correction\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "dateTrain1 = datetime.strptime(str(Ttrain[0]), '%Y%m%d')\n",
    "dateTrain2 = datetime.strptime(str(Ttrain[1]), '%Y%m%d')\n",
    "delta_train = dateTrain2 - dateTrain1\n",
    "num_days_train = delta_train.days\n",
    "\n",
    "\n",
    "if TDOpt is True:\n",
    "    # Below options are only for running models with dynamic parameters\n",
    "    tdRep = [1, 13] # When using dynamic parameters, this list defines which parameters to set as dynamic\n",
    "    tdRepS = [str(ix) for ix in tdRep]\n",
    "    # ETMod: if True, use the added shape parameter (index 13) for ET. Default as False.\n",
    "    # Must set below ETMod as True and Nfea=13 when including 13 index in above tdRep list for dynamic parameters\n",
    "    # If 13 not in tdRep list, set below ETMod=False and Nfea=12 to use the original HBV without ET shape para\n",
    "    ETMod = True\n",
    "    Nfea = 13 # should be 13 when setting ETMod=True. 12 when ETMod=False\n",
    "    dydrop = 0.0 # dropout possibility for those dynamic parameters: 0.0 always dynamic; 1.0 always static\n",
    "    staind = -1 # which time step to use from the learned para time series for those static parameters\n",
    "    TDN = '/TDTestforc/'+'TD'+\"_\".join(tdRepS) +'/'\n",
    "else:\n",
    "    TDN = '/Testforc/'\n",
    "\n",
    "# Define root directory of database and output\n",
    "# Modify these based on your own location of CAMELS dataset\n",
    "# Following the data download instruction in README file, you should organize the folders like\n",
    "# 'your/path/to/Camels/basin_timeseries_v1p2_metForcing_obsFlow' and 'your/path/to/Camels/camels_attributes_v2.0'\n",
    "# Then 'rootDatabase' here should be 'your/path/to/Camels';\n",
    "# 'rootOut' is the root dir where you save the trained model\n",
    "rootDatabase = os.path.join(os.path.sep, 'scratch', 'Camels')  # CAMELS dataset root directory\n",
    "# rootDatabase = os.path.join(os.path.sep, 'data', 'kas7897', 'dPLHBVrelease')  # CAMELS dataset root directory\n",
    "camels.initcamels(rootDatabase)  # initialize camels module-scope variables in camels.py (dirDB, gageDict) to read basin info\n",
    "\n",
    "# rootOut = os.path.join(os.path.sep, 'data', 'rnnStreamflow')  # Model output root directory\n",
    "rootOut = os.path.join(os.path.sep, 'data', 'kas7897', 'dPLHBVrelease', 'output')  # Model output root directory\n",
    "\n",
    "## set up different data loadings for ALL, PUB, PUR\n",
    "testfoldInd = 1\n",
    "# Which fold to hold out for PUB (10 folds, from 1 to 10) and PUR (7 folds, from 1 to 7).\n",
    "# It doesn't matter when training on ALL basins (setting PUOpt=0), could always set testfoldInd=1 for this case.\n",
    "\n",
    "# load CAMELS basin information\n",
    "gageinfo = camels.gageDict\n",
    "hucinfo = gageinfo['huc']\n",
    "gageid = gageinfo['id']\n",
    "gageidLst = gageid.tolist()\n",
    "\n",
    "if PUOpt == 0: # training on all basins without spatial hold-out\n",
    "    puN = 'ALL'\n",
    "    TrainLS = gageidLst # all basins\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    TestLS = gageidLst\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 1: # random hold out basins. hold out the fold set by testfoldInd\n",
    "    puN = 'PUB'\n",
    "    # load the PUB basin groups\n",
    "    # randomly divide CAMELS basins into 10 groups and this file contains the basin ID for each group\n",
    "    # located in splitPath\n",
    "    splitPath = 'PUBsplitLst.txt'\n",
    "    with open(splitPath, 'r') as fp:\n",
    "        testIDLst=json.load(fp)\n",
    "    # Generate training ID lists excluding the hold out fold\n",
    "    TestLS = testIDLst[testfoldInd - 1]\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS))\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 2:\n",
    "    puN = 'PUR'\n",
    "    # Divide CAMELS dataset into 7 continous PUR regions, as shown in Feng et al, 2021 GRL; 2022 HESSD\n",
    "    # get the id list of each PUR region, save to list\n",
    "    regionID = list()\n",
    "    regionNum = list()\n",
    "    # seven regions including different HUCs\n",
    "    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ]\n",
    "    for ii in range(len(regionDivide)):\n",
    "        tempcomb = regionDivide[ii]\n",
    "        tempregid = list()\n",
    "        for ih in tempcomb:\n",
    "            tempid = gageid[hucinfo==ih].tolist()\n",
    "            tempregid = tempregid + tempid\n",
    "        regionID.append(tempregid)\n",
    "        regionNum.append(len(tempregid))\n",
    "\n",
    "    iexp = testfoldInd - 1  #index\n",
    "    TestLS = regionID[iexp] # basin ID list for testing, hold out for training\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS)) # basin ID for training\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID': TrainLS, 'TestID': TestLS}\n",
    "\n",
    "\n",
    "# apply buffOPt to solve the warm-up for the first year\n",
    "if buffOpt ==2: # load more BUFFTIME data for the first year\n",
    "    sd = utils.time.t2dt(Ttrain[0]) - dt.timedelta(days=BUFFTIME)\n",
    "    sdint = int(sd.strftime(\"%Y%m%d\"))\n",
    "    TtrainLoad = [sdint, Ttrain[1]]\n",
    "    TinvLoad = [sdint, Ttrain[1]]\n",
    "else:\n",
    "    TtrainLoad = Ttrain\n",
    "    TinvLoad = Tinv\n",
    "\n",
    "## prepare input data\n",
    "## load camels dataset\n",
    "# if forType == 'daymet' or forType==['daymet', 'maurer_extended', 'nldas_extended']:\n",
    "#     varF = ['prcp', 'tmean']\n",
    "#     varFInv = ['prcp', 'tmean']\n",
    "# else:\n",
    "#     varF = ['prcp', 'tmax'] # For CAMELS maurer and nldas forcings, tmax is actually tmean\n",
    "#     varFInv = ['prcp', 'tmax']\n",
    "\n",
    "# the attributes used to learn parameters\n",
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "               'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "               'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "               'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "               'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
    "attrWghts = ['p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "               'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "               'lai_diff', 'gvf_max', 'gvf_diff']\n",
    "\n",
    "optData = default.optDataCamels # a default dictionary for logging, updated below\n",
    "# Update the training period and variables\n",
    "\n",
    "# if forType==['daymet', 'maurer_extended', 'nldas_extended']:\n",
    "if type(forType) == list:\n",
    "    #for all forcings\n",
    "    # forcUN = np.empty([len(TrainInd), num_days_train, len(forType)*2])\n",
    "    # forcInvUN = np.empty([len(TrainInd), num_days_train, len(forType)*2])\n",
    "\n",
    "    #for multiple prcp only\n",
    "    forcUN = np.empty([len(TrainInd), num_days_train, len(forType) + 1])\n",
    "    forcInvUN = np.empty([len(TrainInd), num_days_train, len(forType) +1])\n",
    "    # counter = 0\n",
    "    for i in range(len(forType)):\n",
    "        if forType[i] == 'daymet':\n",
    "            varF = ['prcp', 'tmean']\n",
    "            varFInv = ['prcp', 'tmean']\n",
    "        else:\n",
    "            varF = ['prcp', 'tmax']  # For CAMELS maurer and nldas forcings, tmax is actually tmean\n",
    "            varFInv = ['prcp', 'tmax']\n",
    "\n",
    "        if 'daymet' in forType:\n",
    "            optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS,\n",
    "                                     forType='daymet')\n",
    "        elif 'nldas' in forType:\n",
    "            optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS,\n",
    "                                     forType='nldas')\n",
    "        elif 'nldas_extended' in forType:\n",
    "            optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS,\n",
    "                                     forType='nldas_extended')\n",
    "        else:\n",
    "            optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS,\n",
    "                                     forType=forType[0])\n",
    "\n",
    "        dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType[i])\n",
    "        forcUN_type = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False)\n",
    "\n",
    "        dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType[i])\n",
    "        forcInvUN_type = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False)\n",
    "\n",
    "        forcUN[:, :, i] = forcUN_type[:, :, 0]\n",
    "        forcInvUN[:, :, i] = forcInvUN_type[:, :, 0]\n",
    "        forcUN[:, :, -1] = forcUN_type[:, :, 1]\n",
    "        forcInvUN[:, :, -1] = forcInvUN_type[:, :, 1]\n",
    "        if forType[i] == 'daymet':\n",
    "            daymet_temp = forcUN_type[:, :, 1]\n",
    "            daymetInV_temp = forcInvUN_type[:, :, 1]\n",
    "        if forType[i] == 'nldas' or forType[i] == 'nldas_extended':\n",
    "            nldas_temp = forcUN_type[:, :, 1]\n",
    "            nldasInV_temp = forcInvUN_type[:, :, 1]\n",
    "\n",
    "        #for all forcings\n",
    "        # forcUN[:,:,i] = forcUN_type[:,:,0]\n",
    "        # forcUN[:,:,i+3] = forcUN_type[:,:,1]\n",
    "        # forcInvUN[:,:,i] = forcInvUN_type[:,:,0]\n",
    "        # forcInvUN[:,:,i+3] = forcInvUN_type[:,:,1]\n",
    "\n",
    "\n",
    "    obsUN = dfTrain.getDataObs(doNorm=False, rmNan=False, basinnorm=False)\n",
    "    attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)\n",
    "    attrs_wghtsUN = dfInv.getDataConst(varLst=attrWghts, doNorm=False, rmNan=False)\n",
    "\n",
    "    if 'daymet' in forType:\n",
    "        forcUN[:, :, -1] = daymet_temp\n",
    "        forcInvUN[:, :, -1] = daymetInV_temp\n",
    "    elif 'nldas' in forType or 'nldas_extended' in forType:\n",
    "        forcUN[:, :, -1] = nldas_temp\n",
    "        forcInvUN[:, :, -1] = nldasInV_temp\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    if forType == 'daymet':\n",
    "        varF = ['prcp', 'tmean']\n",
    "        varFInv = ['prcp', 'tmean']\n",
    "    else:\n",
    "        varF = ['prcp', 'tmax']  # For CAMELS maurer and nldas forcings, tmax is actually tmean\n",
    "        varFInv = ['prcp', 'tmax']\n",
    "    optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS, forType=forType)\n",
    "    dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n",
    "    forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False)\n",
    "    obsUN = dfTrain.getDataObs(doNorm=False, rmNan=False, basinnorm=False)\n",
    "    # for dPL inversion data, inputs of gA\n",
    "    dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "    forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False)\n",
    "    attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)\n",
    "\n",
    "\n",
    "# for HBV model training inputs\n",
    "\n",
    "\n",
    "# dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "# forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False)\n",
    "# attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)\n",
    "\n",
    "# Unit transformation, discharge obs from ft3/s to mm/day\n",
    "areas = gageinfo['area'][TrainInd] # unit km2\n",
    "temparea = np.tile(areas[:, None, None], (1, obsUN.shape[1],1))\n",
    "obsUN = (obsUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3 # transform to mm/day\n",
    "\n",
    "# load potential ET calculated by hargreaves method\n",
    "varLstNL = ['PEVAP']\n",
    "usgsIdLst = gageid\n",
    "\n",
    "#for multiple PETs\n",
    "# PETUN = np.empty([len(usgsIdLst), num_days_train, len(forType)])\n",
    "# PETInvUN = np.empty([len(usgsIdLst), num_days_train, len(forType)])\n",
    "# if type(forType) == list:\n",
    "#     for i in range(len(forType)):\n",
    "#         if forType[i] == 'nldas_extended' or forType[i] == 'nldas':\n",
    "#             PETDir = rootDatabase + '/pet_harg/' + 'nldas' + '/'\n",
    "#             tPETRange = [19800101, 20150101]\n",
    "#             tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "#         if forType[i] == 'maurer_extended' or forType[i] == 'maurer':\n",
    "#             PETDir = rootDatabase + '/pet_harg/' + 'maurer' + '/'\n",
    "#             tPETRange = [19800101, 20090101]\n",
    "#             tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "#         if forType[i] == 'daymet':\n",
    "#             PETDir = rootDatabase + '/pet_harg/' + 'daymet' + '/'\n",
    "#             tPETRange = [19800101, 20150101]\n",
    "#             tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "#         ntime = len(tPETLst)\n",
    "#         PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "#         for k in range(len(usgsIdLst)):\n",
    "#             dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "#             PETfull[k, :, :] = dataTemp\n",
    "#         TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "#         TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "#         C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "#         PETUN_type = PETfull[:, ind2, :]\n",
    "#         PETUN_type = PETUN_type[TrainInd, :, :] # select basins\n",
    "#         PETUN[:,:,i]  = PETUN_type[:,:,0]\n",
    "#         C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "#         PETInvUN_type = PETfull[:, ind2inv, :]\n",
    "#         PETInvUN_type = PETInvUN_type[TrainInd, :, :]\n",
    "#         PETInvUN[:,:,i] = PETInvUN_type[:,:,0]\n",
    "\n",
    "if forType == 'maurer' or forType=='maurer_extended' or forType == ['maurer'] or forType == ['maurer_extended']:\n",
    "    tPETRange = [19800101, 20090101]\n",
    "else:\n",
    "    tPETRange = [19800101, 20150101]\n",
    "tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "# Modify this as the directory where you put PET\n",
    "if type(forType) == list:\n",
    "    if forType[0]=='nldas_extended':\n",
    "        PETDir = rootDatabase + '/pet_harg/' + 'nldas' + '/'\n",
    "    elif forType[0]=='maurer_extended':\n",
    "        PETDir = rootDatabase + '/pet_harg/' + 'maurer' + '/'\n",
    "    else:\n",
    "        PETDir = rootDatabase + '/pet_harg/' + forType[0] + '/'\n",
    "else:\n",
    "    PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n",
    "\n",
    "ntime = len(tPETLst)\n",
    "PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "for k in range(len(usgsIdLst)):\n",
    "    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "    PETfull[k, :, :] = dataTemp\n",
    "\n",
    "TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "PETUN = PETfull[:, ind2, :]\n",
    "PETUN = PETUN[TrainInd, :, :] # select basins\n",
    "C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "PETInvUN = PETfull[:, ind2inv, :]\n",
    "PETInvUN = PETInvUN[TrainInd, :, :]\n",
    "\n",
    "# process data, do normalization and remove nan\n",
    "series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n",
    "series_inv_hbv = series_inv[:,:,(0,-2,-1)]\n",
    "seriesvarLst = varFInv + ['pet']\n",
    "# calculate statistics for normalization and saved to a dictionary\n",
    "statDict_hbv = camels.getStatDic(attrLst=attrnewLst, attrdata=attrsUN, seriesLst=seriesvarLst, seriesdata=series_inv_hbv)\n",
    "statDict_wghts = camels.getStatDic(attrLst=attrWghts, attrdata=attrs_wghtsUN, seriesLst=['prcp_daymet', 'prcp_maurer', 'prcp_nldas', 'tmax', 'pet'], seriesdata=series_inv)\n",
    "# normalize data\n",
    "attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict_hbv, toNorm=True)\n",
    "attrWghts_norm = camels.transNormbyDic(attrs_wghtsUN, attrWghts, statDict_hbv, toNorm=True)\n",
    "attr_norm[np.isnan(attr_norm)] = 0.0\n",
    "attrWghts_norm[np.isnan(attrWghts_norm)] = 0.0\n",
    "series_norm_hbv = camels.transNormbyDic(series_inv_hbv, seriesvarLst, statDict_hbv, toNorm=True)\n",
    "series_Wghts_norm = camels.transNormbyDic(series_inv, ['prcp_daymet', 'prcp_maurer', 'prcp_nldas', 'tmax', 'pet'], statDict_wghts, toNorm=True)\n",
    "series_norm_hbv[np.isnan(series_norm_hbv)] = 0.0\n",
    "series_Wghts_norm[np.isnan(series_Wghts_norm)] = 0.0\n",
    "\n",
    "# prepare the inputs\n",
    "zTrain_hbv = series_norm_hbv # used as the inputs for dPL inversion gA along with attributes\n",
    "zTrain_wghts = series_Wghts_norm # used as the inputs for dPL inversion gA along with attributes\n",
    "xTrain_wghts = np.concatenate([forcUN, PETUN], axis=2) # used as HBV forcing\n",
    "xTrain_hbv = xTrain_wghts[:,:,(0,-2,-1)] # used as HBV forcing\n",
    "xTrain_wghts[np.isnan(xTrain_wghts)] = 0.0\n",
    "xTrain_hbv[np.isnan(xTrain_hbv)] = 0.0\n",
    "\n",
    "if buffOpt == 1: # repeat the first year warm up the first year itself\n",
    "    zTrainIn_hbv = np.concatenate([zTrain_hbv[:,0:BUFFTIME,:], zTrain_hbv], axis=1)\n",
    "    zTrainIn_wghts = np.concatenate([zTrain_wghts[:,0:BUFFTIME,:], zTrain_wghts], axis=1)\n",
    "    xTrainIn_wghts = np.concatenate([xTrain_wghts[:,0:BUFFTIME,:], xTrain_wghts], axis=1) # repeat forcing to warm up the first year\n",
    "    xTrainIn_hbv = np.concatenate([xTrain_hbv[:,0:BUFFTIME,:], xTrain_hbv], axis=1) # repeat \n",
    "else: # no repeat, original data, the first year data would only be used as warmup for the next following year\n",
    "    zTrainIn_hbv = zTrain_hbv\n",
    "    zTrainIn_wghts = zTrain_wghts\n",
    "    xTrainIn_wghts = xTrain_wghts\n",
    "    xTrainIn_hbv = xTrain_hbv\n",
    "\n",
    "forcTuple_hbv = (xTrainIn_hbv, zTrainIn_hbv)\n",
    "forcTuple_wghts = (xTrainIn_wghts, zTrainIn_wghts)\n",
    "attrs = attr_norm\n",
    "attrs_wghts = attrWghts_norm\n",
    "\n",
    "## Train the model\n",
    "# define loss function\n",
    "alpha = 0.25 # a weight for RMSE loss to balance low and peak flow\n",
    "optLoss = default.update(default.optLossComb, name='hydroDL.model.crit.RmseLossComb', weight=alpha)\n",
    "lossFun = crit.RmseLossComb(alpha=alpha)\n",
    "\n",
    "# define training options\n",
    "optTrain = default.update(default.optTrainCamels, miniBatch=[BATCH_SIZE, RHO], nEpoch=EPOCH, saveEpoch=saveEPOCH)\n",
    "# define output folder to save model results\n",
    "exp_name = 'CAMELSDemo'\n",
    "if forType==['daymet', 'maurer_extended', 'nldas_extended']:\n",
    "    exp_disp = 'LSTM-dPLHBV/' + puN + TDN + 'allprcp_withloss' + str(prcp_loss_factor) + 'smooth' + str(smooth_loss_factor) + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "           '/Fold' + str(testfoldInd)\n",
    "elif forType==['daymet', 'maurer', 'nldas']:\n",
    "    exp_disp = 'dPLHBV/' + puN + TDN + 'all_withloss' + str(prcp_loss_factor) + 'smooth' + str(smooth_loss_factor) + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "           '/Fold' + str(testfoldInd)\n",
    "elif type(forType)==list:\n",
    "    forType_string = '|'.join(forType)\n",
    "    exp_disp = 'dPLHBV/' + puN + TDN + forType_string + 'withloss' + str(prcp_loss_factor)+ 'smooth' + str(smooth_loss_factor) + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "           '/Fold' + str(testfoldInd)\n",
    "else:\n",
    "    exp_disp = 'dPLHBV/' + puN + TDN + forType + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "            '/Fold' + str(testfoldInd)\n",
    "exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n",
    "           +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n",
    "save_path = os.path.join(exp_name, exp_disp)\n",
    "out = os.path.join(rootOut, save_path, exp_info) # output folder to save results\n",
    "# define and load model\n",
    "Ninv = zTrain_wghts.shape[-1] + attrs_wghts.shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "##################### MODEL ##########################\n",
    "model = prcp_weights(ninv=Ninv, hiddeninv=HIDDENSIZE, prcp_datatypes=len(forType))\n",
    "\n",
    "\n",
    "\n",
    "# dict only for logging\n",
    "optModel = OrderedDict(name='LSTM-dPLHBV', nx=Ninv, nfea=Nfea, nmul=Nmul, hiddenSize=HIDDENSIZE, doReLU=True,\n",
    "                        Tinv=Tinv, Trainbuff=BUFFTIME, routOpt=routing, comprout=comprout, compwts=compwts,\n",
    "                        pcorr=pcorr, staind=staind, tdlst=tdRep, dydrop=dydrop,buffOpt=buffOpt, TDOpt=TDOpt, ETMod=ETMod)\n",
    "\n",
    "\n",
    "\n",
    "# Wrap up all the training configurations to one dictionary in order to save into \"out\" folder as logging\n",
    "masterDict = master.wrapMaster(out, optData, optModel, optLoss, optTrain)\n",
    "master.writeMasterFile(masterDict)\n",
    "# log statistics for normalization\n",
    "statFile_wghts = os.path.join(out, 'statDict_wghts.json')\n",
    "with open(statFile_wghts, 'w') as fp:\n",
    "    json.dump(statDict_wghts, fp, indent=4)\n",
    "statFile_hbv = os.path.join(out, 'statDict_hbv.json')\n",
    "with open(statFile_hbv, 'w') as fp:\n",
    "    json.dump(statDict_hbv, fp, indent=4)\n",
    "\n",
    "testout = \"/data/kas7897/dPLHBVrelease/output/CAMELSDemo/dPLHBV/ALL\" \\\n",
    "          \"/TDTestforc/TD1_13/daymet/BuffOpt0/RMSE_para0.25/111111/Fold1\" \\\n",
    "          \"/T_19801001_19951001_BS_100_HS_256_RHO_365_NF_13_Buff_365_Mul_16/model_Ep50.pt\"\n",
    "loaded_hbv = torch.load(testout)\n",
    "\n",
    "# Train the model\n",
    "# trainedModel = train.train2Model(\n",
    "#     model,\n",
    "#     loaded_hbv,\n",
    "#     forcTuple_wghts,\n",
    "#     forcTuple_hbv,\n",
    "#     yTrainIn,\n",
    "#     attrs_wghts,\n",
    "#     attrs,\n",
    "#     lossFun,\n",
    "#     nEpoch=EPOCH,\n",
    "#     miniBatch=[BATCH_SIZE, RHO],\n",
    "#     saveEpoch=saveEPOCH,\n",
    "#     saveFolder=out,\n",
    "#     bufftime=BUFFTIME,\n",
    "#     multiforcing=multiforcing,\n",
    "#     prcp_loss_factor=prcp_loss_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "               'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "               'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "               'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "               'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attrnewLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'omegaconf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ConfigDict, PrivateAttr\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictConfig\n\u001b[1;32m     11\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDates\u001b[39;00m(BaseModel):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'omegaconf'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, ConfigDict, PrivateAttr\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Dates(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    daily_format: str = \"%Y/%m/%d\"\n",
    "    hourly_format: str = \"%Y/%m/%d %H:%M:%S\"\n",
    "    origin_start_date: str = \"1980/01/01\"\n",
    "    start_time: str\n",
    "    end_time: str\n",
    "    rho: Optional[int] = None\n",
    "    batch_daily_time_range: pd.DatetimeIndex = PrivateAttr\n",
    "    batch_hourly_time_range: pd.DatetimeIndex = PrivateAttr\n",
    "    daily_time_range: pd.DatetimeIndex = PrivateAttr\n",
    "    hourly_indices: torch.Tensor = PrivateAttr\n",
    "    hourly_time_range: pd.DatetimeIndex = PrivateAttr\n",
    "    numerical_time_range: np.ndarray = PrivateAttr\n",
    "\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super(Dates, self).__init__(\n",
    "            start_time=cfg.dataset.time.start,\n",
    "            end_time=cfg.dataset.time.end,\n",
    "            rho=cfg.dataset.get(\"rho\", None),\n",
    "        )\n",
    "\n",
    "    def model_post_init(self, __context: Any) -> None:\n",
    "        self.daily_time_range = pd.date_range(\n",
    "            datetime.strptime(self.start_time, self.daily_format),\n",
    "            datetime.strptime(self.end_time, self.daily_format),\n",
    "            freq=\"D\",\n",
    "            inclusive=\"both\",\n",
    "        )\n",
    "        self.hourly_time_range = pd.date_range(\n",
    "            start=self.daily_time_range[0],\n",
    "            end=self.daily_time_range[-1],\n",
    "            freq=\"h\",\n",
    "            inclusive=\"both\",\n",
    "        )\n",
    "        self.batch_daily_time_range = self.daily_time_range\n",
    "        self.set_batch_time(self.daily_time_range)\n",
    "\n",
    "    def set_batch_time(self, daily_time_range: pd.DatetimeIndex):\n",
    "        self.batch_hourly_time_range = pd.date_range(\n",
    "            start=daily_time_range[0],\n",
    "            end=daily_time_range[-1],\n",
    "            freq=\"h\",\n",
    "            inclusive=\"both\",\n",
    "        )\n",
    "        origin_start_date = datetime.strptime(self.origin_start_date, self.daily_format)\n",
    "        origin_base_start_time = int(\n",
    "            (daily_time_range[0].to_pydatetime() - origin_start_date).total_seconds()\n",
    "            / 86400\n",
    "        )\n",
    "        origin_base_end_time = int(\n",
    "            (daily_time_range[-1].to_pydatetime() - origin_start_date).total_seconds()\n",
    "            / 86400\n",
    "        )\n",
    "\n",
    "        # The indices for the dates in your selected routing time range\n",
    "        self.numerical_time_range = np.arange(\n",
    "            origin_base_start_time, origin_base_end_time + 1, 1\n",
    "        )\n",
    "\n",
    "        common_elements = self.hourly_time_range.intersection(\n",
    "            self.batch_hourly_time_range\n",
    "        )\n",
    "        self.hourly_indices = torch.tensor(\n",
    "            [self.hourly_time_range.get_loc(time) for time in common_elements]\n",
    "        )\n",
    "\n",
    "    def calculate_time_period(self) -> None:\n",
    "        if self.rho is not None:\n",
    "            sample_size = len(self.daily_time_range)\n",
    "            random_start = torch.randint(\n",
    "                low=0, high=sample_size - self.rho, size=(1, 1)\n",
    "            )[0][0].item()\n",
    "            self.batch_daily_time_range = self.daily_time_range[\n",
    "                random_start : (random_start + self.rho)\n",
    "            ]\n",
    "            self.set_batch_time(self.batch_daily_time_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGML_STemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
