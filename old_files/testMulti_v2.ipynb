{"cells":[{"cell_type":"markdown","metadata":{"id":"q1jGJFeaF0oG"},"source":["### Running bulk of multimodel testing\n","\n","This is equivalent to that present in the multimodel wrapper.\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1832,"status":"ok","timestamp":1707705234766,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"2Vt-fsprDO_u","outputId":"c300c26f-3e5f-4134-c3f5-a0182475218a"},"outputs":[],"source":["from config.read_configurations import config_hbv as hbvArgs\n","from config.read_configurations import config_prms as prmsArgs\n","from config.read_configurations import config_sacsma as sacsmaArgs\n","from config.read_configurations import config_sacsma_snow as sacsmaSnowArgs\n","from config.read_configurations import config_hbv_hydrodl as hbvhyArgs_d\n","\n","\n","import torch\n","import os\n","import platform\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import scipy.stats\n","# from post import plot\n","\n","from core.utils.randomseed_config import randomseed_config\n","from core.utils.master import create_output_dirs\n","from MODELS.loss_functions.get_loss_function import get_lossFun\n","from MODELS.test_dp_HBV_dynamic import test_dp_hbv\n","from core.data_processing.data_loading import loadData\n","from core.data_processing.normalization import transNorm\n","from core.utils.randomseed_config import randomseed_config\n","from core.data_processing.model import (\n","    take_sample_test,\n","    converting_flow_from_ft3_per_sec_to_mm_per_day\n",")\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","\n","# Set path to `hydro_multimodel_results` directory.\n","if platform.system() == 'Darwin':\n","    # For mac os\n","    out_dir = '/Users/leoglonz/Desktop/water/data/model_runs/hydro_multimodel_results'\n","    # Some operations are not yet working with MPS, so we might need to set some environment variables to use CPU fall instead\n","    # %env PYTORCH_ENABLE_MPS_FALLBACK=1\n","\n","elif platform.system() == 'Windows':\n","    # For windows\n","    out_dir = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n","\n","elif platform.system() == 'Linux':\n","    # For Colab\n","    out_dir = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results'\n","\n","else:\n","    raise ValueError('Unsupported operating system.')\n","\n","\n","##-----## Multi-model Parameters ##-----##\n","##--------------------------------------##\n","# Setting dictionaries to separately manage each diff model's attributes.\n","models = {'HBV_dynamic': None, 'SACSMA_snow':None, 'marrmot_PRMS':None}  # 'HBV':None, 'hbvhy': None, 'SACSMA_snow':None, 'SACSMA':None,\n","args_list = {'HBV_dynamic': hbvhyArgs_d, 'SACSMA_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\n","ENSEMBLE_TYPE = 'max'  # 'median', 'avg', 'max', 'softmax'\n","\n","# Load test observations and predictions from a prior run.\n","pred_path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_test_preds_obs', 'HBV_dynamic_SACSMA_snow_marrmot_PRMS', 'preds_multim_E50_B25_R365_BT365_H256_tr1980_1995_n16.npy')\n","obs_path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_test_preds_obs', 'HBV_dynamic_SACSMA_snow_marrmot_PRMS', 'obs_multim_E50_B25_R365_BT365_H256_tr1980_1995_n16.npy')\n","preds = np.load(pred_path, allow_pickle=True).item()\n","obs = np.load(obs_path, allow_pickle=True).item()\n","\n","model_output = preds\n","y_obs = obs"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":216,"status":"ok","timestamp":1707706633956,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"typgVyc9DO_z"},"outputs":[],"source":["# included in hydrodl.master.master\n","def calFDC(data):\n","    # data = Ngrid * Nday\n","    Ngrid, Nday = data.shape\n","    FDC100 = np.full([Ngrid, 100], np.nan)\n","    for ii in range(Ngrid):\n","        tempdata0 = data[ii, :]\n","        tempdata = tempdata0[~np.isnan(tempdata0)]\n","        # deal with no data case for some gages\n","        if len(tempdata)==0:\n","            tempdata = np.full(Nday, 0)\n","        # sort from large to small\n","        temp_sort = np.sort(tempdata)[::-1]\n","        # select 100 quantile points\n","        Nlen = len(tempdata)\n","        ind = (np.arange(100)/100*Nlen).astype(int)\n","        FDCflow = temp_sort[ind]\n","        if len(FDCflow) != 100:\n","            raise Exception('unknown assimilation variable')\n","        else:\n","            FDC100[ii, :] = FDCflow\n","\n","    return FDC100\n","\n","# included in hydrodl.post.stat\n","def statError(pred, target):\n","    ngrid, nt = pred.shape\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n","    # Bias\n","        Bias = np.nanmean(pred - target, axis=1)\n","        # RMSE\n","        RMSE = np.sqrt(np.nanmean((pred - target)**2, axis=1))\n","        # ubRMSE\n","        predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n","        targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n","        predAnom = pred - predMean\n","        targetAnom = target - targetMean\n","        ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom)**2, axis=1))\n","        # FDC metric\n","        predFDC = calFDC(pred)\n","        targetFDC = calFDC(target)\n","        FDCRMSE = np.sqrt(np.nanmean((predFDC - targetFDC) ** 2, axis=1))\n","    # rho R2 NSE\n","        Corr = np.full(ngrid, np.nan)\n","        CorrSp = np.full(ngrid, np.nan)\n","        R2 = np.full(ngrid, np.nan)\n","        NSE = np.full(ngrid, np.nan)\n","        PBiaslow = np.full(ngrid, np.nan)\n","        PBiashigh = np.full(ngrid, np.nan)\n","        PBias = np.full(ngrid, np.nan)\n","        PBiasother = np.full(ngrid, np.nan)\n","        KGE = np.full(ngrid, np.nan)\n","        KGE12 = np.full(ngrid, np.nan)\n","        RMSElow = np.full(ngrid, np.nan)\n","        RMSEhigh = np.full(ngrid, np.nan)\n","        RMSEother = np.full(ngrid, np.nan)\n","        for k in range(0, ngrid):\n","            x = pred[k, :]\n","            y = target[k, :]\n","            ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n","            if ind.shape[0] > 0:\n","                xx = x[ind]\n","                yy = y[ind]\n","                # percent bias\n","                PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n","\n","                # FHV the peak flows bias 2%\n","                # FLV the low flows bias bottom 30%, log space\n","                pred_sort = np.sort(xx)\n","                target_sort = np.sort(yy)\n","                indexlow = round(0.3 * len(pred_sort))\n","                indexhigh = round(0.98 * len(pred_sort))\n","                lowpred = pred_sort[:indexlow]\n","                highpred = pred_sort[indexhigh:]\n","                otherpred = pred_sort[indexlow:indexhigh]\n","                lowtarget = target_sort[:indexlow]\n","                hightarget = target_sort[indexhigh:]\n","                othertarget = target_sort[indexlow:indexhigh]\n","                PBiaslow[k] = np.sum((lowpred - lowtarget)) / (np.sum(lowtarget) +0.0001)* 100\n","                PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n","                PBiasother[k] = np.sum(otherpred - othertarget) / np.sum(othertarget) * 100\n","                RMSElow[k] = np.sqrt(np.nanmean((lowpred - lowtarget)**2))\n","                RMSEhigh[k] = np.sqrt(np.nanmean((highpred - hightarget)**2))\n","                RMSEother[k] = np.sqrt(np.nanmean((otherpred - othertarget)**2))\n","\n","                if ind.shape[0] > 1:\n","                    # Theoretically at least two points for correlation\n","                    Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n","                    CorrSp[k] = scipy.stats.spearmanr(xx, yy)[0]\n","                    yymean = yy.mean()\n","                    yystd = np.std(yy)\n","                    xxmean = xx.mean()\n","                    xxstd = np.std(xx)\n","                    KGE[k] = 1 - np.sqrt((Corr[k]-1)**2 + (xxstd/yystd-1)**2 + (xxmean/yymean-1)**2)\n","                    KGE12[k] = 1 - np.sqrt((Corr[k] - 1) ** 2 + ((xxstd*yymean)/ (yystd*xxmean) - 1) ** 2 + (xxmean / yymean - 1) ** 2)\n","                    SST = np.sum((yy-yymean)**2)\n","                    SSReg = np.sum((xx-yymean)**2)\n","                    SSRes = np.sum((yy-xx)**2)\n","                    R2[k] = 1-SSRes/SST\n","                    NSE[k] = 1-SSRes/SST\n","\n","    outDict = dict(Bias=Bias, RMSE=RMSE, ubRMSE=ubRMSE, Corr=Corr, CorrSp=CorrSp, R2=R2, NSE=NSE,\n","                   FLV=PBiaslow, FHV=PBiashigh, PBias=PBias, PBiasother=PBiasother, KGE=KGE, KGE12=KGE12, fdcRMSE=FDCRMSE,\n","                   lowRMSE=RMSElow, highRMSE=RMSEhigh, midRMSE=RMSEother)\n","\n","    return outDict"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1707706634441,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"uJhJHca9DO_0"},"outputs":[],"source":["def calculate_metrics_multi(args_list, model_outputs, y_obs_list, ensemble_type='max', out_dir=None):\n","    \"\"\"\n","    Calculate stats for a multimodel ensemble.\n","    \"\"\"\n","    stats_list = dict()\n","\n","    for mod in args_list:\n","        args = args_list[mod]\n","        mod_out = model_outputs[mod]\n","        y_obs = y_obs_list[mod]\n","\n","        if mod in ['SACSMA', 'SACSMA_snow', 'marrmot_PRMS', 'HBV']:\n","            # Note for hydrodl HBV, calculations have already been done so skip.\n","\n","            # Saving data\n","            if out_dir:\n","                path = os.path.join(out_dir, 'models', '671_sites_dp', mod)\n","                if not os.path.exists(path):\n","                    os.makedirs(path, exist_ok=True)\n","\n","                # Test data (obs and model results).\n","                for key in mod_out[0].keys():\n","                    if len(mod_out[0][key].shape) == 3:\n","                        dim = 1\n","                    else:\n","                        dim = 0\n","                    concatenated_tensor = torch.cat([d[key] for d in mod_out], dim=dim)\n","                    file_name = key + \".npy\"\n","                    np.save(os.path.join(path, file_name), concatenated_tensor.numpy())\n","                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n","\n","                # Reading and flow observations.\n","                print(args['target'])\n","                for var in args[\"target\"]:\n","                    item_obs = y_obs[:, :, args[\"target\"].index(var)]\n","                    file_name = var + \".npy\"\n","                    np.save(os.path.join(path, file_name), item_obs)\n","                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n","\n","\n","            ###################### calculations here ######################\n","            pred_list = list()\n","            obs_list = list()\n","            flow_sim = torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1)\n","            flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n","\n","            pred_list.append(flow_sim.numpy())\n","            obs_list.append(np.expand_dims(flow_obs, 2))\n","            \n","\n","            # we need to swap axes here to have [basin, days], and remove redundant\n","            # dimensions with np.squeeze().\n","            stats_list[mod] = [\n","                statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n","                for (x, y) in zip(pred_list, obs_list)\n","            ]\n","        elif mod in ['hbvhy', 'HBV_dynamic']:\n","            stats_list[mod] = [statError(mod_out[:,:,0], y_obs.squeeze())]\n","        else:\n","            raise ValueError(f\"Unsupported model type in `models`.\")\n","\n","    # Calculating final statistics for the whole set of basins.\n","    name_list = [\"flow\"]\n","    for st, name in zip(stats_list[mod], name_list):\n","        count = 0\n","        mdstd = np.zeros([len(st), 3])\n","        for key in st.keys():\n","            # st contains the statistics on a model run like NSE and KGE.\n","            # Find the best result (e.g., the max, avg, median) and merge from each model.\n","            for i, mod in enumerate(args_list):\n","                if i == 0:\n","                    # temp contains the values of key per basin.\n","                    temp = stats_list[mod][0][key]\n","                    continue\n","                elif i == 1:\n","                    temp = np.stack((temp, stats_list[mod][0][key]), axis=1)\n","                else:\n","                    temp = np.hstack((temp, stats_list[mod][0][key].reshape(-1,1)))\n","            \n","            if len(args_list) > 1:\n","                if ensemble_type == 'max':\n","                    # print(temp, key)\n","                    temp = np.amax(temp, axis=1)\n","                    # print(temp, key)\n","                elif ensemble_type == 'avg':\n","                    temp = np.mean(temp, axis=1)\n","                elif ensemble_type == 'median':\n","                    temp = np.median(temp, axis=1)\n","                elif ensemble_type == 'softmax':\n","                    # # Softmax gets relative contributions of each model.\n","                    # weights = torch.nn.functional.softmax(torch.from_numpy(temp), dim=1)\n","                    # temp = np.sum(temp * weights.numpy(), axis=1)\n","\n","                    # Instantiate weighting lstm with softmax.\n","                    lstm = hydroEnsemble(num_models=len(args_list), hidden_size=192, num_layers=3)\n","                    # Forward pass through the model\n","                    temp = lstm(torch.tensor(temp, dtype=torch.float))\n","                else:\n","                    raise ValueError(\"Invalid model ensemble type specified.\")\n","\n","            median = np.nanmedian(temp)  # abs(i)\n","            std = np.nanstd(temp)  # abs(i)\n","            mean = np.nanmean(temp)  # abs(i)\n","            k = np.array([[median, std, mean]])\n","            mdstd[count] = k\n","            count = count + 1\n","\n","        # mdstd displays the statistics for each error measure in stats_list.\n","        mdstd = pd.DataFrame(\n","            mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n","        )\n","        # Save the data stats from the training run:\n","        if out_dir and len(args_list) > 1:\n","            path = os.path.join(out_dir, 'multimodels', '671_sites_dp', 'n_' + ensemble_type)\n","            if not os.path.exists(path):\n","                os.makedirs(path, exist_ok=True)\n","\n","            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + \"_\" + ensemble_type +\".csv\")))\n","        elif out_dir:\n","            path = os.path.join(out_dir, 'models', '671_sites_dp', args_list[0])\n","            if not os.path.exists(path):\n","                os.makedirs(path, exist_ok=True)\n","\n","            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + \"_\" + \".csv\")))\n","        else: continue\n","\n","    # Show boxplots of the results\n","    # plt.rcParams[\"font.size\"] = 14\n","    # keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n","    # dataBox = list()\n","    # for iS in range(len(keyLst)):\n","    #     statStr = keyLst[iS]\n","    #     temp = list()\n","    #     # for k in range(len(st)):\n","    #     data = st[statStr]\n","    #     data = data[~np.isnan(data)]\n","    #     temp.append(data)\n","    #     dataBox.append(temp)\n","    # labelname = [\n","    #     \"Hybrid differentiable model\"\n","    # ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n","\n","    # xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n","    # fig = plot.plotBoxFig(\n","    #     dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n","    # )\n","    # fig.patch.set_facecolor(\"white\")\n","    # boxPlotName = \"PGML\"\n","    # fig.suptitle(boxPlotName, fontsize=12)\n","    # plt.rcParams[\"font.size\"] = 12\n","    # # plt.savefig(\n","    # #     os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n","    # # )  # , dpi=500\n","    # # fig.show()\n","    # plt.close()\n","\n","    torch.cuda.empty_cache()\n","    print(\"Testing ended\")\n","\n","    return stats_list, mdstd\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Multimodel wrapper internals:\n","---"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['HBV_dynamic', 'SACSMA_snow', 'marrmot_PRMS'])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["args_list = {'HBV_dynamic':hbvhyArgs_d}   # 'marrmot_PRMS':prmsArgs, 'HBV_dynamic':hbvhyArgs_d, 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\n","ensemble_type = 'hacc' \n","y_obs.keys()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["HBV_dynamic\n","SACSMA_snow\n","marrmot_PRMS\n"]}],"source":["# Initialize\n","flow_preds = []\n","flow_obs = None\n","obs_trig = False\n","\n","# Concatenate individual model predictions, and observation data.\n","for i, mod in enumerate(args_list):\n","    args = args_list[mod]\n","    mod_out = model_output[mod]\n","    y_ob = y_obs[mod]\n","\n","    print(mod)\n","\n","    if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n","        # Hydro models are tested in batches, so we concatenate them and select\n","        # the desired flow.\n","        # Note: modified HBV already has this preparation done during testing.\n","\n","        # Get flow predictions and swap axes to get shape [basins, days]\n","        pred = np.swapaxes(torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1).squeeze().numpy(), 0, 1)\n","\n","        if obs_trig == False:\n","            # dPLHBV uses GAGES while the other hydro models use CAMELS data. This means small \n","            # e-5 variation in observation data between the two. This is averaged if both models\n","            # are used, but to avoid double-counting data from multiply hydro models, use a trigger.\n","            obs = np.swapaxes(y_ob[:, :, args[\"target\"].index(\"00060_Mean\")].numpy(), 0, 1)\n","            obs_trig = True\n","            dup = False\n","        else:\n","            dup = True\n","\n","    elif mod in ['HBV_dynamic']:\n","        pred = mod_out[:,:,0][:,365:] # Set dim2 = 0 to get streamflow Qr\n","        obs = y_ob.squeeze()[:,365:]\n","        dup = False\n","\n","    else:\n","        raise ValueError(f\"Unsupported model type in `models`.\")\n","    \n","    if i == 0:\n","        tmp_pred = pred\n","        tmp_obs = obs\n","    elif i == 1:\n","        tmp_pred = np.stack((tmp_pred, pred), axis=2)\n","        if not dup:\n","            # Avoid double-counting GAGES obs.\n","            tmp_obs = np.stack((tmp_obs, obs), axis=2)\n","    else:\n","        # Combine outputs of >3 models.\n","        tmp_pred = np.concatenate((tmp_pred,np.expand_dims(pred, 2)), axis=2)\n","        if not dup:\n","            # Avoid double-counting GAGES obs.\n","            tmp_obs = np.concatenate((tmp_obs,np.expand_dims(obs, 2)), axis=2)\n","\n","preds = tmp_pred\n","obs = tmp_obs"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# Merge observation data.\n","if len(obs.shape) == 3:\n","    comp_obs = np.mean(obs, axis = 2)\n","elif len(obs.shape) == 2:\n","    comp_obs = obs\n","else:\n","    raise ValueError(\"Error reading prediction data: incorrect formatting.\")\n","\n","\n","# Merge model predictions using specified merging type.\n","if len(preds.shape) == 3:\n","    if ensemble_type == 'max':\n","        comp_preds = np.amax(preds, axis=2)\n","    if ensemble_type == 'hacc':\n","        pass\n","        # comp_preds = np.amax(preds, axis=2)\n","    elif ensemble_type == 'min':\n","        comp_preds = np.amin(preds, axis=2)\n","    elif ensemble_type == 'avg':\n","        comp_preds = np.mean(preds, axis=2)\n","    elif ensemble_type == 'median':\n","        comp_preds = np.median(preds, axis=2)\n","    elif ensemble_type == 'softmax':\n","        pass\n","        ########### IN PROGRESS ############\n","        ####################################\n","    else:\n","        raise ValueError(\"Invalid model ensemble type specified.\")\n","elif len(preds.shape) == 2:\n","    comp_preds = preds  # For single model, do nothing\n","else:\n","    raise ValueError(\"Error reading prediction data: incorrect formatting.\")\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing ended\n"]}],"source":["# Calculating performance statistics at each basin e.g., NSE, KGE.\n","stat = statError(comp_preds, comp_obs)\n","\n","# Testing Tadd's metrics wrapper against statError.\n","from dPLHydro_multimodel.metrics import Metrics\n","metrics = Metrics(comp_preds, comp_obs)\n","metrics_stats = metrics.model_dump(by_alias=True, exclude=['pred', 'target'])\n","\n","\n","mdstd = np.zeros([len(stat), 3])\n","count = 0\n","\n","# Find median statistics across all basins.\n","for key in stat.keys():\n","    temp = stat[key]\n","    median = np.nanmedian(temp)\n","    std = np.nanstd(temp)\n","    mean = np.nanmean(temp)\n","\n","    k = np.array([[median, std, mean]])\n","    mdstd[count] = k\n","    count = count + 1\n","\n","\n","mdstd_met = np.zeros([len(metrics_stats), 3])\n","count = 0\n","\n","# Find median statistics across all basins.\n","for key in metrics_stats.keys():\n","    temp = metrics_stats[key]\n","    median = np.nanmedian(temp)\n","    std = np.nanstd(temp)\n","    mean = np.nanmean(temp)\n","\n","    k = np.array([[median, std, mean]])\n","    mdstd_met[count] = k\n","    count = count + 1\n","\n","    \n","# mdstd gives the statistics for each error measure in stats_list.\n","mdstd = pd.DataFrame(\n","    mdstd, index=stat.keys(), columns=[\"median\", \"STD\", \"mean\"]\n",")\n","mdstd_met = pd.DataFrame(\n","    mdstd_met, index=metrics_stats.keys(), columns=[\"median\", \"STD\", \"mean\"]\n",")\n","    \n","\n","torch.cuda.empty_cache()\n","print(\"Testing ended\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>median</th>\n","      <th>STD</th>\n","      <th>mean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bias</th>\n","      <td>0.082842</td>\n","      <td>0.358185</td>\n","      <td>0.063822</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>1.092191</td>\n","      <td>1.031007</td>\n","      <td>1.299537</td>\n","    </tr>\n","    <tr>\n","      <th>ubRMSE</th>\n","      <td>1.062194</td>\n","      <td>1.002141</td>\n","      <td>1.270872</td>\n","    </tr>\n","    <tr>\n","      <th>Corr</th>\n","      <td>0.873422</td>\n","      <td>0.113820</td>\n","      <td>0.838929</td>\n","    </tr>\n","    <tr>\n","      <th>CorrSp</th>\n","      <td>0.880218</td>\n","      <td>0.099203</td>\n","      <td>0.856538</td>\n","    </tr>\n","    <tr>\n","      <th>R2</th>\n","      <td>0.729054</td>\n","      <td>0.383030</td>\n","      <td>0.629756</td>\n","    </tr>\n","    <tr>\n","      <th>NSE</th>\n","      <td>0.729054</td>\n","      <td>0.383030</td>\n","      <td>0.629756</td>\n","    </tr>\n","    <tr>\n","      <th>FLV</th>\n","      <td>76.345316</td>\n","      <td>NaN</td>\n","      <td>inf</td>\n","    </tr>\n","    <tr>\n","      <th>FHV</th>\n","      <td>-6.393238</td>\n","      <td>25.197306</td>\n","      <td>-6.333314</td>\n","    </tr>\n","    <tr>\n","      <th>PBias</th>\n","      <td>9.792652</td>\n","      <td>28.731551</td>\n","      <td>11.954376</td>\n","    </tr>\n","    <tr>\n","      <th>PBiasother</th>\n","      <td>12.992710</td>\n","      <td>706.473812</td>\n","      <td>46.423915</td>\n","    </tr>\n","    <tr>\n","      <th>KGE</th>\n","      <td>0.715133</td>\n","      <td>0.269494</td>\n","      <td>0.637340</td>\n","    </tr>\n","    <tr>\n","      <th>KGE12</th>\n","      <td>0.697526</td>\n","      <td>0.259366</td>\n","      <td>0.627730</td>\n","    </tr>\n","    <tr>\n","      <th>fdcRMSE</th>\n","      <td>0.935174</td>\n","      <td>2.819844</td>\n","      <td>1.801341</td>\n","    </tr>\n","    <tr>\n","      <th>lowRMSE</th>\n","      <td>0.094267</td>\n","      <td>0.178791</td>\n","      <td>0.148073</td>\n","    </tr>\n","    <tr>\n","      <th>highRMSE</th>\n","      <td>2.433919</td>\n","      <td>4.424166</td>\n","      <td>3.659977</td>\n","    </tr>\n","    <tr>\n","      <th>midRMSE</th>\n","      <td>0.271030</td>\n","      <td>0.415354</td>\n","      <td>0.365291</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               median         STD       mean\n","Bias         0.082842    0.358185   0.063822\n","RMSE         1.092191    1.031007   1.299537\n","ubRMSE       1.062194    1.002141   1.270872\n","Corr         0.873422    0.113820   0.838929\n","CorrSp       0.880218    0.099203   0.856538\n","R2           0.729054    0.383030   0.629756\n","NSE          0.729054    0.383030   0.629756\n","FLV         76.345316         NaN        inf\n","FHV         -6.393238   25.197306  -6.333314\n","PBias        9.792652   28.731551  11.954376\n","PBiasother  12.992710  706.473812  46.423915\n","KGE          0.715133    0.269494   0.637340\n","KGE12        0.697526    0.259366   0.627730\n","fdcRMSE      0.935174    2.819844   1.801341\n","lowRMSE      0.094267    0.178791   0.148073\n","highRMSE     2.433919    4.424166   3.659977\n","midRMSE      0.271030    0.415354   0.365291"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["mdstd"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>median</th>\n","      <th>STD</th>\n","      <th>mean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bias</th>\n","      <td>0.082842</td>\n","      <td>3.581848e-01</td>\n","      <td>6.382193e-02</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>1.092191</td>\n","      <td>1.031007e+00</td>\n","      <td>1.299537e+00</td>\n","    </tr>\n","    <tr>\n","      <th>ubRMSE</th>\n","      <td>1.062194</td>\n","      <td>1.002141e+00</td>\n","      <td>1.270872e+00</td>\n","    </tr>\n","    <tr>\n","      <th>Corr</th>\n","      <td>0.873422</td>\n","      <td>1.138202e-01</td>\n","      <td>8.389287e-01</td>\n","    </tr>\n","    <tr>\n","      <th>CorrSp</th>\n","      <td>0.880218</td>\n","      <td>9.920339e-02</td>\n","      <td>8.565381e-01</td>\n","    </tr>\n","    <tr>\n","      <th>R2</th>\n","      <td>0.729054</td>\n","      <td>3.830304e-01</td>\n","      <td>6.297559e-01</td>\n","    </tr>\n","    <tr>\n","      <th>NSE</th>\n","      <td>0.729054</td>\n","      <td>3.830304e-01</td>\n","      <td>6.297559e-01</td>\n","    </tr>\n","    <tr>\n","      <th>FLV</th>\n","      <td>76.345291</td>\n","      <td>1.985796e+07</td>\n","      <td>2.037864e+06</td>\n","    </tr>\n","    <tr>\n","      <th>FHV</th>\n","      <td>-6.393238</td>\n","      <td>2.519731e+01</td>\n","      <td>-6.333314e+00</td>\n","    </tr>\n","    <tr>\n","      <th>PBias</th>\n","      <td>9.792652</td>\n","      <td>2.873155e+01</td>\n","      <td>1.195438e+01</td>\n","    </tr>\n","    <tr>\n","      <th>PBiasother</th>\n","      <td>12.992710</td>\n","      <td>7.064738e+02</td>\n","      <td>4.642391e+01</td>\n","    </tr>\n","    <tr>\n","      <th>KGE</th>\n","      <td>0.715133</td>\n","      <td>2.694937e-01</td>\n","      <td>6.373397e-01</td>\n","    </tr>\n","    <tr>\n","      <th>KGE12</th>\n","      <td>0.697526</td>\n","      <td>2.593657e-01</td>\n","      <td>6.277301e-01</td>\n","    </tr>\n","    <tr>\n","      <th>fdcRMSE</th>\n","      <td>0.935174</td>\n","      <td>2.819844e+00</td>\n","      <td>1.801341e+00</td>\n","    </tr>\n","    <tr>\n","      <th>lowRMSE</th>\n","      <td>0.094267</td>\n","      <td>1.787908e-01</td>\n","      <td>1.480735e-01</td>\n","    </tr>\n","    <tr>\n","      <th>highRMSE</th>\n","      <td>2.433919</td>\n","      <td>4.424166e+00</td>\n","      <td>3.659977e+00</td>\n","    </tr>\n","    <tr>\n","      <th>midRMSE</th>\n","      <td>0.271030</td>\n","      <td>4.153538e-01</td>\n","      <td>3.652913e-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               median           STD          mean\n","Bias         0.082842  3.581848e-01  6.382193e-02\n","RMSE         1.092191  1.031007e+00  1.299537e+00\n","ubRMSE       1.062194  1.002141e+00  1.270872e+00\n","Corr         0.873422  1.138202e-01  8.389287e-01\n","CorrSp       0.880218  9.920339e-02  8.565381e-01\n","R2           0.729054  3.830304e-01  6.297559e-01\n","NSE          0.729054  3.830304e-01  6.297559e-01\n","FLV         76.345291  1.985796e+07  2.037864e+06\n","FHV         -6.393238  2.519731e+01 -6.333314e+00\n","PBias        9.792652  2.873155e+01  1.195438e+01\n","PBiasother  12.992710  7.064738e+02  4.642391e+01\n","KGE          0.715133  2.694937e-01  6.373397e-01\n","KGE12        0.697526  2.593657e-01  6.277301e-01\n","fdcRMSE      0.935174  2.819844e+00  1.801341e+00\n","lowRMSE      0.094267  1.787908e-01  1.480735e-01\n","highRMSE     2.433919  4.424166e+00  3.659977e+00\n","midRMSE      0.271030  4.153538e-01  3.652913e-01"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["mdstd"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Naive model wrapper\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import logging\n","from typing import Any, Optional, Tuple\n","\n","import numpy as np\n","import numpy.typing as npt\n","from pydantic import BaseModel, ConfigDict\n","import scipy.stats as stats\n","\n","log = logging.getLogger()\n","\n","\n","class Metrics(BaseModel):\n","    model_config = ConfigDict(arbitrary_types_allowed=True)\n","    pred: npt.NDArray[np.float64]\n","    target: npt.NDArray[np.float64]\n","    bias: npt.NDArray[np.float64] = np.ndarray([])\n","    rmse: npt.NDArray[np.float64] = np.ndarray([])\n","    ub_rmse: npt.NDArray[np.float64] = np.ndarray([])\n","    fdc_rmse: npt.NDArray[np.float64] = np.ndarray([])\n","    corr: npt.NDArray[np.float64] = np.ndarray([])\n","    corr_spearman: npt.NDArray[np.float64] = np.ndarray([])\n","    r2: npt.NDArray[np.float64] = np.ndarray([])\n","    nse: npt.NDArray[np.float64] = np.ndarray([])\n","    pbias_low: npt.NDArray[np.float64] = np.ndarray([])\n","    pbias_high: npt.NDArray[np.float64] = np.ndarray([])\n","    pbias: npt.NDArray[np.float64] = np.ndarray([])\n","    pbias_mid: npt.NDArray[np.float64] = np.ndarray([])\n","    kge: npt.NDArray[np.float64] = np.ndarray([])\n","    kge_12: npt.NDArray[np.float64] = np.ndarray([])\n","    rmse_low: npt.NDArray[np.float64] = np.ndarray([])\n","    rmse_high: npt.NDArray[np.float64] = np.ndarray([])\n","    rmse_mid: npt.NDArray[np.float64] = np.ndarray([])\n","    \n","    \n","    def __init__(self, pred: npt.NDArray[np.float64], target: npt.NDArray[np.float64]):\n","        super(Metrics, self).__init__(pred=pred, target=target)\n","\n","    def model_post_init(self, __context: Any):\n","        self.bias = self._bias(self.pred, self.target)\n","        self.rmse = self._rmse(self.pred, self.target)\n","        \n","        pred_mean = self.tile_mean(self.pred)\n","        target_mean = self.tile_mean(self.target)        \n","        pred_anom = self.pred - pred_mean\n","        target_anom = self.target - target_mean\n","        self.ub_rmse = self._rmse(pred_anom, target_anom)\n","        \n","        pred_fdc = self._calc_fdc(self.pred)\n","        target_fdc = self._calc_fdc(self.target)\n","        self.fdc_rmse = self._rmse(pred_fdc, target_fdc)\n","        \n","        self.corr = np.full(self.ngrid, np.nan)\n","        self.corr_spearman = np.full(self.ngrid, np.nan)\n","        self.r2 = np.full(self.ngrid, np.nan)\n","        self.nse = np.full(self.ngrid, np.nan)\n","        self.pbias_low = np.full(self.ngrid, np.nan)\n","        self.pbias_high = np.full(self.ngrid, np.nan)\n","        self.pbias = np.full(self.ngrid, np.nan)\n","        self.pbias_mid = np.full(self.ngrid, np.nan)\n","        self.kge = np.full(self.ngrid, np.nan)\n","        self.kge_12 = np.full(self.ngrid, np.nan)\n","        self.rmse_low = np.full(self.ngrid, np.nan)\n","        self.rmse_high = np.full(self.ngrid, np.nan)\n","        self.rmse_mid = np.full(self.ngrid, np.nan)\n","        for i in range(0, self.ngrid):\n","            _pred = self.pred[i]\n","            _target = self.target[i]\n","            non_nan_idx = np.where(\n","                np.logical_and(~np.isnan(_pred), ~np.isnan(_target))\n","            )[0]\n","            if non_nan_idx.shape[0] > 0:\n","                pred = _pred[non_nan_idx]\n","                target = _target[non_nan_idx]\n","\n","                pred_sort = np.sort(pred)\n","                target_sort = np.sort(target)\n","                index_low = round(0.3 * pred_sort.shape[0])\n","                index_high = round(0.98 * pred_sort.shape[0])\n","                low_pred = pred_sort[:index_low]\n","                high_pred = pred_sort[index_high:]\n","                mid_pred = pred_sort[index_low:index_high]\n","                low_target = target_sort[:index_low]\n","                high_target = target_sort[index_high:]\n","                mid_target = target_sort[index_low:index_high]\n","\n","                self.pbias[i] = self._p_bias(pred, target)\n","                self.pbias_low[i] = self._p_bias(low_pred, low_target)\n","                self.pbias_high[i] = self._p_bias(high_pred, high_target)\n","                self.pbias_mid[i] = self._p_bias(mid_pred, mid_target)\n","                self.rmse_low[i] = self._rmse(low_pred, low_target, axis=0)\n","                self.rmse_high[i] = self._rmse(high_pred, high_target, axis=0)\n","                self.rmse_mid[i] = self._rmse(mid_pred, mid_target, axis=0)\n","\n","                if non_nan_idx.shape[0] > 1:\n","                    self.corr[i] = self._corr(pred, target)\n","                    self.corr_spearman[i] = self._corr_spearman(pred, target)\n","                    _pred_mean = pred.mean()\n","                    _target_mean = target.mean()\n","                    _pred_std = np.std(pred)\n","                    _target_std = np.std(target)\n","                    self.kge[i] = self._kge(\n","                        _pred_mean, _target_mean, _pred_std, _target_std, self.corr[i]\n","                    )\n","                    self.kge_12[i] = self._kge_12(\n","                        _pred_mean, _target_mean, _pred_std, _target_std, self.corr[i]\n","                    )\n","                    self.nse[i], self.r2[i] = self._nse_r2(pred, target, _target_mean)\n","\n","    def _calc_fdc(self, data: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate flow duration curve for each grid\n","        \"\"\"\n","        fdc_100 = np.full([self.ngrid, 100], np.nan)\n","        for i in range(self.ngrid):\n","            data_slice = data[i]\n","            non_nan_data_slice = data_slice[~np.isnan(data_slice)]\n","            if len(non_nan_data_slice) == 0:\n","                non_nan_data_slice = np.full(self.nt, 0)\n","            sorted_data = np.sort(non_nan_data_slice)[::-1]\n","            Nlen = len(non_nan_data_slice)\n","            ind = (np.arange(100) / 100 * Nlen).astype(int)\n","            fdc_flow = sorted_data[ind]\n","            if len(fdc_flow) != 100:\n","                raise Exception(\"unknown assimilation variable\")\n","            else:\n","                fdc_100[i] = fdc_flow\n","        return fdc_100\n","\n","    @property\n","    def ngrid(self) -> int:\n","        \"\"\"\n","        Calculate number of grids\n","        \"\"\"\n","        return self.pred.shape[0]\n","\n","    @property\n","    def nt(self) -> int:\n","        \"\"\"\n","        Calculate number of time steps\n","        \"\"\"\n","        return self.pred.shape[1]\n","    \n","    def tile_mean(self, data: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate mean of target\n","        \"\"\"\n","        return np.tile(np.nanmean(data, axis=1), (self.nt, 1)).transpose()\n","\n","    @staticmethod\n","    def _rmse(\n","        pred: npt.NDArray[np.float64],\n","        target: npt.NDArray[np.float64],\n","        axis: Optional[int] = 1,\n","    ) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate root mean square error\n","        \"\"\"\n","        return np.sqrt(np.nanmean((pred - target) ** 2, axis=axis))\n","    \n","    @staticmethod\n","    def _bias(pred: npt.NDArray[np.float64], target: npt.NDArray[np.float64],) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate bias\n","        \"\"\"\n","        return np.nanmean((pred - target), axis=1)\n","\n","    @staticmethod\n","    def _p_bias(\n","        pred: npt.NDArray[np.float64], target: npt.NDArray[np.float64]\n","    ) -> np.float64:\n","        \"\"\"\n","        Calculate p bias\n","        \"\"\"\n","        p_bias = np.sum(pred - target) / np.sum(target) * 100\n","        return p_bias\n","\n","    @staticmethod\n","    def _corr(\n","        pred: npt.NDArray[np.float64], target: npt.NDArray[np.float64]\n","    ) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate correlation\n","        \"\"\"\n","        corr = stats.pearsonr(pred, target)[0]\n","        return corr\n","\n","    @staticmethod\n","    def _corr_spearman(\n","        pred: npt.NDArray[np.float64], target: npt.NDArray[np.float64]\n","    ) -> np.float64:\n","        \"\"\"\n","        Calculate spearman r\n","        \"\"\"\n","        corr_spearman = stats.spearmanr(pred, target)[0]\n","        return corr_spearman\n","\n","    @staticmethod\n","    def _kge(\n","        pred_mean: np.float64,\n","        target_mean: np.float64,\n","        pred_std: np.float64,\n","        target_std: np.float64,\n","        corr: np.float64,\n","    ) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate KGE\n","        \"\"\"\n","        kge = 1 - np.sqrt(\n","            (corr - 1) ** 2\n","            + (pred_std / target_std - 1) ** 2\n","            + (pred_mean / target_mean - 1) ** 2\n","        )\n","        return kge\n","\n","    @staticmethod\n","    def _kge_12(\n","        pred_mean: np.float64,\n","        target_mean: np.float64,\n","        pred_std: np.float64,\n","        target_std: np.float64,\n","        corr: np.float64,\n","    ) -> npt.NDArray[np.float64]:\n","        \"\"\"\n","        Calculate KGE 1-2\n","        \"\"\"\n","        kge_12 = 1 - np.sqrt(\n","            (corr - 1) ** 2\n","            + ((pred_std * target_mean) / (target_std * pred_mean) - 1) ** 2\n","            + (pred_mean / target_mean - 1) ** 2\n","        )\n","        return kge_12\n","\n","    @staticmethod\n","    def _nse_r2(\n","        pred: npt.NDArray[np.float64],\n","        target: npt.NDArray[np.float64],\n","        target_mean: np.float64,\n","    ) -> Tuple[np.float64, np.float64]:\n","        \"\"\"\n","        Calculate NSE/R2\n","        \"\"\"\n","        sst = np.sum((target - target_mean) ** 2)\n","        ssres = np.sum((target - pred) ** 2)\n","        r2 = 1 - ssres / sst\n","        nse = 1 - ssres / sst\n","        return nse, r2"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
