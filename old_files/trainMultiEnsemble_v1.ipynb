{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n",
      "Setting seed 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from mm_interface import master\n",
    "from mm_interface.train_test import test_models\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import create_output_dirs\n",
    "from MODELS.loss_functions.get_loss_function import get_lossFun\n",
    "# from mm_interface.train_test import test_differentiable_model, test_dp_hbv\n",
    "\n",
    "from hydroDL.master.master import calFDC\n",
    "from hydroDL.post.stat import statError\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# Settings for personal run.\n",
    "MODELS = ('dPLHBV_dyn', 'SACSMA_snow', 'marrmot_PRMS')  # HBV, dPLHBV_stat, dPLHBV_dyn, SACSMA, SACSMA_snow, PRMS \n",
    "ENSEMBLE_TYPE = 'max'  # max, median, avg, ensemble_softmax, ensemble_sigmoid, mosaid\n",
    "\n",
    "\n",
    "# Setting globals, dirs, randomseed\n",
    "device, dtype = master.set_globals()\n",
    "randomseed_config(0)\n",
    "out_dir = master.set_platform_dir()  # or set your alt path to model results here.\n",
    "\n",
    "# Setting dictionaries to separately manage each model's attributes.\n",
    "models, args_list = master.get_model_dict(MODELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting predictions, observations for dPLHBV (HydroDL).\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 20.615614414215088\n",
      "read usgs streamflow 20.214037656784058\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 19.95155096054077\n",
      "read usgs streamflow 19.89000153541565\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 19.91431212425232\n",
      "read usgs streamflow 19.820414066314697\n",
      "read usgs streamflow 24.167844533920288\n",
      "read master file D:\\code_repos\\water\\data\\model_runs\\rnnStreamflow\\CAMELSDemo/dPLHBV/ALL/TDTestforc/TD1_13/daymet/BuffOpt0/RMSE_para0.25/111111\\Fold1\\T_19801001_19951001_BS_100_HS_256_RHO_365_NF_13_Buff_365_Mul_16\\master.json\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "Setting seed 0\n",
      "Collecting predictions, observations for SACSMA_snow in batches of 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [06:45<00:00, 15.01s/Batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed 0\n",
      "Collecting predictions, observations for marrmot_PRMS in batches of 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [04:04<00:00,  9.05s/Batch]\n"
     ]
    }
   ],
   "source": [
    "# Forward individual hydro models and get their predictions.\n",
    "preds, y_obs, models = test_models(models, args_list, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_' + 'test2' + '_preds_obs')\n",
    "# master.save_output(args_list, preds, y_obs, out_dir=path)\n",
    "\n",
    "\n",
    "# If you've already run the trained models on train/test data, you can import\n",
    "# the set of model predictions and observed streamflows to save some time.\n",
    "pred_path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_train_preds_obs', 'HBV_dynamic_SACSMA_snow_marrmot_PRMS', 'preds_multim_E50_B25_R365_BT365_H256_tr1980_1995_n16.npy')\n",
    "obs_path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_train_preds_obs', 'HBV_dynamic_SACSMA_snow_marrmot_PRMS', 'obs_multim_E50_B25_R365_BT365_H256_tr1980_1995_n16.npy')\n",
    "preds = np.load(pred_path, allow_pickle=True).item()\n",
    "y_obs = np.load(obs_path, allow_pickle=True).item()\n",
    "\n",
    "preds['dPLHBV_dyn'] = preds.pop('HBV_dynamic')\n",
    "y_obs['dPLHBV_dyn'] = y_obs.pop('HBV_dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dPLHBV_dyn\n",
      "SACSMA_snow\n",
      "marrmot_PRMS\n"
     ]
    }
   ],
   "source": [
    "# Initializing\n",
    "flow_preds = []\n",
    "flow_obs = None\n",
    "obs_trig = False\n",
    "\n",
    "# Concatenate individual model predictions, and observation data.\n",
    "for i, mod in enumerate(args_list):\n",
    "    args = args_list[mod]\n",
    "    mod_out = preds[mod]\n",
    "    y_ob = y_obs[mod]\n",
    "\n",
    "    print(mod)\n",
    "\n",
    "    if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n",
    "        # Hydro models are tested in batches, so we concatenate them and select\n",
    "        # the desired flow.\n",
    "        # Note: modified HBV already has this preparation done during testing.\n",
    "\n",
    "        # Get flow predictions and swap axes to get shape [basins, days]\n",
    "        pred = np.swapaxes(torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1).squeeze().numpy(), 0, 1)\n",
    "\n",
    "        if obs_trig == False:\n",
    "            # dPLHBV uses GAGES while the other hydro models use CAMELS data. This means small\n",
    "            # e-5 variation in observation data between the two. This is averaged if both models\n",
    "            # are used, but to avoid double-counting data from multiply hydro models, use a trigger.\n",
    "            obs = np.swapaxes(y_ob[:, :, args[\"target\"].index(\"00060_Mean\")].numpy(), 0, 1)\n",
    "            obs_trig = True\n",
    "            dup = False\n",
    "        else:\n",
    "            dup = True\n",
    "\n",
    "    elif mod in ['dPLHBV_stat', 'dPLHBV_dyn']:\n",
    "        pred = mod_out[:,:,0][:,365:] # Set dim2 = 0 to get streamflow Qr\n",
    "        obs = y_ob.squeeze()[:,365:]\n",
    "        dup = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type in `models`.\")\n",
    "\n",
    "    if i == 0:\n",
    "        tmp_pred = pred\n",
    "        tmp_obs = obs\n",
    "    elif i == 1:\n",
    "        tmp_pred = np.stack((tmp_pred, pred), axis=2)\n",
    "        if not dup:\n",
    "            # Avoid double-counting GAGES obs.\n",
    "            tmp_obs = np.stack((tmp_obs, obs), axis=2)\n",
    "    else:\n",
    "        # Combine outputs of >3 models.\n",
    "        tmp_pred = np.concatenate((tmp_pred,np.expand_dims(pred, 2)), axis=2)\n",
    "        if not dup:\n",
    "            # Avoid double-counting GAGES obs.\n",
    "            tmp_obs = np.concatenate((tmp_obs,np.expand_dims(obs, 2)), axis=2)\n",
    "\n",
    "preds = tmp_pred\n",
    "obs = tmp_obs\n",
    "\n",
    "# Merge observation data.\n",
    "if len(obs.shape) == 3:\n",
    "    comp_obs = np.mean(obs, axis = 2)\n",
    "elif len(obs.shape) == 2:\n",
    "    comp_obs = obs\n",
    "else:\n",
    "    raise ValueError(\"Error reading prediction data: incorrect formatting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Weighting LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daymet tmean was used!\n",
      "read usgs streamflow 22.05735492706299\n",
      "read usgs streamflow 22.18307137489319\n",
      "read usgs streamflow 19.470875024795532\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 22.071636199951172\n",
      "read usgs streamflow 22.37849497795105\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from hydroDL import master, utils\n",
    "from hydroDL.data import camels\n",
    "from hydroDL.master import default\n",
    "from hydroDL.master.master import loadModel\n",
    "from hydroDL.model import rnn, crit, train\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "## fix the random seeds for reproducibility\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## GPU setting\n",
    "# which GPU to use when having multiple\n",
    "traingpuid = 0\n",
    "# torch.cuda.set_device(traingpuid)\n",
    "\n",
    "## Setting training options here\n",
    "PUOpt = 0\n",
    "# PUOpt values and explanations:\n",
    "# 0: train and test on ALL basins;\n",
    "# 1 for PUB spatial test, randomly hold out basins;\n",
    "# 2 for PUR spatial test, hold out a continuous region;\n",
    "buffOpt = 1\n",
    "# buffOpt defines the warm-up option for the first year of training forcing data\n",
    "# 0: do nothing, the first year forcing would only be used to warm up the next year;\n",
    "# 1: repeat first year forcing to warm up the first year;\n",
    "# 2: load one more year forcing to warm up the first year\n",
    "TDOpt = True\n",
    "# TDOpt, True as using dynamic parameters and False as using static parameters\n",
    "forType = 'daymet'\n",
    "# for Type defines which forcing in CAMELS to use: 'daymet', 'nldas', 'maurer'\n",
    "\n",
    "\n",
    "# used for multimodel ensembling.\n",
    "prcp_loss_factor = 23\n",
    "smooth_loss_factor = 0\n",
    "\n",
    "## Set hyperparameters\n",
    "EPOCH = 50 # total epoches to train the mode\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "saveEPOCH = 10\n",
    "Ttrain = [19801001, 19951001] # Training period\n",
    "# Ttrain = [19891001, 19991001] # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # Inversion period for historical forcings\n",
    "# Tinv = [19891001, 19991001] # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters. 12:original HBV; 13:includes the added dynamic ET para when setting ETMod=True\n",
    "BUFFTIME = 365 # for each training sample, to use BUFFTIME days to warm up the states.\n",
    "routing = True # Whether to use the routing module for simulated runoff\n",
    "Nmul = 16 # Multi-component model. How many parallel HBV components to use. 1 means the original HBV.\n",
    "comprout = False # True is doing routing for each component\n",
    "compwts = False # True is using weighted average for components; False is the simple mean\n",
    "pcorr = None # or a list to give the range of precip correction\n",
    "\n",
    "flow_regime = 0  # 1 is high flow expert.\n",
    "\n",
    "if TDOpt is True:\n",
    "    # Below options are only for running models with dynamic parameters\n",
    "    tdRep = [1, 13] # When using dynamic parameters, this list defines which parameters to set as dynamic\n",
    "    tdRepS = [str(ix) for ix in tdRep]\n",
    "    # ETMod: if True, use the added shape parameter (index 13) for ET. Default as False.\n",
    "    # Must set below ETMod as True and Nfea=13 when including 13 index in above tdRep list for dynamic parameters\n",
    "    # If 13 not in tdRep list, set below ETMod=False and Nfea=12 to use the original HBV without ET shape para\n",
    "    ETMod = True\n",
    "    Nfea = 13 # should be 13 when setting ETMod=True. 12 when ETMod=False\n",
    "    dydrop = 0.0 # dropout possibility for those dynamic parameters: 0.0 always dynamic; 1.0 always static\n",
    "    staind = -1 # which time step to use from the learned para time series for those static parameters\n",
    "    TDN = '/TDTestforc/'+'TD'+\"_\".join(tdRepS) +'/'\n",
    "else:\n",
    "    TDN = '/Testforc/'\n",
    "\n",
    "# Define root directory of database and output\n",
    "# Modify these based on your own location of CAMELS dataset\n",
    "# Following the data download instruction in README file, you should organize the folders like\n",
    "# 'your/path/to/Camels/basin_timeseries_v1p2_metForcing_obsFlow' and 'your/path/to/Camels/camels_attributes_v2.0'\n",
    "# Then 'rootDatabase' here should be 'your/path/to/Camels';\n",
    "# 'rootOut' is the root dir where you save the trained model\n",
    "\n",
    "\n",
    "\n",
    "# CAMELS dataset root directory\n",
    "# sysroot = '/Users/leoglonz/Desktop/water/data'\n",
    "# sysroot = '/content/'\n",
    "sysroot = 'D:\\code_repos\\water\\data'\n",
    "rootDatabase = os.path.join(os.path.sep, sysroot, 'Camels')    # CAMELS dataset root directory\n",
    "# rootDatabase = os.path.join(os.path.sep, 'data', 'kas7897', 'dPLHBVrelease')  # CAMELS dataset root directory\n",
    "camels.initcamels(flow_regime, forType=forType, rootDB=rootDatabase)  # initialize camels module-scope variables in camels.py (dirDB, gageDict) to read basin info\n",
    "\n",
    "# rootOut = os.path.join(os.path.sep, 'data', 'rnnStreamflow')  # Model output root directory\n",
    "rootOut = os.path.join(os.path.sep, sysroot, 'data', 'model_runs', 'hydro_multimodel_results', 'multimodels', '671_sites_dp', 'mm_ensemble_sigmoid')  # Model output root directory\n",
    "\n",
    "\n",
    "\n",
    "## If you have a checkpoint file for a previous run, set the path to directory here.\n",
    "# saved epoch is the epoch corresponding to the checkpoint.\n",
    "saved_epoch = 6\n",
    "checkpoint_file = None #'/Users/leoglonz/Desktop/water/data/model_runs/rnnStreamflow/CAMELSDemo/dPLHBV/ALL/TDTestforc/TD1_13/daymet/BuffOpt0/RMSE_para0.25/111111/Fold1/T_19801001_19951001_BS_100_HS_256_RHO_365_NF_13_Buff_365_Mul_16/'\n",
    "\n",
    "## set up different data loadings for ALL, PUB, PUR\n",
    "testfoldInd = 1\n",
    "# Which fold to hold out for PUB (10 folds, from 1 to 10) and PUR (7 folds, from 1 to 7).\n",
    "# It doesn't matter when training on ALL basins (setting PUOpt=0), could always set testfoldInd=1 for this case.\n",
    "\n",
    "# load CAMELS basin information\n",
    "gageinfo = camels.gageDict\n",
    "hucinfo = gageinfo['huc']\n",
    "gageid = gageinfo['id']\n",
    "gageidLst = gageid.tolist()\n",
    "\n",
    "if PUOpt == 0: # training on all basins without spatial hold-out\n",
    "    puN = 'ALL'\n",
    "    TrainLS = gageidLst # all basins\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    TestLS = gageidLst\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 1: # random hold out basins. hold out the fold set by testfoldInd\n",
    "    puN = 'PUB'\n",
    "    # load the PUB basin groups\n",
    "    # randomly divide CAMELS basins into 10 groups and this file contains the basin ID for each group\n",
    "    # located in splitPath\n",
    "    splitPath = 'PUBsplitLst.txt'\n",
    "    with open(splitPath, 'r') as fp:\n",
    "        testIDLst=json.load(fp)\n",
    "    # Generate training ID lists excluding the hold out fold\n",
    "    TestLS = testIDLst[testfoldInd - 1]\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS))\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 2:\n",
    "    puN = 'PUR'\n",
    "    # Divide CAMELS dataset into 7 continous PUR regions, as shown in Feng et al, 2021 GRL; 2022 HESSD\n",
    "    # get the id list of each PUR region, save to list\n",
    "    regionID = list()\n",
    "    regionNum = list()\n",
    "    # seven regions including different HUCs\n",
    "    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ]\n",
    "    for ii in range(len(regionDivide)):\n",
    "        tempcomb = regionDivide[ii]\n",
    "        tempregid = list()\n",
    "        for ih in tempcomb:\n",
    "            tempid = gageid[hucinfo==ih].tolist()\n",
    "            tempregid = tempregid + tempid\n",
    "        regionID.append(tempregid)\n",
    "        regionNum.append(len(tempregid))\n",
    "\n",
    "    iexp = testfoldInd - 1  #index\n",
    "    TestLS = regionID[iexp] # basin ID list for testing, hold out for training\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS)) # basin ID for training\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID': TrainLS, 'TestID': TestLS}\n",
    "\n",
    "\n",
    "# apply buffOPt to solve the warm-up for the first year\n",
    "if buffOpt ==2: # load more BUFFTIME data for the first year\n",
    "    sd = utils.time.t2dt(Ttrain[0]) - dt.timedelta(days=BUFFTIME)\n",
    "    sdint = int(sd.strftime(\"%Y%m%d\"))\n",
    "    TtrainLoad = [sdint, Ttrain[1]]\n",
    "    TinvLoad = [sdint, Ttrain[1]]\n",
    "else:\n",
    "    TtrainLoad = Ttrain\n",
    "    TinvLoad = Tinv\n",
    "\n",
    "## prepare input data\n",
    "## load camels dataset\n",
    "if forType == 'daymet':\n",
    "    varF = ['prcp', 'tmean']\n",
    "    varFInv = ['prcp', 'tmean']\n",
    "else:\n",
    "    varF = ['prcp', 'tmax'] # For CAMELS maurer and nldas forcings, tmax is actually tmean\n",
    "    varFInv = ['prcp', 'tmax']\n",
    "\n",
    "# the attributes used to learn parameters\n",
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "               'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "               'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "               'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "               'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
    "\n",
    "optData = default.optDataCamels # a default dictionary for logging, updated below\n",
    "# Update the training period and variables\n",
    "optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS, forType=forType)\n",
    "\n",
    "# for HBV model training inputs\n",
    "dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n",
    "forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "obsUN = dfTrain.getDataObs(doNorm=False, rmNan=False, basinnorm=False, flow_regime=flow_regime)\n",
    "\n",
    "# for dPL inversion data, inputs of gA\n",
    "dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "\n",
    "# Unit transformation, discharge obs from ft3/s to mm/day\n",
    "areas = gageinfo['area'][TrainInd] # unit km2\n",
    "temparea = np.tile(areas[:, None, None], (1, obsUN.shape[1],1))\n",
    "obsUN = (obsUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3 # transform to mm/day\n",
    "\n",
    "# load potential ET calculated by hargreaves method\n",
    "varLstNL = ['PEVAP']\n",
    "usgsIdLst = gageid\n",
    "if forType == 'maurer':\n",
    "    tPETRange = [19800101, 20090101]\n",
    "else:\n",
    "    tPETRange = [19800101, 20150101]\n",
    "tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "\n",
    "# Modify this as the directory where you put PET\n",
    "PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n",
    "ntime = len(tPETLst)\n",
    "PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "for k in range(len(usgsIdLst)):\n",
    "    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "    PETfull[k, :, :] = dataTemp\n",
    "\n",
    "TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "PETUN = PETfull[:, ind2, :]\n",
    "PETUN = PETUN[TrainInd, :, :] # select basins\n",
    "C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "PETInvUN = PETfull[:, ind2inv, :]\n",
    "PETInvUN = PETInvUN[TrainInd, :, :]\n",
    "\n",
    "# process data, do normalization and remove nan\n",
    "series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n",
    "seriesvarLst = varFInv + ['pet']\n",
    "# calculate statistics for normalization and saved to a dictionary\n",
    "statDict = camels.getStatDic(attrLst=attrnewLst, attrdata=attrsUN, seriesLst=seriesvarLst, seriesdata=series_inv, flow_regime=flow_regime)\n",
    "# normalize data\n",
    "attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict, toNorm=True, flow_regime=flow_regime)\n",
    "attr_norm[np.isnan(attr_norm)] = 0.0\n",
    "series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True, flow_regime=flow_regime)\n",
    "series_norm[np.isnan(series_norm)] = 0.0\n",
    "\n",
    "# prepare the inputs\n",
    "zTrain = series_norm # used as the inputs for dPL inversion gA along with attributes\n",
    "xTrain = np.concatenate([forcUN, PETUN], axis=2) # used as HBV forcing\n",
    "xTrain[np.isnan(xTrain)] = 0.0\n",
    "\n",
    "if buffOpt == 1: # repeat the first year warm up the first year itself\n",
    "    zTrainIn = np.concatenate([zTrain[:,0:BUFFTIME,:], zTrain], axis=1)\n",
    "    xTrainIn = np.concatenate([xTrain[:,0:BUFFTIME,:], xTrain], axis=1) # repeat forcing to warm up the first year\n",
    "    yTrainIn = np.concatenate([obsUN[:,0:BUFFTIME,:], obsUN], axis=1)\n",
    "else: # no repeat, original data, the first year data would only be used as warmup for the next following year\n",
    "    zTrainIn = zTrain\n",
    "    xTrainIn = xTrain\n",
    "    yTrainIn = obsUN\n",
    "\n",
    "forcTuple = (xTrainIn, zTrainIn)\n",
    "attrs = attr_norm\n",
    "\n",
    "## Train the model\n",
    "# define loss function\n",
    "alpha = 0.25 # a weight for RMSE loss to balance low and peak flow\n",
    "optLoss = default.update(default.optLossComb, name='hydroDL.model.crit.RmseLossComb', weight=alpha)\n",
    "lossFun = crit.RmseLossComb(alpha=alpha)\n",
    "\n",
    "# define training options\n",
    "optTrain = default.update(default.optTrainCamels, miniBatch=[BATCH_SIZE, RHO], nEpoch=EPOCH, saveEpoch=saveEPOCH)\n",
    "# define output folder to save model results\n",
    "exp_name = 'CAMELSDemo'\n",
    "exp_disp = 'dPLHBV/' + puN + TDN + forType + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "           '/Fold' + str(testfoldInd)\n",
    "exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n",
    "           +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n",
    "save_path = os.path.join(exp_name, exp_disp)\n",
    "out = os.path.join(rootOut, save_path, exp_info) # output folder to save results\n",
    "# define and load model\n",
    "Ninv = zTrain.shape[-1] + attrs.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmm_interface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_test\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CudnnLstmModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmm_interface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_functional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m range_bound_loss\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnsembleWeights\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, ninv, hiddeninv, drinv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, nmodels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28msuper\u001b[39m(EnsembleWeights, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from mm_interface.train_test import CudnnLstmModel\n",
    "from mm_interface.mm_functional import range_bound_loss\n",
    "\n",
    "class EnsembleWeights(torch.nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, nmodels=1):\n",
    "        super(EnsembleWeights, self).__init__()\n",
    "        self.name = 'EnsembleWeights'\n",
    "        self.ninv = ninv\n",
    "        self.nmodels = nmodels\n",
    "\n",
    "        ntp = nmodels\n",
    "        self.hiddeninv = hiddeninv\n",
    "\n",
    "        self.lstminv = CudnnLstmModel(\n",
    "            nx=ninv, ny=ntp, hiddenSize=hiddeninv, dr=drinv)\n",
    "        lb_prcp = [0.95]\n",
    "        ub_prcp = [1.05]\n",
    "        self.RangeBoundLoss = range_bound_loss(lb=lb_prcp, ub=ub_prcp)\n",
    "\n",
    "    def forward(self, x, z, prcp_loss_factor):\n",
    "        z.requires_grad = True\n",
    "\n",
    "        wghts = self.lstminv(z)\n",
    "        ntstep = wghts.shape[0]\n",
    "        ngage = wghts.shape[1]\n",
    "        wghts_scaled = torch.sigmoid(wghts)  # Change weighting method here.\n",
    "\n",
    "        prcp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        prcp_wghts_sum = torch.sum(wghts_scaled, dim=2)\n",
    "        range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)\n",
    "\n",
    "        # print(prcp_wavg.shape, wghts_scaled.shape, x.shape, z.shape)\n",
    "\n",
    "        for para in range(wghts.shape[2]):\n",
    "            prcp_wavg = prcp_wavg + wghts_scaled[:, :, para] * x[:, :, para]\n",
    "\n",
    "        x_new = torch.empty((ntstep, ngage, 3), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        # x_new[:, :, 0] = prcp_wavg\n",
    "        # x_new[:, :, 1] = x[:, :, self.nmodels]\n",
    "        # x_new[:, :, 2] = x[:, :, -1]\n",
    "\n",
    "        # For gradient analysis.\n",
    "        # grad_daymet = autograd.grad(outputs=wghts_scaled[:, :, 0], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 0]), retain_graph=True)[0]\n",
    "        # grad_maurer = autograd.grad(outputs=wghts_scaled[:, :, 1], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 1]), retain_graph=True)[0]\n",
    "        # grad_nldas = autograd.grad(outputs=wghts_scaled[:, :, 2], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 2]), retain_graph=True)[0]\n",
    "\n",
    "        # return x_new, range_bound_loss_prcp, wghts_scaled, grad_daymet, grad_maurer, grad_nldas\n",
    "        return x_new, range_bound_loss_prcp, wghts_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn.prcp_weights(ninv=Ninv, hiddeninv=HIDDENSIZE, prcp_datatypes=len(forType))\n",
    "\n",
    "# dict only for logging\n",
    "optModel = OrderedDict(name='LSTM-dPLHBV', nx=Ninv, nfea=Nfea, nmul=Nmul, hiddenSize=HIDDENSIZE, doReLU=True,\n",
    "                        Tinv=Tinv, Trainbuff=BUFFTIME, routOpt=routing, comprout=comprout, compwts=compwts,\n",
    "                        pcorr=pcorr, staind=staind, tdlst=tdRep, dydrop=dydrop,buffOpt=buffOpt, TDOpt=TDOpt, ETMod=ETMod)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGML_STemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
