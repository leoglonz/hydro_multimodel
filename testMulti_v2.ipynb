{"cells":[{"cell_type":"markdown","metadata":{"id":"q1jGJFeaF0oG"},"source":["### Running bulk of multimodel testing\n","\n","This is equivalent to that present in the multimodel wrapper.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1832,"status":"ok","timestamp":1707705234766,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"2Vt-fsprDO_u","outputId":"c300c26f-3e5f-4134-c3f5-a0182475218a"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading package hydroDL\n"]}],"source":["from config.read_configurations import config_hbv as hbvArgs\n","from config.read_configurations import config_prms as prmsArgs\n","from config.read_configurations import config_sacsma as sacsmaArgs\n","from config.read_configurations import config_sacsma_snow as sacsmaSnowArgs\n","from config.read_configurations import config_hbv_hydrodl as hbvhyArgs_d\n","\n","\n","import torch\n","import os\n","import platform\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import scipy.stats\n","# from post import plot\n","\n","from core.utils.randomseed_config import randomseed_config\n","from core.utils.master import create_output_dirs\n","from MODELS.loss_functions.get_loss_function import get_lossFun\n","from MODELS.test_dp_HBV_dynamic import test_dp_hbv\n","from core.data_processing.data_loading import loadData\n","from core.data_processing.normalization import transNorm\n","from core.utils.randomseed_config import randomseed_config\n","from core.data_processing.model import (\n","    take_sample_test,\n","    converting_flow_from_ft3_per_sec_to_mm_per_day\n",")\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","\n","# Set path to `hydro_multimodel_results` directory.\n","if platform.system() == 'Darwin':\n","    # For mac os\n","    out_dir = '/Users/leoglonz/Desktop/water/data/model_runs/hydro_multimodel_results'\n","    # Some operations are not yet working with MPS, so we might need to set some environment variables to use CPU fall instead\n","    # %env PYTORCH_ENABLE_MPS_FALLBACK=1\n","\n","elif platform.system() == 'Windows':\n","    # For windows\n","    out_dir = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n","\n","elif platform.system() == 'Linux':\n","    # For Colab\n","    out_dir = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results'\n","\n","else:\n","    raise ValueError('Unsupported operating system.')\n","\n","\n","##-----## Multi-model Parameters ##-----##\n","##--------------------------------------##\n","# Setting dictionaries to separately manage each diff model's attributes.\n","models = {'dPLHBV_dyn': None, 'SACSMA_snow':None, 'marrmot_PRMS':None}  # 'HBV':None, 'hbvhy': None, 'SACSMA_snow':None, 'SACSMA':None,\n","args_list = {'dPLHBV_dyn': hbvhyArgs_d, 'SACSMA_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\n","ENSEMBLE_TYPE = 'max'  # 'median', 'avg', 'max', 'softmax'\n","\n","# Load test observations and predictions from a prior run.\n","pred_path = os.path.join(out_dir, 'multimodels', '671_sites_dp', 'output', 'preds_671_dPLHBVd_SACSMASnow_PRMS.npy')\n","obs_path = os.path.join(out_dir, 'multimodels', '671_sites_dp', 'output', 'obs_671_dPLHBVd_SACSMASnow_PRMS.npy')\n","preds = np.load(pred_path, allow_pickle=True).item()\n","obs = np.load(obs_path, allow_pickle=True).item()\n","\n","model_output = preds\n","y_obs = obs"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["(5114, 671)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["sac = y_obs['SACSMA_snow'].squeeze().numpy()\n","per = y_obs['marrmot_PRMS'].squeeze().numpy()\n","\n","per.shape"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["np.array_equal(sac,per, equal_nan=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":216,"status":"ok","timestamp":1707706633956,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"typgVyc9DO_z"},"outputs":[],"source":["def calFDC(data):\n","    # data = Ngrid * Nday\n","    Ngrid, Nday = data.shape\n","    FDC100 = np.full([Ngrid, 100], np.nan)\n","    for ii in range(Ngrid):\n","        tempdata0 = data[ii, :]\n","        tempdata = tempdata0[~np.isnan(tempdata0)]\n","        # deal with no data case for some gages\n","        if len(tempdata)==0:\n","            tempdata = np.full(Nday, 0)\n","        # sort from large to small\n","        temp_sort = np.sort(tempdata)[::-1]\n","        # select 100 quantile points\n","        Nlen = len(tempdata)\n","        ind = (np.arange(100)/100*Nlen).astype(int)\n","        FDCflow = temp_sort[ind]\n","        if len(FDCflow) != 100:\n","            raise Exception('unknown assimilation variable')\n","        else:\n","            FDC100[ii, :] = FDCflow\n","\n","    return FDC100\n","\n","\n","def statError(pred, target):\n","    ngrid, nt = pred.shape\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n","    # Bias\n","        Bias = np.nanmean(pred - target, axis=1)\n","        # RMSE\n","        RMSE = np.sqrt(np.nanmean((pred - target)**2, axis=1))\n","        # ubRMSE\n","        predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n","        targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n","        predAnom = pred - predMean\n","        targetAnom = target - targetMean\n","        ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom)**2, axis=1))\n","        # FDC metric\n","        predFDC = calFDC(pred)\n","        targetFDC = calFDC(target)\n","        FDCRMSE = np.sqrt(np.nanmean((predFDC - targetFDC) ** 2, axis=1))\n","    # rho R2 NSE\n","        Corr = np.full(ngrid, np.nan)\n","        CorrSp = np.full(ngrid, np.nan)\n","        R2 = np.full(ngrid, np.nan)\n","        NSE = np.full(ngrid, np.nan)\n","        PBiaslow = np.full(ngrid, np.nan)\n","        PBiashigh = np.full(ngrid, np.nan)\n","        PBias = np.full(ngrid, np.nan)\n","        PBiasother = np.full(ngrid, np.nan)\n","        KGE = np.full(ngrid, np.nan)\n","        KGE12 = np.full(ngrid, np.nan)\n","        RMSElow = np.full(ngrid, np.nan)\n","        RMSEhigh = np.full(ngrid, np.nan)\n","        RMSEother = np.full(ngrid, np.nan)\n","        for k in range(0, ngrid):\n","            x = pred[k, :]\n","            y = target[k, :]\n","            ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n","            if ind.shape[0] > 0:\n","                xx = x[ind]\n","                yy = y[ind]\n","                # percent bias\n","                PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n","\n","                # FHV the peak flows bias 2%\n","                # FLV the low flows bias bottom 30%, log space\n","                pred_sort = np.sort(xx)\n","                target_sort = np.sort(yy)\n","                indexlow = round(0.3 * len(pred_sort))\n","                indexhigh = round(0.98 * len(pred_sort))\n","                lowpred = pred_sort[:indexlow]\n","                highpred = pred_sort[indexhigh:]\n","                otherpred = pred_sort[indexlow:indexhigh]\n","                lowtarget = target_sort[:indexlow]\n","                hightarget = target_sort[indexhigh:]\n","                othertarget = target_sort[indexlow:indexhigh]\n","                PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n","                PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n","                PBiasother[k] = np.sum(otherpred - othertarget) / np.sum(othertarget) * 100\n","                RMSElow[k] = np.sqrt(np.nanmean((lowpred - lowtarget)**2))\n","                RMSEhigh[k] = np.sqrt(np.nanmean((highpred - hightarget)**2))\n","                RMSEother[k] = np.sqrt(np.nanmean((otherpred - othertarget)**2))\n","\n","                if ind.shape[0] > 1:\n","                    # Theoretically at least two points for correlation\n","                    Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n","                    CorrSp[k] = scipy.stats.spearmanr(xx, yy)[0]\n","                    yymean = yy.mean()\n","                    yystd = np.std(yy)\n","                    xxmean = xx.mean()\n","                    xxstd = np.std(xx)\n","                    KGE[k] = 1 - np.sqrt((Corr[k]-1)**2 + (xxstd/yystd-1)**2 + (xxmean/yymean-1)**2)\n","                    KGE12[k] = 1 - np.sqrt((Corr[k] - 1) ** 2 + ((xxstd*yymean)/ (yystd*xxmean) - 1) ** 2 + (xxmean / yymean - 1) ** 2)\n","                    SST = np.sum((yy-yymean)**2)\n","                    SSReg = np.sum((xx-yymean)**2)\n","                    SSRes = np.sum((yy-xx)**2)\n","                    R2[k] = 1-SSRes/SST\n","                    NSE[k] = 1-SSRes/SST\n","\n","    outDict = dict(Bias=Bias, RMSE=RMSE, ubRMSE=ubRMSE, Corr=Corr, CorrSp=CorrSp, R2=R2, NSE=NSE,\n","                   FLV=PBiaslow, FHV=PBiashigh, PBias=PBias, PBiasother=PBiasother, KGE=KGE, KGE12=KGE12, fdcRMSE=FDCRMSE,\n","                   lowRMSE=RMSElow, highRMSE=RMSEhigh, midRMSE=RMSEother)\n","\n","    return outDict"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":126,"status":"ok","timestamp":1707706633957,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"RtdVYl00DO_z"},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mhydroEnsemble\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Wrapper for multiple hydrologic models.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# In future, consider just passing the models you want to ensemble explicitly.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_models, hidden_size, num_layers):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(hydroEnsemble, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["class hydroEnsemble(torch.nn.Module):\n","    # Wrapper for multiple hydrologic models.\n","    # In future, consider just passing the models you want to ensemble explicitly.\n","    def __init__(self, num_models, hidden_size, num_layers):\n","        super(hydroEnsemble, self).__init__()\n","\n","        self.lstm = torch.nn.LSTM(num_models, hidden_size, num_layers, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_size, num_models)  # Two models (modelA and modelB)\n","\n","        # self.modelA = modelA\n","        # self.modelB = modelB\n","        # self.classifier = torch.nn.Linear(4, 2)\n","\n","    def forward(self, x):\n","        # x is the input sequence tensor with shape (batch_size, sequence_length, num_models)\n","\n","        # Setting randomseed for deterministic output.\n","        randomseed_config(0)\n","\n","        # Add batch dimension to input and convert to tensor.\n","        x_exp = x.unsqueeze(0)\n","\n","        # LSTM layer\n","        lstm_out, _ = self.lstm(x_exp)\n","\n","        # Fully connected layer\n","        fc_out = self.fc(lstm_out)\n","\n","        # Apply softmax activation to obtain weights\n","        weights = torch.nn.functional.softmax(fc_out, dim=2).squeeze()\n","\n","        # Weighted combination of predictions.\n","        weighted_preds = np.multiply(weights.detach(), x)\n","\n","        # Or take the max weight and return the corresponding value.\n","        max_vals, _ = torch.max(weights, dim=1)\n","        btensor = torch.zeros_like(weights)\n","        btensor[weights==max_vals.view(-1,1)] = 1\n","        weighted_preds = np.multiply(btensor.detach(), x)\n","\n","        preds = torch.sum(weighted_preds, dim=1)\n","\n","        # All tensors\n","        # return preds, weights, weighted_preds\n","        return preds\n"]},{"cell_type":"code","execution_count":378,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1707706634441,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"uJhJHca9DO_0"},"outputs":[],"source":["def calculate_metrics_multi(args_list, model_outputs, y_obs_list, ensemble_type='max', out_dir=None):\n","    \"\"\"\n","    Calculate stats for a multimodel ensemble.\n","    \"\"\"\n","    stats_list = dict()\n","\n","    for mod in args_list:\n","        args = args_list[mod]\n","        mod_out = model_outputs[mod]\n","        y_obs = y_obs_list[mod]\n","\n","        if mod in ['SACSMA', 'SACSMA_snow', 'marrmot_PRMS', 'HBV']:\n","            # Note for hydrodl HBV, calculations have already been done so skip.\n","\n","            # Saving data\n","            if out_dir:\n","                path = os.path.join(out_dir, 'models', '671_sites_dp', mod)\n","                if not os.path.exists(path):\n","                    os.makedirs(path, exist_ok=True)\n","\n","                # Test data (obs and model results).\n","                for key in mod_out[0].keys():\n","                    if len(mod_out[0][key].shape) == 3:\n","                        dim = 1\n","                    else:\n","                        dim = 0\n","                    concatenated_tensor = torch.cat([d[key] for d in mod_out], dim=dim)\n","                    file_name = key + \".npy\"\n","                    np.save(os.path.join(path, file_name), concatenated_tensor.numpy())\n","                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n","\n","                # Reading and flow observations.\n","                print(args['target'])\n","                for var in args[\"target\"]:\n","                    item_obs = y_obs[:, :, args[\"target\"].index(var)]\n","                    file_name = var + \".npy\"\n","                    np.save(os.path.join(path, file_name), item_obs)\n","                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n","\n","\n","            ###################### calculations here ######################\n","            pred_list = list()\n","            obs_list = list()\n","            flow_sim = torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1)\n","            flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n","\n","            pred_list.append(flow_sim.numpy())\n","            obs_list.append(np.expand_dims(flow_obs, 2))\n","            \n","\n","            # we need to swap axes here to have [basin, days], and remove redundant\n","            # dimensions with np.squeeze().\n","            stats_list[mod] = [\n","                statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n","                for (x, y) in zip(pred_list, obs_list)\n","            ]\n","        elif mod in ['hbvhy', 'dPLHBV_dyn']:\n","            stats_list[mod] = [statError(mod_out[:,:,0], y_obs.squeeze())]\n","        else:\n","            raise ValueError(f\"Unsupported model type in `models`.\")\n","\n","    # Calculating final statistics for the whole set of basins.\n","    name_list = [\"flow\"]\n","    for st, name in zip(stats_list[mod], name_list):\n","        count = 0\n","        mdstd = np.zeros([len(st), 3])\n","        for key in st.keys():\n","            # st contains the statistics on a model run like NSE and KGE.\n","            # Find the best result (e.g., the max, avg, median) and merge from each model.\n","            for i, mod in enumerate(args_list):\n","                if i == 0:\n","                    # temp contains the values of key per basin.\n","                    temp = stats_list[mod][0][key]\n","                    continue\n","                elif i == 1:\n","                    temp = np.stack((temp, stats_list[mod][0][key]), axis=1)\n","                else:\n","                    temp = np.hstack((temp, stats_list[mod][0][key].reshape(-1,1)))\n","\n","            print(temp, temp.shape)\n","            \n","            if len(args_list) > 1:\n","                if ensemble_type == 'max':\n","                    # print(temp, key)\n","                    temp = np.amax(temp, axis=1)\n","                    # print(temp, key)\n","                elif ensemble_type == 'avg':\n","                    temp = np.mean(temp, axis=1)\n","                elif ensemble_type == 'median':\n","                    temp = np.median(temp, axis=1)\n","                elif ensemble_type == 'softmax':\n","                    # # Softmax gets relative contributions of each model.\n","                    # weights = torch.nn.functional.softmax(torch.from_numpy(temp), dim=1)\n","                    # temp = np.sum(temp * weights.numpy(), axis=1)\n","\n","                    # Instantiate weighting lstm with softmax.\n","                    lstm = hydroEnsemble(num_models=len(args_list), hidden_size=192, num_layers=3)\n","                    # Forward pass through the model\n","                    temp = lstm(torch.tensor(temp, dtype=torch.float))\n","                else:\n","                    raise ValueError(\"Invalid model ensemble type specified.\")\n","\n","            median = np.nanmedian(temp)  # abs(i)\n","            std = np.nanstd(temp)  # abs(i)\n","            mean = np.nanmean(temp)  # abs(i)\n","            k = np.array([[median, std, mean]])\n","            mdstd[count] = k\n","            count = count + 1\n","\n","        # mdstd displays the statistics for each error measure in stats_list.\n","        mdstd = pd.DataFrame(\n","            mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n","        )\n","        # Save the data stats from the training run:\n","        if out_dir and len(args_list) > 1:\n","            path = os.path.join(out_dir, 'multimodels', '671_sites_dp', 'n_' + ensemble_type)\n","            if not os.path.exists(path):\n","                os.makedirs(path, exist_ok=True)\n","\n","            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + \"_\" + ensemble_type +\".csv\")))\n","        elif out_dir:\n","            path = os.path.join(out_dir, 'models', '671_sites_dp', args_list[0])\n","            if not os.path.exists(path):\n","                os.makedirs(path, exist_ok=True)\n","\n","            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + \"_\" + \".csv\")))\n","        else: continue\n","\n","    # Show boxplots of the results\n","    # plt.rcParams[\"font.size\"] = 14\n","    # keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n","    # dataBox = list()\n","    # for iS in range(len(keyLst)):\n","    #     statStr = keyLst[iS]\n","    #     temp = list()\n","    #     # for k in range(len(st)):\n","    #     data = st[statStr]\n","    #     data = data[~np.isnan(data)]\n","    #     temp.append(data)\n","    #     dataBox.append(temp)\n","    # labelname = [\n","    #     \"Hybrid differentiable model\"\n","    # ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n","\n","    # xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n","    # fig = plot.plotBoxFig(\n","    #     dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n","    # )\n","    # fig.patch.set_facecolor(\"white\")\n","    # boxPlotName = \"PGML\"\n","    # fig.suptitle(boxPlotName, fontsize=12)\n","    # plt.rcParams[\"font.size\"] = 12\n","    # # plt.savefig(\n","    # #     os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n","    # # )  # , dpi=500\n","    # # fig.show()\n","    # plt.close()\n","\n","    torch.cuda.empty_cache()\n","    print(\"Testing ended\")\n","\n","    return stats_list, mdstd\n"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["args_list = {'dPLHBV_dyn': hbvhyArgs_d, 'SACSMA_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}\n","\n","# Initialize\n","flow_preds = []\n","flow_obs = None\n","obs_trig = False\n","\n","# Concatenate individual model predictions, and observation data.\n","for i, mod in enumerate(args_list):\n","    args = args_list[mod]\n","    mod_out = model_output[mod]\n","    y_ob = y_obs[mod]\n","\n","    if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n","        # Hydro models are tested in batches, so we concatenate them and select\n","        # the desired flow.\n","        # Note: modified HBV already has this preparation done during testing.\n","\n","        # Get flow predictions and swap axes to get shape [basins, days]\n","        pred = np.swapaxes(torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1).squeeze().numpy(), 0, 1)\n","\n","        if obs_trig == False:\n","            # dPLHBV uses GAGES while the other hydro models use CAMELS data. This means small \n","            # e-5 variation in observation data between the two. This is averaged if both models\n","            # are used, but to avoid double-counting data from multiply hydro models, use a trigger.\n","            obs = np.swapaxes(y_ob[:, :, args[\"target\"].index(\"00060_Mean\")].numpy(), 0, 1)\n","            obs_trig = True\n","            dup = False\n","        else:\n","            dup = True\n","            \n","\n","    elif mod in ['dPLHBV_dyn']:\n","        pred = mod_out[:,:,0][:,365:] # Set dim2 = 0 to get streamflow Qr\n","        obs = y_ob.squeeze()[:,365:]\n","        dup = False\n","\n","    else:\n","        raise ValueError(f\"Unsupported model type in `models`.\")\n","    \n","    if i == 0:\n","        tmp_pred = pred\n","        tmp_obs = obs\n","    elif i == 1:\n","        tmp_pred = np.stack((tmp_pred, pred), axis=2)\n","        if not dup:\n","            # Avoid double-counting GAGES obs.\n","            tmp_obs = np.stack((tmp_obs, obs), axis=2)\n","    else:\n","        # Combine outputs of >3 models.\n","        tmp_pred = np.concatenate((tmp_pred,np.expand_dims(pred, 2)), axis=2)\n","        if not dup:\n","            # Avoid double-counting GAGES obs.\n","            tmp_obs = np.concatenate((tmp_obs,np.expand_dims(obs, 2)), axis=2)\n","\n"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35429,"status":"ok","timestamp":1707706669822,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"gyETa7dZDO_0","outputId":"41921b5c-c2b8-46cc-eef1-3f0d4dd3b21d"},"outputs":[{"ename":"NameError","evalue":"name 'calculate_metrics_multi' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[94], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m args_list \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdPLHBV_dyn\u001b[39m\u001b[38;5;124m'\u001b[39m: hbvhyArgs_d,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSACSMA_snow\u001b[39m\u001b[38;5;124m'\u001b[39m:sacsmaSnowArgs}   \u001b[38;5;66;03m# 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ENSEMBLE_TYPE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 'median', 'avg', 'max', 'softmax'\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m stats_list, mtstd \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics_multi\u001b[49m(args_list, model_outputs\u001b[38;5;241m=\u001b[39mmodel_output, y_obs_list\u001b[38;5;241m=\u001b[39my_obs, ensemble_type\u001b[38;5;241m=\u001b[39mENSEMBLE_TYPE)\n\u001b[1;32m      6\u001b[0m mtstd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'calculate_metrics_multi' is not defined"]}],"source":["# models = {'SACSMA':None, 'marrmot_PRMS':None}  # 'HBV':None\n","args_list = {'dPLHBV_dyn': hbvhyArgs_d,'SACSMA_snow':sacsmaSnowArgs}   # 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\n","ENSEMBLE_TYPE = 'avg'  # 'median', 'avg', 'max', 'softmax'\n","stats_list, mtstd = calculate_metrics_multi(args_list, model_outputs=model_output, y_obs_list=y_obs, ensemble_type=ENSEMBLE_TYPE)\n","\n","mtstd['median']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
