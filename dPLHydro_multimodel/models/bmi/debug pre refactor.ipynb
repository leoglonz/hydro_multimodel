{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package_path = '/data/lgl5139/hydro_multimodel/dPLHydro_multimodel'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "\n",
    "from core.data.dataset_loading import get_data_dict\n",
    "dataset_dict, _ = get_data_dict(model.config, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config['test']['end_time'] = \"1995/10/05\"\n",
    "model.config['rho'] = 0\n",
    "model.config['routing_hydro_model'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_sample_test(config: Dict, dataset_dictionary: Dict[str, torch.Tensor], \n",
    "                     i_s: int, i_e: int) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Take sample of data for testing batch.\n",
    "    \"\"\"\n",
    "    dataset_sample = {}\n",
    "    for key, value in dataset_dictionary.items():\n",
    "        if value.ndim == 3:\n",
    "            # TODO: I don't think we actually need this.\n",
    "            # Remove the warmup period for all except airtemp_memory and hydro inputs.\n",
    "            if key in ['airT_mem_temp_model', 'x_hydro_model', 'inputs_nn_scaled']:\n",
    "                warm_up = 0\n",
    "            else:\n",
    "                warm_up = config['warm_up']\n",
    "            dataset_sample[key] = value[:5, i_s:i_e, :].to(config['device'])\n",
    "        elif value.ndim == 2:\n",
    "            dataset_sample[key] = value[i_s:i_e, :].to(config['device'])\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect input dimensions. {key} array must have 2 or 3 dimensions.\")\n",
    "    return dataset_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors:\n",
    "dataset_dict_torch = {}\n",
    "\n",
    "for key in dataset_dict.keys():\n",
    "    if type(dataset_dict[key]) == np.ndarray:\n",
    "        dataset_dict_torch[key] = torch.from_numpy(dataset_dict[key]).float() #.to(self.config['device'])\n",
    "\n",
    "ngrid = dataset_dict_torch['inputs_nn_scaled'].shape[1]\n",
    "iS = np.arange(0, ngrid, model.config['batch_basins'])\n",
    "iE = np.append(iS[1:], ngrid)\n",
    "\n",
    "sample = take_sample_test(model.config, dataset_dict_torch, iS[0], iE[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "upper bound and larger bound inconsistent with step sign",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/model_handler.py:90\u001b[0m, in \u001b[0;36mModelHandler.forward\u001b[0;34m(self, dataset_dict_sample, eval)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# self.model_dict[mod].eval()  # For testing.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_out_dict[mod] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/differentiable_model.py:94\u001b[0m, in \u001b[0;36mdPLHydroModel.forward\u001b[0;34m(self, dataset_dict_sample)\u001b[0m\n\u001b[1;32m     91\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreakdown_params(params_all)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Hydro model\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m flow_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydro_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dict_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dict_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhydro_params_raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwarm_up\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrouting_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_params_hydro\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv_params_hydro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Baseflow index percentage\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m## Using two deep groundwater buckets: gwflow & bas_shallow\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbas_shallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m flow_out\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/hydro_models/HBV/HBVmul.py:305\u001b[0m, in \u001b[0;36mHBVMul.forward\u001b[0;34m(self, x_hydro_model, c_hydro_model, params_raw, args, muwts, warm_up, init, routing, comprout, conv_params_hydro)\u001b[0m\n\u001b[1;32m    303\u001b[0m routa \u001b[38;5;241m=\u001b[39m tempa\u001b[38;5;241m.\u001b[39mrepeat(Nstep, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    304\u001b[0m routb \u001b[38;5;241m=\u001b[39m tempb\u001b[38;5;241m.\u001b[39mrepeat(Nstep, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m UH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUH_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroutb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenF\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lenF: folter\u001b[39;00m\n\u001b[1;32m    306\u001b[0m rf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(Qsim, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m])   \u001b[38;5;66;03m# dim:gage*var*time\u001b[39;00m\n\u001b[1;32m    307\u001b[0m UH \u001b[38;5;241m=\u001b[39m UH\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# dim: gage*var*time\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/hydro_models/HBV/HBVmul.py:43\u001b[0m, in \u001b[0;36mHBVMul.UH_gamma\u001b[0;34m(self, a, b, lenF)\u001b[0m\n\u001b[1;32m     41\u001b[0m aa \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(a[\u001b[38;5;241m0\u001b[39m:lenF, :, :])\u001b[38;5;241m.\u001b[39mview([lenF, m[\u001b[38;5;241m1\u001b[39m], m[\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# minimum 0.1. First dimension of a is repeat\u001b[39;00m\n\u001b[1;32m     42\u001b[0m theta \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(b[\u001b[38;5;241m0\u001b[39m:lenF, :, :])\u001b[38;5;241m.\u001b[39mview([lenF, m[\u001b[38;5;241m1\u001b[39m], m[\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# minimum 0.5\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenF\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview([lenF, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mrepeat([\u001b[38;5;241m1\u001b[39m, m[\u001b[38;5;241m1\u001b[39m], m[\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m     44\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcuda(aa\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     45\u001b[0m denom \u001b[38;5;241m=\u001b[39m (aa\u001b[38;5;241m.\u001b[39mlgamma()\u001b[38;5;241m.\u001b[39mexp()) \u001b[38;5;241m*\u001b[39m (theta \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m aa)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: upper bound and larger bound inconsistent with step sign"
     ]
    }
   ],
   "source": [
    "model._model.forward(sample, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 08:32:04][__main__][INFO] - Creating dPLHydro BMI model instance\n",
      "[2024-07-15 08:32:04][__main__][INFO] - INITIALIZING BMI\n",
      "[2024-07-15 08:32:14][__main__][INFO] - Collecting attribute and forcing data\n",
      "[2024-07-15 08:32:14][__main__][INFO] - BEGIN BMI FORWARD: 1 timesteps...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar_t_nn\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     69\u001b[0m     standard_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_var_name_map_short_first[var]\n\u001b[0;32m---> 70\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_value(standard_name, \u001b[43mdataset_dict\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs_nn_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m][t, :n_basins, i], model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m n_forc \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Set NN attributes...\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_dict' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a testing script for running a dPL, physics-informed machine learning\n",
    "model BMI that is NextGen framework and NOAA OWP operation-ready.\n",
    "\n",
    "Note:\n",
    "- The current setup only passes CAMELS (671 basins) data to the BMI. For\n",
    "    different datasets, `.set_value()` mappings must be modeified to the respective\n",
    "    forcing + attribute key values.\n",
    "\"\"\"\n",
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(name)s][%(levelname)s] - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from core.data.dataset_loading import get_data_dict\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "################## Initialize the BMI ##################\n",
    "# Path to BMI config.\n",
    "config_path = '/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/bmi/bmi_config.yaml' #\"bmi_config.yaml\"\n",
    "\n",
    "# Create instance of BMI model.\n",
    "log.info(\"Creating dPLHydro BMI model instance\")\n",
    "model = BMIdPLHydroModel()\n",
    "\n",
    "# [CONTROL FUNCTION] Initialize the BMI.\n",
    "log.info(f\"INITIALIZING BMI\")\n",
    "model.initialize(bmi_cfg_filepath=config_path)\n",
    "\n",
    "\n",
    "################## Get test data ##################\n",
    "log.info(f\"Collecting attribute and forcing data\")\n",
    "\n",
    "# TODO: Adapt this PMI data loader to be more-BMI friendly, less a function iceberg.\n",
    "# dataset_dict, _ = get_data_dict(model.config, train=False)\n",
    "\n",
    "# Fixing typo in CAMELS dataset: 'geol_porostiy'.\n",
    "# (Written into config somewhere inside get_data_dict...)\n",
    "var_c_nn = model.config['observations']['var_c_nn']\n",
    "if 'geol_porostiy' in var_c_nn:\n",
    "    model.config['observations']['var_c_nn'][var_c_nn.index('geol_porostiy')] = 'geol_porosity'\n",
    "\n",
    "\n",
    "################## Forward model for 1 or multiple timesteps ##################\n",
    "# n_timesteps = dataset_dict['inputs_nn_scaled'].shape[0]\n",
    "n_timesteps = 1  # debug\n",
    "n_basins = 671\n",
    "\n",
    "log.info(f\"BEGIN BMI FORWARD: {n_timesteps} timesteps...\")\n",
    "\n",
    "# TODO: write a timestep handler/translator so we can pull out\n",
    "# forcings/attributes for the specific timesteps we want streamflow predictions for.\n",
    "\n",
    "# Loop through and return streamflow at each timestep.\n",
    "for t in range(n_timesteps):\n",
    "    # NOTE: for each timestep in this loop, the data assignments below are of\n",
    "    # arrays of basins. e.g., forcings['key'].shape = (1, # basins)\n",
    "\n",
    "    ################## Map forcings + attributes into BMI ##################\n",
    "    # Set NN forcings...\n",
    "    for i, var in enumerate(model.config['observations']['var_t_nn']):\n",
    "        standard_name = model._var_name_map_short_first[var]\n",
    "        model.set_value(standard_name, dataset_dict['inputs_nn_scaled'][t, :n_basins, i], model='nn')\n",
    "    n_forc = i\n",
    "    \n",
    "    # Set NN attributes...\n",
    "    for i, var in enumerate(model.config['observations']['var_c_nn']):\n",
    "        standard_name = model._var_name_map_short_first[var]\n",
    "        model.set_value(standard_name, dataset_dict['inputs_nn_scaled'][t, :n_basins, n_forc + i + 1], model='nn') \n",
    "\n",
    "    # Set physics model forcings...\n",
    "    for i, var in enumerate(model.config['observations']['var_t_hydro_model']):\n",
    "        standard_name = model._var_name_map_short_first[var]\n",
    "        model.set_value(standard_name, dataset_dict['x_hydro_model'][t, :n_basins, i], model='pm') \n",
    "\n",
    "    # Set physics model attributes...\n",
    "    for i, var in enumerate(model.config['observations']['var_c_hydro_model']):\n",
    "        standard_name = model._var_name_map_short_first[var]\n",
    "        # NOTE: These attributes don't have a time dimension...\n",
    "        model.set_value(standard_name, dataset_dict['c_hydro_model'][:n_basins, i], model='pm') \n",
    "\n",
    "    # print(model._values)\n",
    "\n",
    "    # [CONTROL FUNCTION] Update the model at all basins for one timestep.\n",
    "    model.update()\n",
    "    # print(f\"Streamflow at time {model.t} is {model.streamflow_cms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.forward(model.dataset_sample, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "upper bound and larger bound inconsistent with step sign",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/model_handler.py:90\u001b[0m, in \u001b[0;36mModelHandler.forward\u001b[0;34m(self, dataset_dict_sample, eval)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# self.model_dict[mod].eval()  # For testing.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_out_dict[mod] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/differentiable_model.py:94\u001b[0m, in \u001b[0;36mdPLHydroModel.forward\u001b[0;34m(self, dataset_dict_sample)\u001b[0m\n\u001b[1;32m     91\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreakdown_params(params_all)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Hydro model\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m flow_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydro_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dict_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dict_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhydro_params_raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwarm_up\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrouting_hydro_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_params_hydro\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv_params_hydro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Baseflow index percentage\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m## Using two deep groundwater buckets: gwflow & bas_shallow\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbas_shallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m flow_out\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/mulhydrodl/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/hydro_models/HBV/HBVmul.py:305\u001b[0m, in \u001b[0;36mHBVMul.forward\u001b[0;34m(self, x_hydro_model, c_hydro_model, params_raw, args, muwts, warm_up, init, routing, comprout, conv_params_hydro)\u001b[0m\n\u001b[1;32m    303\u001b[0m routa \u001b[38;5;241m=\u001b[39m tempa\u001b[38;5;241m.\u001b[39mrepeat(Nstep, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    304\u001b[0m routb \u001b[38;5;241m=\u001b[39m tempb\u001b[38;5;241m.\u001b[39mrepeat(Nstep, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m UH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUH_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroutb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenF\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lenF: folter\u001b[39;00m\n\u001b[1;32m    306\u001b[0m rf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(Qsim, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m])   \u001b[38;5;66;03m# dim:gage*var*time\u001b[39;00m\n\u001b[1;32m    307\u001b[0m UH \u001b[38;5;241m=\u001b[39m UH\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# dim: gage*var*time\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/hydro_models/HBV/HBVmul.py:43\u001b[0m, in \u001b[0;36mHBVMul.UH_gamma\u001b[0;34m(self, a, b, lenF)\u001b[0m\n\u001b[1;32m     41\u001b[0m aa \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(a[\u001b[38;5;241m0\u001b[39m:lenF, :, :])\u001b[38;5;241m.\u001b[39mview([lenF, m[\u001b[38;5;241m1\u001b[39m], m[\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# minimum 0.1. First dimension of a is repeat\u001b[39;00m\n\u001b[1;32m     42\u001b[0m theta \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(b[\u001b[38;5;241m0\u001b[39m:lenF, :, :])\u001b[38;5;241m.\u001b[39mview([lenF, m[\u001b[38;5;241m1\u001b[39m], m[\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# minimum 0.5\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenF\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview([lenF, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mrepeat([\u001b[38;5;241m1\u001b[39m, m[\u001b[38;5;241m1\u001b[39m], m[\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m     44\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcuda(aa\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     45\u001b[0m denom \u001b[38;5;241m=\u001b[39m (aa\u001b[38;5;241m.\u001b[39mlgamma()\u001b[38;5;241m.\u001b[39mexp()) \u001b[38;5;241m*\u001b[39m (theta \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m aa)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: upper bound and larger bound inconsistent with step sign"
     ]
    }
   ],
   "source": [
    "model._model.forward(sample, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['inputs_nn_scaled'][:1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dataset_sample['inputs_nn_scaled'][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(model.dataset_sample['inputs_nn_scaled'][0:], sample['inputs_nn_scaled'][:1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._values[standard_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2, 3))\n",
    "\n",
    "x[:,0] = np.array([model._values['atmosphere_water__liquid_equivalent_precipitation_rate']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([model._values['atmosphere_water__liquid_equivalent_precipitation_rate']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BMI wrapper for interfacing dPL hydrology models with NOAA OWP NextGen framework.\n",
    "\"\"\"\n",
    "# Need this to get external packages like conf.config.\n",
    "import sys\n",
    "package_path = '/data/lgl5139/hydro_multimodel/dPLHydro_multimodel'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, Any, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from ruamel.yaml import YAML\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from bmipy import Bmi\n",
    "from conf.config import Config\n",
    "from models.model_handler import ModelHandler\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pydantic import ValidationError\n",
    "from core.data import take_sample_test\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "class BMIdPLHydroModel(Bmi):\n",
    "    \"\"\"\n",
    "    Run forward with BMI for a trained differentiable hydrology model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Create a dPLHydro model BMI ready for initialization.\n",
    "        \"\"\"\n",
    "        super(BMIdPLHydroModel, self).__init__()\n",
    "        start_time = time.time()\n",
    "\n",
    "        self._model = None\n",
    "        self._initialized = False\n",
    "\n",
    "        self._start_time = 0.0\n",
    "        self._values = {}\n",
    "        self._nn_values = {}\n",
    "        self._pm_values = {}\n",
    "        self._end_time = np.finfo(float).max\n",
    "        self.var_array_lengths = 1\n",
    "\n",
    "        self.bmi_process_time = 0\n",
    "\n",
    "\n",
    "        # Required, static attributes of the model\n",
    "        _att_map = {\n",
    "        'model_name':         \"Differentiable Parameter Learning Hydrology BMI\",\n",
    "        'version':            '1.0',\n",
    "        'author_name':        'MHPI',\n",
    "        'time_units':         'days',\n",
    "        }\n",
    "        \n",
    "        # Input forcing/attribute CSDMS Standard Names.\n",
    "        self._input_var_names = [\n",
    "            ############## Forcings ##############\n",
    "            'atmosphere_water__liquid_equivalent_precipitation_rate',\n",
    "            'land_surface_air__temperature',\n",
    "            'land_surface_air__max_of_temperature',  # custom name\n",
    "            'land_surface_air__min_of_temperature',  # custom name\n",
    "            'land_surface_water__potential_evaporation_volume_flux',  # check name,\n",
    "            ############## Attributes ##############\n",
    "            'atmosphere_water__daily_mean_of_liquid_equivalent_precipitation_rate',\n",
    "            'land_surface_water__daily_mean_of_potential_evaporation_flux',\n",
    "            'p_seasonality',  # custom name\n",
    "            'atmosphere_water__precipitation_falling_as_snow_fraction',\n",
    "            'ratio__mean_potential_evapotranspiration__mean_precipitation',\n",
    "            'atmosphere_water__frequency_of_high_precipitation_events',\n",
    "            'atmosphere_water__mean_duration_of_high_precipitation_events',\n",
    "            'atmosphere_water__precipitation_frequency',\n",
    "            'atmosphere_water__low_precipitation_duration',\n",
    "            'basin__mean_of_elevation',\n",
    "            'basin__mean_of_slope',\n",
    "            'basin__area',\n",
    "            'land_vegetation__forest_area_fraction',\n",
    "            'land_vegetation__max_monthly_mean_of_leaf-area_index',\n",
    "            'land_vegetation__diff_max_min_monthly_mean_of_leaf-area_index',\n",
    "            'land_vegetation__max_monthly_mean_of_green_vegetation_fraction',\n",
    "            'land_vegetation__diff__max_min_monthly_mean_of_green_vegetation_fraction',\n",
    "            'region_state_land~covered__area_fraction',  # custom name\n",
    "            'region_state_land~covered__area',  # custom name\n",
    "            'root__depth',  # custom name\n",
    "            'soil_bedrock_top__depth__pelletier',\n",
    "            'soil_bedrock_top__depth__statsgo',\n",
    "            'soil__porosity',\n",
    "            'soil__saturated_hydraulic_conductivity',\n",
    "            'maximum_water_content',\n",
    "            'soil_sand__volume_fraction',\n",
    "            'soil_silt__volume_fraction', \n",
    "            'soil_clay__volume_fraction',\n",
    "            'geol_1st_class',  # custom name\n",
    "            'geol_1st_class__fraction',  # custom name\n",
    "            'geol_2nd_class',  # custom name\n",
    "            'geol_2nd_class__fraction',  # custom name\n",
    "            'basin__carbonate_rocks_area_fraction',\n",
    "            'soil_active-layer__porosity',  # check name\n",
    "            'bedrock__permeability'\n",
    "        ]\n",
    "\n",
    "        # Output variable names (CSDMS standard names)\n",
    "        self._output_var_names = ['land_surface_water__runoff_volume_flux']\n",
    "\n",
    "        # Map CSDMS Standard Names to the model's internal variable names (For CAMELS).\n",
    "        self._var_name_units_map = {\n",
    "            ############## Forcings ##############\n",
    "            'atmosphere_water__liquid_equivalent_precipitation_rate':['prcp(mm/day)', 'mm d-1'],\n",
    "            'land_surface_air__temperature':['tmean(C)','degC'],\n",
    "            'land_surface_air__max_of_temperature':['tmax(C)', 'degC'],  # custom name\n",
    "            'land_surface_air__min_of_temperature':['tmin(C)', 'degC'],  # custom name\n",
    "            'land_surface_water__potential_evaporation_volume_flux':['PET_hargreaves(mm/day)', 'mm d-1'],  # check name\n",
    "            ############## Attributes ##############\n",
    "            'atmosphere_water__daily_mean_of_liquid_equivalent_precipitation_rate':['p_mean','mm d-1'],\n",
    "            'land_surface_water__daily_mean_of_potential_evaporation_flux':['pet_mean','mm d-1'],\n",
    "            'p_seasonality':['p_seasonality', '-'],  # custom name\n",
    "            'atmosphere_water__precipitation_falling_as_snow_fraction':['frac_snow','-'],\n",
    "            'ratio__mean_potential_evapotranspiration__mean_precipitation':['aridity','-'],\n",
    "            'atmosphere_water__frequency_of_high_precipitation_events':['high_prec_freq','d yr-1'],\n",
    "            'atmosphere_water__mean_duration_of_high_precipitation_events':['high_prec_dur','d'],\n",
    "            'atmosphere_water__precipitation_frequency':['low_prec_freq','d yr-1'],\n",
    "            'atmosphere_water__low_precipitation_duration':['low_prec_dur','d'],\n",
    "            'basin__mean_of_elevation':['elev_mean','m'],\n",
    "            'basin__mean_of_slope':['slope_mean','m km-1'],\n",
    "            'basin__area':['area_gages2','km2'],\n",
    "            'land_vegetation__forest_area_fraction':['frac_forest','-'],\n",
    "            'land_vegetation__max_monthly_mean_of_leaf-area_index':['lai_max','-'],\n",
    "            'land_vegetation__diff_max_min_monthly_mean_of_leaf-area_index':['lai_diff','-'],\n",
    "            'land_vegetation__max_monthly_mean_of_green_vegetation_fraction':['gvf_max','-'],\n",
    "            'land_vegetation__diff__max_min_monthly_mean_of_green_vegetation_fraction':['gvf_diff','-'],\n",
    "            'region_state_land~covered__area_fraction':['dom_land_cover_frac', 'percent'],  # custom name\n",
    "            'region_state_land~covered__area':['dom_land_cover', '-'],  # custom name\n",
    "            'root__depth':['root_depth_50', '-'],  # custom name\n",
    "            'soil_bedrock_top__depth__pelletier':['soil_depth_pelletier','m'],\n",
    "            'soil_bedrock_top__depth__statsgo':['soil_depth_statsgo','m'],\n",
    "            'soil__porosity':['soil_porosity','-'],\n",
    "            'soil__saturated_hydraulic_conductivity':['soil_conductivity','cm hr-1'],\n",
    "            'maximum_water_content':['max_water_content','m'],\n",
    "            'soil_sand__volume_fraction':['sand_frac','percent'],\n",
    "            'soil_silt__volume_fraction':['silt_frac','percent'], \n",
    "            'soil_clay__volume_fraction':['clay_frac','percent'],\n",
    "            'geol_1st_class':['geol_1st_class', '-'],  # custom name\n",
    "            'geol_1st_class__fraction':['glim_1st_class_frac', '-'],  # custom name\n",
    "            'geol_2nd_class':['geol_2nd_class', '-'],  # custom name\n",
    "            'geol_2nd_class__fraction':['glim_2nd_class_frac', '-'],  # custom name\n",
    "            'basin__carbonate_rocks_area_fraction':['carbonate_rocks_frac','-'],\n",
    "            'soil_active-layer__porosity':['geol_porosity', '-'],  # check name\n",
    "            'bedrock__permeability':['geol_permeability','m2'],\n",
    "            'drainage__area':['DRAIN_SQKM', 'km2'],  # custom name\n",
    "            'land_surface__latitude':['lat','degrees'],\n",
    "            ############## Outputs ##############\n",
    "            'land_surface_water__runoff_volume_flux':['streamflow_cms','m3 s-1']\n",
    "            }\n",
    "        \n",
    "        # Keep running total of BMI runtime.\n",
    "        self.bmi_process_time += time.time() - start_time\n",
    "    \n",
    "    def initialize(self, bmi_cfg_filepath: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        (BMI Control function) Initialize the dPLHydro model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bmi_cfg_filepath : str, optional\n",
    "            Path to the BMI configuration file.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Read in BMI configurations.\n",
    "        self.initialize_config(bmi_cfg_filepath)\n",
    "        \n",
    "        # Make lookup tables (Peckham et al.).\n",
    "        self._var_name_map_long_first = {\n",
    "            long_name:self._var_name_units_map[long_name][0] for \\\n",
    "            long_name in self._var_name_units_map.keys()\n",
    "            }\n",
    "        self._var_name_map_short_first = {\n",
    "            self._var_name_units_map[long_name][0]:long_name for \\\n",
    "            long_name in self._var_name_units_map.keys()}\n",
    "        self._var_units_map = {\n",
    "            long_name:self._var_name_units_map[long_name][1] for \\\n",
    "            long_name in self._var_name_units_map.keys()\n",
    "        }\n",
    "\n",
    "        # Initialize inputs and outputs.\n",
    "        for var in self.config['observations']['var_t_nn'] + self.config['observations']['var_c_nn']:\n",
    "            standard_name = self._var_name_map_short_first[var]\n",
    "            self._nn_values[standard_name] = []\n",
    "            # setattr(self, var, 0)\n",
    "\n",
    "        for var in self.config['observations']['var_t_hydro_model'] + self.config['observations']['var_c_hydro_model']:\n",
    "            standard_name = self._var_name_map_short_first[var]\n",
    "            self._pm_values[standard_name] = []\n",
    "            # setattr(self, var, 0)\n",
    "\n",
    "\n",
    "        # Set a simulation start time and gettimestep size.\n",
    "        self.current_time = self._start_time\n",
    "        self._time_step_size = self.config['time_step_delta']\n",
    "\n",
    "        # Load a trained model.\n",
    "        self._model = ModelHandler(self.config).to(self.config['device'])\n",
    "        self._initialized = True\n",
    "\n",
    "        # Intialize dataset (NOTE: move this externally).\n",
    "        # self._get_data_dict()\n",
    "\n",
    "        # Keep running total of BMI runtime.\n",
    "        self.bmi_process_time += time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _get_data_dict(self) -> None:\n",
    "        \"\"\"\n",
    "        Construct data dictionary from BMI input data.\n",
    "\n",
    "        iS, iE: arrays of start and end pairs of basin indicies for batching.\n",
    "        \"\"\"\n",
    "        dataset_dict, self.config = self._values_to_dict()\n",
    "\n",
    "\n",
    "\n",
    "    def update_frac(self, time_frac: float) -> None:\n",
    "        \"\"\"\n",
    "        Update model by a fraction of a time step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        time_frac : float\n",
    "            Fraction fo a time step.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Warning: This model is trained to make predictions on one day timesteps.\")\n",
    "        time_step = self.get_time_step()\n",
    "        self._time_step_size = self._time_step_size * time_frac\n",
    "        self.update()\n",
    "        self._time_step_size = time_step\n",
    "\n",
    "    def update_until(self, end_time: float) -> None:\n",
    "        \"\"\"\n",
    "        (BMI Control function) Update model until a particular time.\n",
    "        Note: Models should be trained standalone with dPLHydro_PMI first before forward predictions with this BMI.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        end_time : float\n",
    "            Time to run model until.\n",
    "        \"\"\"\n",
    "        n_steps = (end_time - self.get_current_time()) / self.get_time_step()\n",
    "\n",
    "        for _ in range(int(n_steps)):\n",
    "            self.update()\n",
    "        self.update_frac(n_steps - int(n_steps))\n",
    "\n",
    "    def finalize(self) -> None:\n",
    "        \"\"\"\n",
    "        (BMI Control function) Finalize model.\n",
    "        \"\"\"\n",
    "        # TODO: Force destruction of ESMF and other objects when testing is done\n",
    "        # to save space.\n",
    "\n",
    "        self._model = None\n",
    "\n",
    "    def array_to_tensor(self) -> None:\n",
    "        \"\"\"\n",
    "        Converts input values into Torch tensor object to be read by model. \n",
    "        \"\"\"  \n",
    "        raise NotImplementedError(\"array_to_tensor\")\n",
    "    \n",
    "    def tensor_to_array(self) -> None:\n",
    "        \"\"\"\n",
    "        Converts model output Torch tensor into date + gradient arrays to be\n",
    "        passed out of BMI for backpropagation, loss, optimizer tuning.\n",
    "        \"\"\"  \n",
    "        raise NotImplementedError(\"tensor_to_array\")\n",
    "    \n",
    "    def get_tensor_slice(self):\n",
    "        \"\"\"\n",
    "        Get tensor of input data for a single timestep.\n",
    "        \"\"\"\n",
    "        # sample_dict = take_sample_test(self.bmi_config, self.dataset_dict)\n",
    "        # self.input_tensor = torch.Tensor()\n",
    "    \n",
    "        raise NotImplementedError(\"get_tensor_slice\")\n",
    "\n",
    "    # ------------------ Finished up to here ------------------\n",
    "    # ---------------------------------------------------------\n",
    "    def get_var_type(self, var_name):\n",
    "        \"\"\"\n",
    "        Data type of variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Data type.\n",
    "        \"\"\"\n",
    "        return str(self.get_value_ptr(var_name).dtype)\n",
    "\n",
    "    def get_var_units(self, var_name):\n",
    "        \"\"\"Get units of variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Variable units.\n",
    "        \"\"\"\n",
    "        return self._var_units[var_name]\n",
    "\n",
    "    def get_var_nbytes(self, var_name):\n",
    "        \"\"\"Get units of variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Size of data array in bytes.\n",
    "        \"\"\"\n",
    "        return self.get_value_ptr(var_name).nbytes\n",
    "\n",
    "    def get_var_itemsize(self, name):\n",
    "        return np.dtype(self.get_var_type(name)).itemsize\n",
    "\n",
    "    def get_var_location(self, name):\n",
    "        return self._var_loc[name]\n",
    "\n",
    "    def get_var_grid(self, var_name):\n",
    "        \"\"\"Grid id for a variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Grid id.\n",
    "        \"\"\"\n",
    "        # for grid_id, var_name_list in self._grids.items():\n",
    "        #     if var_name in var_name_list:\n",
    "        #         return grid_id\n",
    "        raise NotImplementedError(\"get_var_grid\")\n",
    "\n",
    "    def get_grid_rank(self, grid_id):\n",
    "        \"\"\"Rank of grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_id : int\n",
    "            Identifier of a grid.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Rank of grid.\n",
    "        \"\"\"\n",
    "        # return len(self._model.shape)\n",
    "        raise NotImplementedError(\"get_grid_rank\")\n",
    "\n",
    "\n",
    "    def get_grid_size(self, grid_id):\n",
    "        \"\"\"Size of grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_id : int\n",
    "            Identifier of a grid.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Size of grid.\n",
    "        \"\"\"\n",
    "        # return int(np.prod(self._model.shape))\n",
    "        raise NotImplementedError(\"get_grid_size\")\n",
    "\n",
    "\n",
    "    def get_value_ptr(self, var_name: str, model:str) -> np.ndarray:\n",
    "        \"\"\"Reference to values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            Value array.\n",
    "        \"\"\"\n",
    "        if model == 'nn':\n",
    "            if var_name not in self._nn_values.keys():\n",
    "                raise ValueError(f\"No known variable in BMI model: {var_name}\")\n",
    "            return self._nn_values[var_name]\n",
    "\n",
    "        elif model == 'pm':\n",
    "            if var_name not in self._pm_values.keys():\n",
    "                raise ValueError(f\"No known variable in BMI model: {var_name}\")\n",
    "            return self._pm_values[var_name]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Valid model type (nn or pm) must be specified.\")\n",
    "\n",
    "    def get_value(self, var_name, dest):\n",
    "        \"\"\"Copy of values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "        dest : ndarray\n",
    "            A numpy array into which to place the values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            Copy of values.\n",
    "        \"\"\"\n",
    "        dest[:] = self.get_value_ptr(var_name).flatten()\n",
    "        return dest\n",
    "\n",
    "    def get_value_at_indices(self, var_name, dest, indices):\n",
    "        \"\"\"Get values at particular indices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "        dest : ndarray\n",
    "            A numpy array into which to place the values.\n",
    "        indices : array_like\n",
    "            Array of indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            Values at indices.\n",
    "        \"\"\"\n",
    "        dest[:] = self.get_value_ptr(var_name).take(indices)\n",
    "        return dest\n",
    "\n",
    "    def set_value(self, var_name, values: np.ndarray, model:str):\n",
    "        \"\"\"Set model values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "        values : array_like\n",
    "            Array of new values.\n",
    "        \"\"\"\n",
    "        if not isinstance(values, (np.ndarray, list, tuple)):\n",
    "            values = np.array([values])\n",
    "\n",
    "        val = self.get_value_ptr(var_name, model=model)\n",
    "\n",
    "        # val = values.reshape(val.shape)\n",
    "        val[:] = values\n",
    "\n",
    "    def set_value_at_indices(self, name, inds, src):\n",
    "        \"\"\"Set model values at particular indices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name : str\n",
    "            Name of variable as CSDMS Standard Name.\n",
    "        src : array_like\n",
    "            Array of new values.\n",
    "        indices : array_like\n",
    "            Array of indices.\n",
    "        \"\"\"\n",
    "        val = self.get_value_ptr(name)\n",
    "        val.flat[inds] = src\n",
    "\n",
    "    def get_component_name(self):\n",
    "        \"\"\"Name of the component.\"\"\"\n",
    "        return self._name\n",
    "\n",
    "    def get_input_item_count(self):\n",
    "        \"\"\"Get names of input variables.\"\"\"\n",
    "        return len(self._input_var_names)\n",
    "\n",
    "    def get_output_item_count(self):\n",
    "        \"\"\"Get names of output variables.\"\"\"\n",
    "        return len(self._output_var_names)\n",
    "\n",
    "    def get_input_var_names(self):\n",
    "        \"\"\"Get names of input variables.\"\"\"\n",
    "        return self._input_var_names\n",
    "\n",
    "    def get_output_var_names(self):\n",
    "        \"\"\"Get names of output variables.\"\"\"\n",
    "        return self._output_var_names\n",
    "\n",
    "    def get_grid_shape(self, grid_id, shape):\n",
    "        \"\"\"Number of rows and columns of uniform rectilinear grid.\"\"\"\n",
    "        # var_name = self._grids[grid_id][0]\n",
    "        # shape[:] = self.get_value_ptr(var_name).shape\n",
    "        # return shape\n",
    "        raise NotImplementedError(\"get_grid_shape\")\n",
    "\n",
    "    def get_grid_spacing(self, grid_id, spacing):\n",
    "        \"\"\"Spacing of rows and columns of uniform rectilinear grid.\"\"\"\n",
    "        # spacing[:] = self._model.spacing\n",
    "        # return spacing\n",
    "        raise NotImplementedError(\"get_grid_spacing\")\n",
    "\n",
    "    def get_grid_origin(self, grid_id, origin):\n",
    "        \"\"\"Origin of uniform rectilinear grid.\"\"\"\n",
    "        # origin[:] = self._model.origin\n",
    "        # return origin\n",
    "        raise NotImplementedError(\"get_grid_origin\")\n",
    "\n",
    "    def get_grid_type(self, grid_id):\n",
    "        \"\"\"Type of grid.\"\"\"\n",
    "        # return self._grid_type[grid_id]\n",
    "        raise NotImplementedError(\"get_grid_type\")\n",
    "\n",
    "    def get_start_time(self):\n",
    "        \"\"\"Start time of model.\"\"\"\n",
    "        return self._start_time\n",
    "\n",
    "    def get_end_time(self):\n",
    "        \"\"\"End time of model.\"\"\"\n",
    "        return self._end_time\n",
    "\n",
    "    def get_current_time(self):\n",
    "        return self._current_time\n",
    "\n",
    "    def get_time_step(self):\n",
    "        return self._time_step_size\n",
    "\n",
    "    def get_time_units(self):\n",
    "        return self._time_units\n",
    "\n",
    "    def get_grid_edge_count(self, grid):\n",
    "        raise NotImplementedError(\"get_grid_edge_count\")\n",
    "\n",
    "    def get_grid_edge_nodes(self, grid, edge_nodes):\n",
    "        raise NotImplementedError(\"get_grid_edge_nodes\")\n",
    "\n",
    "    def get_grid_face_count(self, grid):\n",
    "        raise NotImplementedError(\"get_grid_face_count\")\n",
    "\n",
    "    def get_grid_face_nodes(self, grid, face_nodes):\n",
    "        raise NotImplementedError(\"get_grid_face_nodes\")\n",
    "\n",
    "    def get_grid_node_count(self, grid):\n",
    "        \"\"\"Number of grid nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid : int\n",
    "            Identifier of a grid.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Size of grid.\n",
    "        \"\"\"\n",
    "        # return self.get_grid_size(grid)\n",
    "        raise NotImplementedError(\"get_grid_node_count\")\n",
    "\n",
    "        \n",
    "\n",
    "    def get_grid_nodes_per_face(self, grid, nodes_per_face):\n",
    "        raise NotImplementedError(\"get_grid_nodes_per_face\")\n",
    "\n",
    "    def get_grid_face_edges(self, grid, face_edges):\n",
    "        raise NotImplementedError(\"get_grid_face_edges\")\n",
    "\n",
    "    def get_grid_x(self, grid, x):\n",
    "        raise NotImplementedError(\"get_grid_x\")\n",
    "\n",
    "    def get_grid_y(self, grid, y):\n",
    "        raise NotImplementedError(\"get_grid_y\")\n",
    "\n",
    "    def get_grid_z(self, grid, z):\n",
    "        raise NotImplementedError(\"get_grid_z\")\n",
    "\n",
    "    def initialize_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Check that config_path is valid path and convert config into a\n",
    "        dictionary object.\n",
    "        \"\"\"\n",
    "        config_path = Path(config_path).resolve()\n",
    "        \n",
    "        if not config_path:\n",
    "            raise RuntimeError(\"No BMI configuration path provided.\")\n",
    "        elif not config_path.is_file():\n",
    "            raise RuntimeError(f\"BMI configuration not found at path {config_path}.\")\n",
    "        else:\n",
    "            with config_path.open('r') as f:\n",
    "                self.config = yaml.safe_load(f)\n",
    "    \n",
    "\n",
    "        # USE BELOW FOR HYDRA + OMEGACONF:\n",
    "        # try:\n",
    "        #     config_dict: Union[Dict[str, Any], Any] = OmegaConf.to_container(\n",
    "        #         cfg, resolve=True\n",
    "        #     )\n",
    "        #     config = Config(**config_dict)\n",
    "        # except ValidationError as e:\n",
    "        #     log.exception(e)\n",
    "        #     raise e\n",
    "        # return config, config_dict\n",
    "\n",
    "    def take_sample_test(self, config: Dict, dataset_dictionary: Dict[str, torch.Tensor], \n",
    "                        i_s: int, i_e: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take sample of data for testing batch.\n",
    "        \"\"\"\n",
    "        dataset_sample = {}\n",
    "        for key, value in dataset_dictionary.items():\n",
    "            if value.ndim == 3:\n",
    "                # TODO: I don't think we actually need this.\n",
    "                # Remove the warmup period for all except airtemp_memory and hydro inputs.\n",
    "                if key in ['airT_mem_temp_model', 'x_hydro_model', 'inputs_nn_scaled']:\n",
    "                    warm_up = 0\n",
    "                else:\n",
    "                    warm_up = config['warm_up']\n",
    "                dataset_sample[key] = value[warm_up:, i_s:i_e, :].to(config['device'])\n",
    "            elif value.ndim == 2:\n",
    "                dataset_sample[key] = value[i_s:i_e, :].to(config['device'])\n",
    "            else:\n",
    "                raise ValueError(f\"Incorrect input dimensions. {key} array must have 2 or 3 dimensions.\")\n",
    "        return dataset_sample\n",
    "\n",
    "\n",
    "    def _values_to_dict(self) -> None:\n",
    "        \"\"\"\n",
    "        Take CSDMS Standard Name-mapped forcings + attributes and construct data\n",
    "        dictionary for NN and physics model.\n",
    "        \"\"\"\n",
    "        # Initialize dict arrays\n",
    "        n_basins = len(self._nn_values[self._var_name_map_short_first[self.config['observations']['var_t_nn'][0]]])\n",
    "\n",
    "        x_nn = np.zeros((n_basins, len(self.config['observations']['var_t_nn'])))\n",
    "        c_nn = np.zeros((n_basins, len(self.config['observations']['var_c_nn'])))\n",
    "        x_hydro_model = np.zeros((n_basins, len(self.config['observations']['var_t_hydro_model'])))\n",
    "        c_hydro_model = np.zeros((n_basins, len(self.config['observations']['var_c_hydro_model'])))\n",
    "\n",
    "        for i, var in enumerate(self.config['observations']['var_t_nn']):\n",
    "            standard_name = self._var_name_map_short_first[var]\n",
    "            # NOTE: Using _values is a bit hacky. Should use get_values I think.    \n",
    "            x_nn[:, i] = np.array([self._nn_values[standard_name]])\n",
    "        \n",
    "        for i, var in enumerate(self.config['observations']['var_c_nn']):\n",
    "            standard_name = self._var_name_map_short_first[var]\n",
    "            c_nn[:, i] = np.array([self._nn_values[standard_name]])\n",
    "\n",
    "        for i, var in enumerate(self.config['observations']['var_t_hydro_model']):\n",
    "            standard_name = self._var_name_map_short_first[var]\n",
    "            x_hydro_model[:, i] = np.array([self._pm_values[standard_name]])\n",
    "\n",
    "        for i, var in enumerate(self.config['observations']['var_c_hydro_model']):\n",
    "            standard_name = self._var_name_map_short_first[var]\n",
    "            c_hydro_model[:, i] = np.array([self._pm_values[standard_name]])\n",
    "        \n",
    "        self.dataset_dict = {\n",
    "            'inputs_nn_scaled': np.concatenate((x_nn, c_nn), axis=1)[np.newaxis,:,:],\n",
    "            'x_hydro_model': x_hydro_model[np.newaxis,:,:],\n",
    "            'c_hydro_model': c_hydro_model\n",
    "        }\n",
    "\n",
    "        # Convert to torch tensors:\n",
    "        for key in self.dataset_dict.keys():\n",
    "            if type(self.dataset_dict[key]) == np.ndarray:\n",
    "                self.dataset_dict[key] = torch.from_numpy(self.dataset_dict[key]).float() #.to(self.config['device'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update(self) -> None:\n",
    "        \"\"\"\n",
    "        (BMI Control function) Advance model state by one time step.\n",
    "\n",
    "        Note: Models should be trained standalone with dPLHydro_PMI first before forward predictions with this BMI.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.current_time += self._time_step_size \n",
    "        \n",
    "        self._values_to_dict()\n",
    "\n",
    "        ngrid = self.dataset_dict['inputs_nn_scaled'].shape[1]\n",
    "        iS = np.arange(0, ngrid, self.config['batch_basins'])\n",
    "        iE = np.append(iS[1:], ngrid)\n",
    "        self.dataset_sample = take_sample_test(self.config,\n",
    "                                          self.dataset_dict,\n",
    "                                          iS[0],\n",
    "                                          iE[0]\n",
    "                                          )\n",
    "\n",
    "        self._model.forward(self.dataset_sample, eval=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # Assembles input values into Torch tensor and takes slice for model forward.\n",
    "        # self.get_tensor_slice()\n",
    "        # self.output = self._model.forward(self.input_tensor)\n",
    "\n",
    "        # # Keep running total of BMI runtime.\n",
    "        # self.bmi_process_time += time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mulhydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
