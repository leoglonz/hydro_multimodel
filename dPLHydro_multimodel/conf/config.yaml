defaults:
    - _self_
    - hydra: settings
    - observations: conus_5000_merit  # gages2_50, camels_671_dp_2024, camels_671_dp_2023, conus_5000_merit



## General Config -------------------------------#
mode: train_test   # train, test, train_test, train_wnn_only
ensemble_type: free_pnn    # none, frozen_pnn, free_pnn, avg, reg_max
use_checkpoint: False    # See bottom

random_seed: 0  # 111111
device: cuda
gpu_id: 1  # This is a list, each ind corresponds to gpu for a model. If only one ind, that gpu will be used for all models.

train:
    start_time: 1980/10/01
    end_time: 1995/10/01
test:
    start_time: 1995/10/01
    end_time: 2010/10/01

batch_basins: 25

name: hmm1.2-${observations.name}
# data_dir: /Users/leoglonz/Desktop/water/data
# data_dir: F:\code_repos\water\data
data_dir: /data/lgl5139/hydro_multimodel/dPLHydro_multimodel/runs  # Might remove #F:\code_repos\water\hydro_multimodel\dPLHydro_multimodel\runs  
output_dir: ${data_dir}/${observations.name}/saved_models/ #3vars_noDayllat #ts1998_2008

## Model Config -------------------------------#
pnn_model: LSTM    # LSTM, MLP
hydro_models: [HBV]    # HBV, HBV_capillary, marrmot_PRMS, SACSMA_with_snow
# TODO: ensemble_seeds: [2,6,6]
dy_params:
    # HBV: parBETA, parBETAET, parK0 | PRMS: alpha, scx, cgw, resmax, k1, k2 | SACSMA: pctim, smax, f1, f2, kuz rexp, f3, f4, pfree, klzp, klzs
    HBV: []
    HBV_capillary: []
    marrmot_PRMS: []
    SACSMA_with_snow: []

dy_drop: 0.0  # 0.0 always dynamic; 1.0 always static
routing_hydro_model: True
pet_module: dataset    # dataset, potet_hamon, potet_hargreaves
pet_dataset_name: PET_hargreaves(mm/day)
target: ['00060_Mean']    # 00060_Mean, 00010_Mean, BFI_AVE, PET
use_log_norm: []  #['prcp(mm/day)']  # For applying log normalization. ([] for HBV1.1p)

loss_function: RmseLossFlowComb  # RMSELossFlowComb, NSEsqrtLossFlow, NseLossBatchFlow (for HBV1.1p)
loss_function_weights:
    w1: 11.0
    w2: 1.0

nmul: 16
warm_up: 365
rho: 365
batch_size: 100
epochs: 50
dropout: 0.5
hidden_size: 256
learning_rate: 1.0
nearzero: 1e-4

save_epoch: 10

weighting_nn:
    dropout: 0.5
    hidden_size: 256
    learning_rate: 0.01
    method: sigmoid
    loss_function: RmseLossFlowComb
    loss_function_weights:
        w1: 11.0
        w2: 1.0
    loss_factor: 15
    loss_lower_bound: 0.9
    loss_upper_bound: 1.1


## Model Checkpoints -------------------------------#
checkpoint:
    start_epoch: 11
    HBV: /data/lgl5139/hydro_multimodel/dPLHydro_multimodel/runs/gages2_50/saved_models/debug_yalan/frozen_pnn/LSTM_E50_R365_B30_H256_n16_0/static_para/HBV_marrmot_PRMS_SACSMA_with_snow_/HBV_model_Ep50.pt
    marrmot_PRMS: /data/lgl5139/hydro_multimodel/dPLHydro_multimodel/runs/gages2_50/saved_models/debug_yalan/frozen_pnn/LSTM_E50_R365_B30_H256_n16_0/static_para/HBV_marrmot_PRMS_SACSMA_with_snow_/marrmot_PRMS_model_Ep50.pt
    SACSMA_with_snow: /data/lgl5139/hydro_multimodel/dPLHydro_multimodel/runs/gages2_50/saved_models/debug_yalan/frozen_pnn/LSTM_E50_R365_B30_H256_n16_0/static_para/HBV_marrmot_PRMS_SACSMA_with_snow_/SACSMA_with_snow_model_Ep50.pt
    weighting_nn: /data/lgl5139/hydro_multimodel/dPLHydro_multimodel/runs/camels_671_dp/saved_models/free_pnn/LSTM_E50_R365_B100_H256_n2_0/static_para/HBV_/wtNN_model_model_Ep10.pt
