{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_hbv as hbvArgs\n",
    "from config.read_configurations import config_prms as prmsArgs\n",
    "from config.read_configurations import config_sacsma as sacsmaArgs\n",
    "from config.read_configurations import config_sacsma_snow as sacsmaSnowArgs\n",
    "from config.read_configurations import config_hbv_hydrodl as hbvhyArgs\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.stats\n",
    "from post import plot\n",
    "\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import create_output_dirs\n",
    "from MODELS.loss_functions.get_loss_function import get_lossFun\n",
    "from MODELS.test_dp_HBV import test_dp_hbv\n",
    "from core.data_processing.data_loading import loadData\n",
    "from core.data_processing.normalization import transNorm\n",
    "from core.data_processing.model import (\n",
    "    take_sample_test,\n",
    "    converting_flow_from_ft3_per_sec_to_mm_per_day\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "##-----## Multi-model Parameters ##-----##\n",
    "# Setting dictionaries to separately manage each diff model's attributes.\n",
    "models = {'hbvhy': None, 'SACSMA_w_snow':None, 'marrmot_PRMS':None}  # 'hbv':None, 'hbvhy': None, 'SACSMA_w_snow':None, 'SACSMA':None,\n",
    "args_list = {'hbvhy': hbvhyArgs, 'SACSMA_w_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'hbv' : hbvArgs, 'SACSMA_w_snow':None, 'SACSMA': sacsmaArgs,\n",
    "ENSEMBLE_TYPE = 'avg'  # 'median', 'avg', 'max', 'softmax'\n",
    "\n",
    "# Set path to `hydro_multimodel_results` directory.\n",
    "if platform.system() == 'Darwin':\n",
    "    # For mac os\n",
    "    OUT_DIR = '/Users/leoglonz/Desktop/water/data/model_runs/hydro_multimodel_results'\n",
    "    # Some operations are not yet working with MPS, so we might need to set some environment variables to use CPU fall instead\n",
    "    # %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "    # Load test predictions from a prior run on mac\n",
    "    path = os.path.join(OUT_DIR, \"multimodels/671_sites_dp/output/preds_671_HBV_SACSMASnow_PRMS.npy\")\n",
    "    preds = np.load(path, allow_pickle=True).item()\n",
    "\n",
    "# model_output = preds\n",
    "elif platform.system() == 'Windows':\n",
    "    # For windows\n",
    "    OUT_DIR = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n",
    "\n",
    "    # Load test predictions from a prior run on windows.\n",
    "    path = os.path.join(OUT_DIR, \"multimodels\\\\671_sites_dp\\\\output\\\\preds_671_HBV_SACSMASnow_PRMS.npy\")\n",
    "    preds = np.load(path, allow_pickle=True).item()\n",
    "else:\n",
    "    raise ValueError('Unsupported operating system.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differentiable_model(args, diff_model):\n",
    "    \"\"\"\n",
    "    This function collects and outputs the model predictions and the corresponding\n",
    "    observations needed to run statistical analyses.\n",
    "    \n",
    "    If rerunning testing in a Jupyter environment, you will need to re-import args\n",
    "    as `batch_size` is overwritten in this function and will throw an error if the\n",
    "    overwrite is attempted a second time.\n",
    "    \"\"\"\n",
    "    warm_up = args[\"warm_up\"]\n",
    "    nmul = args[\"nmul\"]\n",
    "    diff_model.eval()\n",
    "    # read data for test time range\n",
    "    dataset_dictionary = loadData(args, trange=args[\"t_test\"])\n",
    "    np.save(os.path.join(args[\"out_dir\"], \"x.npy\"), dataset_dictionary[\"x_NN\"])  # saves with the overlap in the beginning\n",
    "    # normalizing\n",
    "    x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n",
    "    c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n",
    "    c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n",
    "    dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n",
    "    del x_NN_scaled, dataset_dictionary[\"x_NN\"]\n",
    "    # converting the numpy arrays to torch tensors:\n",
    "    for key in dataset_dictionary.keys():\n",
    "        dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n",
    "    \n",
    "    # args_mod = args.copy()\n",
    "    args[\"batch_size\"] = args[\"no_basins\"] \n",
    "    nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n",
    "\n",
    "    # Making lists of the start and end indices of the basins for each batch.\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    iS = np.arange(0, ngrid, batch_size)    # Start index list.\n",
    "    iE = np.append(iS[1:], ngrid)   # End.\n",
    "    \n",
    "    list_out_diff_model = []\n",
    "    for i in tqdm(range(0, len(iS)), unit='Batch'):\n",
    "        dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n",
    "\n",
    "        out_diff_model = diff_model(dataset_dictionary_sample)\n",
    "        # Convert all tensors in the dictionary to CPU\n",
    "        out_diff_model_cpu = {key: tensor.cpu().detach() for key, tensor in out_diff_model.items()}\n",
    "        # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n",
    "        list_out_diff_model.append(out_diff_model_cpu)\n",
    "\n",
    "    # getting rid of warm-up period in observation dataset and making the dimension similar to\n",
    "    # converting numpy to tensor\n",
    "    # y_obs = torch.tensor(np.swapaxes(y_obs[:, warm_up:, :], 0, 1), dtype=torch.float32)\n",
    "    # c_hydro_model = torch.tensor(c_hydro_model, dtype=torch.float32)\n",
    "    y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args, \n",
    "                                                           dataset_dictionary[\"c_NN\"],\n",
    "                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])\n",
    "        \n",
    "    return list_out_diff_model, y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting predictions, observations for HBV (HydroDL).\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  4.8092639446258545\n",
      "Time to read usgs streamflow:  4.478114128112793\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  4.123967885971069\n",
      "Time to read usgs streamflow:  4.161715030670166\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  4.258048057556152\n",
      "Time to read usgs streamflow:  4.148248672485352\n",
      "read usgs streamflow 5.347940683364868\n",
      "read master file /Users/leoglonz/Desktop/water/data/model_runs/rnnStreamflow/CAMELSDemo/dPLHBV/ALL/Testforc/daymet/BuffOpt0/RMSE_para0.25/111111/Fold1/T_19801001_19951001_BS_100_HS_256_RHO_365_NF_12_Buff_365_Mul_16/master.json\n",
      "Using MPS device  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?Batch/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_cudnn_rnn' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cudnn_rnn' is only available for these backends: [MPS, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/mps/MPSFallback.mm:75 [backend fallback]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhbvhy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting predictions, observations for HBV (HydroDL).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     model_output[mod], y_obs[mod] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dp_hbv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model type in `models`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/MODELS/test_dp_HBV.py:319\u001b[0m, in \u001b[0;36mtest_dp_hbv\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m testTuple \u001b[38;5;241m=\u001b[39m (xTest, zTest) \u001b[38;5;66;03m# nparrays xTest: input forcings to HBV; zTest: inputs to gA LSTM to learn parameters\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# forward the model and save results\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestTuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilePathLst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilePathLst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# read out the saved forward predictions\u001b[39;00m\n\u001b[1;32m    323\u001b[0m dataPred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray([obs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], obs\u001b[38;5;241m.\u001b[39mshape  [\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(filePathLst)])\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/hydroDL/model/train.py:286\u001b[0m, in \u001b[0;36mtestModel\u001b[0;34m(model, x, c, batchSize, filePathLst, doMC, outModel, savePath)\u001b[0m\n\u001b[1;32m    282\u001b[0m         ySS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(ySS)\u001b[38;5;241m/\u001b[39mdoMC\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(model) \u001b[38;5;129;01min\u001b[39;00m [rnn\u001b[38;5;241m.\u001b[39mLstmCloseModel, rnn\u001b[38;5;241m.\u001b[39mAnnCloseModel, rnn\u001b[38;5;241m.\u001b[39mCNN1dLSTMmodel, rnn\u001b[38;5;241m.\u001b[39mCNN1dLSTMInmodel,\n\u001b[1;32m    284\u001b[0m                 rnn\u001b[38;5;241m.\u001b[39mCNN1dLCmodel, rnn\u001b[38;5;241m.\u001b[39mCNN1dLCInmodel, rnn\u001b[38;5;241m.\u001b[39mCudnnInvLstmModel,\n\u001b[1;32m    285\u001b[0m                 rnn\u001b[38;5;241m.\u001b[39mMultiInv_HBVModel, rnn\u001b[38;5;241m.\u001b[39mMultiInv_HBVTDModel]:\n\u001b[0;32m--> 286\u001b[0m     yP \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzTest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(model) \u001b[38;5;129;01min\u001b[39;00m [hydroDL\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mLstmCnnForcast]:\n\u001b[1;32m    288\u001b[0m     yP \u001b[38;5;241m=\u001b[39m model(xTest, zTest)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydrodl_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydrodl_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/hydroDL/model/rnn.py:1315\u001b[0m, in \u001b[0;36mMultiInv_HBVModel.forward\u001b[0;34m(self, x, z, doDropMC)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, z, doDropMC\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1315\u001b[0m     Gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstminv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m     Params0 \u001b[38;5;241m=\u001b[39m Gen[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :] \u001b[38;5;66;03m# the last time step as learned parameters\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     ngage \u001b[38;5;241m=\u001b[39m Params0\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydrodl_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydrodl_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/hydroDL/model/rnn.py:369\u001b[0m, in \u001b[0;36mCudnnLstmModel.forward\u001b[0;34m(self, x, doDropMC, dropoutFalse)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, doDropMC\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dropoutFalse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    368\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinearIn(x))\n\u001b[0;32m--> 369\u001b[0m     outLSTM, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoDropMC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoDropMC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropoutFalse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropoutFalse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# outLSTMdr = self.drtest(outLSTM)\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinearOut(outLSTM)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydrodl_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydrodl_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/water/hydro_multimodel/hydroDL/model/rnn.py:321\u001b[0m, in \u001b[0;36mCudnnLstm.forward\u001b[0;34m(self, input, hx, cx, doDropMC, dropoutFalse)\u001b[0m\n\u001b[1;32m    317\u001b[0m     output, hy, cy, reserve, new_weight_buf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_cudnn_rnn(\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28minput\u001b[39m, weight, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, hx, cx, \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# 2 means LSTM\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhiddenSize, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28;01mFalse\u001b[39;00m, (), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     output, hy, cy, reserve, new_weight_buf \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cudnn_rnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 2 means LSTM\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhiddenSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\t\t\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, (hy, cy)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_cudnn_rnn' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cudnn_rnn' is only available for these backends: [MPS, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/mps/MPSFallback.mm:75 [backend fallback]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "######## NOTE: As of now, testing for this cudnn_rnn model cannot be run with mps or cpu on mac m-series architecture ##########\n",
    "\n",
    "\n",
    "loss_funcs = dict()\n",
    "model_output = dict()\n",
    "y_obs = dict()\n",
    "\n",
    "for mod in models:\n",
    "    mod = str(mod)\n",
    "\n",
    "    if mod in ['SACSMA', 'SACSMA_w_snow', 'marrmot_PRMS', 'hbv']:\n",
    "        randomseed_config(seed=args_list[mod][\"randomseed\"][0])\n",
    "        # Creating output directories and adding them to args.\n",
    "        args_list[mod] = create_output_dirs(args_list[mod])\n",
    "        args = args_list[mod]\n",
    "\n",
    "        loss_funcs[mod] = get_lossFun(args_list[mod])\n",
    "\n",
    "        modelFile = os.path.join(args[\"out_dir\"], \"model_Ep\" + str(args['EPOCHS']) + \".pt\")        \n",
    "        models[mod] = torch.load(modelFile)     # Append instanced models.\n",
    "\n",
    "        print(\"Collecting predictions, observations for %s in batches of %i.\" %(mod, args['no_basins']))\n",
    "        model_output[mod], y_obs[mod] = test_differentiable_model(args=args, \n",
    "                                                                  diff_model=models[mod])\n",
    "    elif mod == 'hbvhy':\n",
    "        print(\"Collecting predictions, observations for HBV (HydroDL).\")\n",
    "        model_output[mod], y_obs[mod] = test_dp_hbv()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type in `models`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how to save the model output to a csv or npy so that we don't waste\n",
    "# time having to recollect data every time we start up this notebook.\n",
    "\n",
    "# path = os.path.join(OUT_DIR, \"multimodels\\\\671_sites_dp\\\\output\\\\\")\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# np.save(os.path.join(path, \"preds_671_HBV_SACSMASnow_PRMS.npy\"),model_output)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calFDC(data):\n",
    "    # data = Ngrid * Nday\n",
    "    Ngrid, Nday = data.shape\n",
    "    FDC100 = np.full([Ngrid, 100], np.nan)\n",
    "    for ii in range(Ngrid):\n",
    "        tempdata0 = data[ii, :]\n",
    "        tempdata = tempdata0[~np.isnan(tempdata0)]\n",
    "        # deal with no data case for some gages\n",
    "        if len(tempdata)==0:\n",
    "            tempdata = np.full(Nday, 0)\n",
    "        # sort from large to small\n",
    "        temp_sort = np.sort(tempdata)[::-1]\n",
    "        # select 100 quantile points\n",
    "        Nlen = len(tempdata)\n",
    "        ind = (np.arange(100)/100*Nlen).astype(int)\n",
    "        FDCflow = temp_sort[ind]\n",
    "        if len(FDCflow) != 100:\n",
    "            raise Exception('unknown assimilation variable')\n",
    "        else:\n",
    "            FDC100[ii, :] = FDCflow\n",
    "\n",
    "    return FDC100\n",
    "\n",
    "\n",
    "def statError(pred, target):\n",
    "    ngrid, nt = pred.shape\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    # Bias\n",
    "        Bias = np.nanmean(pred - target, axis=1)\n",
    "        # RMSE\n",
    "        RMSE = np.sqrt(np.nanmean((pred - target)**2, axis=1))\n",
    "        # ubRMSE\n",
    "        predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n",
    "        targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n",
    "        predAnom = pred - predMean\n",
    "        targetAnom = target - targetMean\n",
    "        ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom)**2, axis=1))\n",
    "        # FDC metric\n",
    "        predFDC = calFDC(pred)\n",
    "        targetFDC = calFDC(target)\n",
    "        FDCRMSE = np.sqrt(np.nanmean((predFDC - targetFDC) ** 2, axis=1))\n",
    "    # rho R2 NSE\n",
    "        Corr = np.full(ngrid, np.nan)\n",
    "        CorrSp = np.full(ngrid, np.nan)\n",
    "        R2 = np.full(ngrid, np.nan)\n",
    "        NSE = np.full(ngrid, np.nan)\n",
    "        PBiaslow = np.full(ngrid, np.nan)\n",
    "        PBiashigh = np.full(ngrid, np.nan)\n",
    "        PBias = np.full(ngrid, np.nan)\n",
    "        PBiasother = np.full(ngrid, np.nan)\n",
    "        KGE = np.full(ngrid, np.nan)\n",
    "        KGE12 = np.full(ngrid, np.nan)\n",
    "        RMSElow = np.full(ngrid, np.nan)\n",
    "        RMSEhigh = np.full(ngrid, np.nan)\n",
    "        RMSEother = np.full(ngrid, np.nan)\n",
    "        for k in range(0, ngrid):\n",
    "            x = pred[k, :]\n",
    "            y = target[k, :]\n",
    "            ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n",
    "            if ind.shape[0] > 0:\n",
    "                xx = x[ind]\n",
    "                yy = y[ind]\n",
    "                # percent bias\n",
    "                PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n",
    "\n",
    "                # FHV the peak flows bias 2%\n",
    "                # FLV the low flows bias bottom 30%, log space\n",
    "                pred_sort = np.sort(xx)\n",
    "                target_sort = np.sort(yy)\n",
    "                indexlow = round(0.3 * len(pred_sort))\n",
    "                indexhigh = round(0.98 * len(pred_sort))\n",
    "                lowpred = pred_sort[:indexlow]\n",
    "                highpred = pred_sort[indexhigh:]\n",
    "                otherpred = pred_sort[indexlow:indexhigh]\n",
    "                lowtarget = target_sort[:indexlow]\n",
    "                hightarget = target_sort[indexhigh:]\n",
    "                othertarget = target_sort[indexlow:indexhigh]\n",
    "                PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n",
    "                PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n",
    "                PBiasother[k] = np.sum(otherpred - othertarget) / np.sum(othertarget) * 100\n",
    "                RMSElow[k] = np.sqrt(np.nanmean((lowpred - lowtarget)**2))\n",
    "                RMSEhigh[k] = np.sqrt(np.nanmean((highpred - hightarget)**2))\n",
    "                RMSEother[k] = np.sqrt(np.nanmean((otherpred - othertarget)**2))\n",
    "\n",
    "                if ind.shape[0] > 1:\n",
    "                    # Theoretically at least two points for correlation\n",
    "                    Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n",
    "                    CorrSp[k] = scipy.stats.spearmanr(xx, yy)[0]\n",
    "                    yymean = yy.mean()\n",
    "                    yystd = np.std(yy)\n",
    "                    xxmean = xx.mean()\n",
    "                    xxstd = np.std(xx)\n",
    "                    KGE[k] = 1 - np.sqrt((Corr[k]-1)**2 + (xxstd/yystd-1)**2 + (xxmean/yymean-1)**2)\n",
    "                    KGE12[k] = 1 - np.sqrt((Corr[k] - 1) ** 2 + ((xxstd*yymean)/ (yystd*xxmean) - 1) ** 2 + (xxmean / yymean - 1) ** 2)\n",
    "                    SST = np.sum((yy-yymean)**2)\n",
    "                    SSReg = np.sum((xx-yymean)**2)\n",
    "                    SSRes = np.sum((yy-xx)**2)\n",
    "                    R2[k] = 1-SSRes/SST\n",
    "                    NSE[k] = 1-SSRes/SST\n",
    "\n",
    "    outDict = dict(Bias=Bias, RMSE=RMSE, ubRMSE=ubRMSE, Corr=Corr, CorrSp=CorrSp, R2=R2, NSE=NSE,\n",
    "                   FLV=PBiaslow, FHV=PBiashigh, PBias=PBias, PBiasother=PBiasother, KGE=KGE, KGE12=KGE12, fdcRMSE=FDCRMSE,\n",
    "                   lowRMSE=RMSElow, highRMSE=RMSEhigh, midRMSE=RMSEother)\n",
    "    \n",
    "    return outDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.randomseed_config import randomseed_config\n",
    "\n",
    "\n",
    "class hydroEnsemble(torch.nn.Module):\n",
    "    # Wrapper for multiple hydrologic models.\n",
    "    # In future, consider just passing the models you want to ensemble explicitly.\n",
    "    def __init__(self, num_models, hidden_size, num_layers):\n",
    "        super(hydroEnsemble, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(num_models, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_models)  # Two models (modelA and modelB)\n",
    "\n",
    "        # self.modelA = modelA\n",
    "        # self.modelB = modelB\n",
    "        # self.classifier = torch.nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence tensor with shape (batch_size, sequence_length, num_models)\n",
    "        \n",
    "        # Setting randomseed for deterministic output.\n",
    "        randomseed_config(0)\n",
    "\n",
    "        # Add batch dimension to input and convert to tensor.\n",
    "        x_exp = x.unsqueeze(0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x_exp)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc_out = self.fc(lstm_out)\n",
    "\n",
    "        # Apply softmax activation to obtain weights\n",
    "        weights = torch.nn.functional.softmax(fc_out, dim=2).squeeze()\n",
    "\n",
    "        # Weighted combination of predictions.\n",
    "        weighted_preds = np.multiply(weights.detach(), x)\n",
    "\n",
    "        # Or take the max weight and return the corresponding value.\n",
    "        max_vals, _ = torch.max(weights, dim=1)\n",
    "        btensor = torch.zeros_like(weights)\n",
    "        btensor[weights==max_vals.view(-1,1)] = 1\n",
    "        weighted_preds = np.multiply(btensor.detach(), x)\n",
    "\n",
    "        preds = torch.sum(weighted_preds, dim=1)\n",
    "\n",
    "        # All tensors\n",
    "        # return preds, weights, weighted_preds\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "lstm = hydroEnsemble(num_models=3, hidden_size=256, num_layers=1)\n",
    "\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size = 1\n",
    "sequence_length = 671\n",
    "num_models = 3\n",
    "input_tensor = torch.randn(sequence_length, num_models)\n",
    "input_array = np.random.rand(sequence_length, num_models)\n",
    "\n",
    "# Forward pass through the model\n",
    "preds= lstm(torch.tensor(input_tensor, dtype=torch.float))\n",
    "\n",
    "# Display the results\n",
    "print(\"Weighted Predictions:\")\n",
    "# print(weighted_predictions)\n",
    "print(\"\\nSoftmax Weights:\")\n",
    "print(preds)\n",
    "\n",
    "# input_tensor, torch.tensor(input_array,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_multi(args_list, model_outputs, y_obs_list, ensemble_type='max', out_dir=None):\n",
    "    \"\"\"\n",
    "    Calculate stats for a multimodel ensemble.\n",
    "    \"\"\"\n",
    "    stats_list = dict()\n",
    "\n",
    "    for mod in args_list:\n",
    "        args = args_list[mod]\n",
    "        mod_out = model_outputs[mod]\n",
    "        y_obs = y_obs_list[mod]\n",
    "\n",
    "        if mod in ['SACSMA', 'SACSMA_w_snow', 'marrmot_PRMS', 'hbv']:\n",
    "            # Note for hydrodl HBV, calculations have already been done,\n",
    "            # so skip this step.\n",
    "            \n",
    "            # Saving data            \n",
    "            if out_dir:\n",
    "                path = os.path.join(out_dir,\"models\\\\671_sites_dp\\\\\" + mod + \"\\\\\")\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "                # Test data (obs and model results).\n",
    "                for key in mod_out[0].keys():\n",
    "                    if len(mod_out[0][key].shape) == 3:\n",
    "                        dim = 1\n",
    "                    else:\n",
    "                        dim = 0\n",
    "                    concatenated_tensor = torch.cat([d[key] for d in mod_out], dim=dim)\n",
    "                    file_name = key + \".npy\"\n",
    "                    np.save(os.path.join(path, file_name), concatenated_tensor.numpy())\n",
    "                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n",
    "\n",
    "                # Reading and flow observations.\n",
    "                print(args['target'])\n",
    "                for var in args[\"target\"]:\n",
    "                    item_obs = y_obs[:, :, args[\"target\"].index(var)]\n",
    "                    file_name = var + \".npy\"\n",
    "                    np.save(os.path.join(path, file_name), item_obs)\n",
    "                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n",
    "\n",
    "\n",
    "            ###################### calculations here ######################\n",
    "            predLst = list()\n",
    "            obsLst = list()\n",
    "            flow_sim = torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1)\n",
    "            flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n",
    "            predLst.append(flow_sim.numpy())\n",
    "            obsLst.append(np.expand_dims(flow_obs, 2))\n",
    "\n",
    "            # if args[\"temp_model_name\"] != \"None\":\n",
    "            #     temp_sim = torch.cat([d[\"temp_sim\"] for d in mod_out], dim=1)\n",
    "            #     temp_obs = y_obs[:, :, args[\"target\"].index(\"00010_Mean\")]\n",
    "            #     predLst.append(temp_sim.numpy())\n",
    "            #     obsLst.append(np.expand_dims(temp_obs, 2))\n",
    "\n",
    "            # we need to swap axes here to have [basin, days], and remove redundant \n",
    "            # dimensions with np.squeeze().\n",
    "            stats_list[mod] = [\n",
    "                statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n",
    "                for (x, y) in zip(predLst, obsLst)\n",
    "            ]\n",
    "        elif mod == 'hbvhy':\n",
    "            stats_list[mod] = [statError(mod_out[:,:,0], y_obs.squeeze())]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type in `models`.\")\n",
    "\n",
    "    # Calculating final statistics for the whole set of basins.\n",
    "    name_list = [\"flow\", \"temp\"]\n",
    "    for st, name in zip(stats_list[mod], name_list):\n",
    "        count = 0\n",
    "        mdstd = np.zeros([len(st), 3])\n",
    "        for key in st.keys():\n",
    "            # st contains the statistics on a model run like NSE and KGE.\n",
    "            # Find the best result (e.g., the max, avg, median) and merge from each model.\n",
    "            for i, mod in enumerate(args_list):\n",
    "                if i == 0:\n",
    "                    # temp contains the values of key per basin.\n",
    "                    temp = stats_list[mod][0][key]\n",
    "                    continue\n",
    "                elif i == 1:\n",
    "                    temp = np.stack((temp, stats_list[mod][0][key]), axis=1)\n",
    "                else:\n",
    "                    temp = np.hstack((temp, stats_list[mod][0][key].reshape(-1,1)))\n",
    "\n",
    "            if len(args_list) > 1:\n",
    "                if ensemble_type == 'max':\n",
    "                    # print(temp, key)\n",
    "                    temp = np.amax(temp, axis=1)\n",
    "                    # print(temp, key)\n",
    "                elif ensemble_type == 'avg':\n",
    "                    temp = np.mean(temp, axis=1)\n",
    "                elif ensemble_type == 'median':\n",
    "                    temp = np.median(temp, axis=1)\n",
    "                elif ensemble_type == 'softmax':\n",
    "                    # # Softmax gets relative contributions of each model.\n",
    "                    # weights = torch.nn.functional.softmax(torch.from_numpy(temp), dim=1)\n",
    "                    # temp = np.sum(temp * weights.numpy(), axis=1)\n",
    "\n",
    "                    # Instantiate weighting lstm with softmax.\n",
    "                    lstm = hydroEnsemble(num_models=len(args_list), hidden_size=192, num_layers=3)\n",
    "                    # Forward pass through the model\n",
    "                    temp = lstm(torch.tensor(temp, dtype=torch.float))\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid model ensemble type specified.\")\n",
    "                \n",
    "            median = np.nanmedian(temp)  # abs(i)\n",
    "            std = np.nanstd(temp)  # abs(i)\n",
    "            mean = np.nanmean(temp)  # abs(i)\n",
    "            k = np.array([[median, std, mean]])\n",
    "            mdstd[count] = k\n",
    "            count = count + 1\n",
    "            \n",
    "        # mdstd displays the statistics for each error measure in stats_list.\n",
    "        mdstd = pd.DataFrame(\n",
    "            mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n",
    "        )\n",
    "        # Save the data stats from the training run:\n",
    "        if out_dir and len(args_list) > 1:\n",
    "            path = os.path.join(out_dir, \"multimodels\\\\671_sites_dp\\\\n_\" + ensemble_type + \"\\\\\")\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                    \n",
    "            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + ensemble_type +\".csv\")))\n",
    "        elif out_dir:\n",
    "            path = os.path.join(out_dir, \"models\\\\671_sites_dp\\\\\" + args_list[0] + \"\\\\\")\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name +\".csv\")))\n",
    "        else: continue\n",
    "\n",
    "     # Show boxplots of the results\n",
    "    plt.rcParams[\"font.size\"] = 14\n",
    "    keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "    dataBox = list()\n",
    "    for iS in range(len(keyLst)):\n",
    "        statStr = keyLst[iS]\n",
    "        temp = list()\n",
    "        # for k in range(len(st)):\n",
    "        data = st[statStr]\n",
    "        data = data[~np.isnan(data)]\n",
    "        temp.append(data)\n",
    "        dataBox.append(temp)\n",
    "    labelname = [\n",
    "        \"Hybrid differentiable model\"\n",
    "    ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n",
    "\n",
    "    xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "    fig = plot.plotBoxFig(\n",
    "        dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n",
    "    )\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    boxPlotName = \"PGML\"\n",
    "    fig.suptitle(boxPlotName, fontsize=12)\n",
    "    plt.rcParams[\"font.size\"] = 12\n",
    "    # plt.savefig(\n",
    "    #     os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n",
    "    # )  # , dpi=500\n",
    "    # fig.show()\n",
    "    plt.close()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(\"Testing ended\")\n",
    "\n",
    "    return stats_list, mdstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {'SACSMA':None, 'marrmot_PRMS':None}  # 'hbv':None\n",
    "args_list = {'SACSMA_w_snow': sacsmaSnowArgs}\n",
    "ENSEMBLE_TYPE = 'softmax'\n",
    "stats_list, mtstd = calculate_metrics_multi(args_list, model_outputs=model_output, y_obs_list=y_obs, ensemble_type=ENSEMBLE_TYPE)\n",
    "\n",
    "mtstd['median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MODELS.Differentiable_models import diff_hydro_temp_model\n",
    "diff_hydro_temp_model(prmsArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstd['median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstd['median'], \"SAC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstd['median'], \"PRMS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstd['median'], 'HBV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstd['median'], 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtstd['median'], 'avg all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output['hbv'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output['marrmot_PRMS'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"testing_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_output['marrmot_PRMS'][0].keys():\n",
    "    if len(model_output['marrmot_PRMS'][0][key].shape) == 3:\n",
    "        dim = 1\n",
    "    else:\n",
    "        dim = 0\n",
    "    concatenated_tensor = torch.cat([d[key] for d in model_output], dim=dim)\n",
    "    file_name = key + \".npy\"\n",
    "    np.save(os.path.join(SAVE_PATH, args[\"testing_dir\"], file_name), concatenated_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(args, list_out_diff_model, y_obs, calculate_metrics=True):\n",
    "    for key in list_out_diff_model[0].keys():\n",
    "        if len(list_out_diff_model[0][key].shape) == 3:\n",
    "            dim = 1\n",
    "        else:\n",
    "            dim = 0\n",
    "        concatenated_tensor = torch.cat([d[key] for d in list_out_diff_model], dim=dim)\n",
    "        file_name = key + \".npy\"\n",
    "        np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n",
    "\n",
    "    # Reading flow observation\n",
    "    for var in args[\"target\"]:\n",
    "        item_obs = y_obs[:, :, args[\"target\"].index(var)]\n",
    "        file_name = var + \".npy\"\n",
    "        np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if calculate_metrics == True:\n",
    "        predLst = list()\n",
    "        obsLst = list()\n",
    "        flow_sim = torch.cat([d[\"flow_sim\"] for d in list_out_diff_model], dim=1)\n",
    "        flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n",
    "        predLst.append(flow_sim.numpy())\n",
    "        obsLst.append(np.expand_dims(flow_obs, 2))\n",
    "        if args[\"temp_model_name\"] != \"None\":\n",
    "            temp_sim = torch.cat([d[\"temp_sim\"] for d in list_out_diff_model], dim=1)\n",
    "            temp_obs = y_obs[:, :, args[\"target\"].index(\"00010_Mean\")]\n",
    "            predLst.append(temp_sim.numpy())\n",
    "            obsLst.append(np.expand_dims(temp_obs, 2))\n",
    "        # we need to swap axes here to have [basin, days]\n",
    "        statDictLst = [\n",
    "            stat.statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n",
    "            for (x, y) in zip(predLst, obsLst)\n",
    "        ]\n",
    "        ### save this file\n",
    "        # median and STD calculation\n",
    "        name_list = [\"flow\", \"temp\"]\n",
    "        for st, name in zip(statDictLst, name_list):\n",
    "            count = 0\n",
    "            mdstd = np.zeros([len(st), 3])\n",
    "            for key in st.keys():\n",
    "                median = np.nanmedian(st[key])  # abs(i)\n",
    "                STD = np.nanstd(st[key])  # abs(i)\n",
    "                mean = np.nanmean(st[key])  # abs(i)\n",
    "                k = np.array([[median, STD, mean]])\n",
    "                mdstd[count] = k\n",
    "                count = count + 1\n",
    "            mdstd = pd.DataFrame(\n",
    "                mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n",
    "            )\n",
    "            mdstd.to_csv((os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"mdstd_\" + name + \".csv\")))\n",
    "\n",
    "            # Show boxplots of the results\n",
    "            plt.rcParams[\"font.size\"] = 14\n",
    "            keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "            dataBox = list()\n",
    "            for iS in range(len(keyLst)):\n",
    "                statStr = keyLst[iS]\n",
    "                temp = list()\n",
    "                # for k in range(len(st)):\n",
    "                data = st[statStr]\n",
    "                data = data[~np.isnan(data)]\n",
    "                temp.append(data)\n",
    "                dataBox.append(temp)\n",
    "            labelname = [\n",
    "                \"Hybrid differentiable model\"\n",
    "            ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n",
    "\n",
    "            xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "            fig = plot.plotBoxFig(\n",
    "                dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n",
    "            )\n",
    "            fig.patch.set_facecolor(\"white\")\n",
    "            boxPlotName = \"PGML\"\n",
    "            fig.suptitle(boxPlotName, fontsize=12)\n",
    "            plt.rcParams[\"font.size\"] = 12\n",
    "            plt.savefig(\n",
    "                os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n",
    "            )  # , dpi=500\n",
    "            # fig.show()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_list['SACSMA'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([[1, 2],\n",
    "                   [3, 4],\n",
    "                   [5, 6]])\n",
    "\n",
    "# Create a 1x3 array\n",
    "a2 = np.array([7, 8, 9])\n",
    "\n",
    "a = np.stack((a2, a2*2), axis=1)\n",
    "# print(a)\n",
    "\n",
    "b = np.hstack((a, a2.reshape(-1,1)))\n",
    "\n",
    "\n",
    "b, np.amax(b, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_nse(pred, target):\n",
    "#     \"\"\"\n",
    "#     Currently returns the overall nse per basin.\n",
    "\n",
    "#     Note: modify this to allow per day per basin as well.\n",
    "#     \"\"\"\n",
    "#     # ngrid: number of basins\n",
    "#     # nt: number of timesteps (in days usually)\n",
    "#     ngrid, nt = pred.shape\n",
    "#     NSE = np.full(ngrid, np.nan)\n",
    "\n",
    "#     print(len(pred[670,:]), len(pred))\n",
    "#     for k in range(0, ngrid):\n",
    "#         x = pred[k, :]\n",
    "#         y = target[k, :]\n",
    "#         ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n",
    "#         if ind.shape[0] > 0:\n",
    "#             xx = x[ind]\n",
    "#             yy = y[ind]\n",
    "\n",
    "#             if ind.shape[0] > 1:\n",
    "#                 yymean = yy.mean()\n",
    "            \n",
    "#                 SST = np.sum((yy-yymean)**2)\n",
    "#                 SSRes = np.sum((yy-xx)**2)\n",
    "#                 NSE[k] = 1-SSRes/SST\n",
    "\n",
    "#     return NSE\n",
    "\n",
    "\n",
    "\n",
    "# for i, (x,y) in enumerate(zip(preds, obs)):\n",
    "#     # print(i)\n",
    "#     # print(x.shape)\n",
    "#     nse = calc_nse(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Getting HBV Model Data\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from hydroDL import master, utils\n",
    "from hydroDL.data import camels\n",
    "from hydroDL.master import loadModel\n",
    "from hydroDL.model import train\n",
    "from hydroDL.post import plot, stat\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "## fix the random seeds\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## GPU setting\n",
    "testgpuid = 0\n",
    "torch.cuda.set_device(testgpuid)\n",
    "\n",
    "## setting options, keep the same as your training\n",
    "PUOpt = 0  # 0 for All; 1 for PUB; 2 for PUR;\n",
    "buffOptOri = 0  # original buffOpt, must be same as what you set for training\n",
    "buffOpt = 0  # control load training data 0: do nothing; 1: repeat first year; 2: load one more year\n",
    "forType = 'daymet'\n",
    "\n",
    "## Hyperparameters, keep the same as your training setup\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "Ttrain = [19801001, 19951001]  # Training period\n",
    "# Ttrain = [19891001, 19991001]  # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # dPL Inversion period\n",
    "# Tinv = [19891001, 19991001]  # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters\n",
    "BUFFTIME = 365\n",
    "routing = True\n",
    "Nmul = 16\n",
    "comprout = False\n",
    "compwts = False\n",
    "pcorr = None\n",
    "\n",
    "Ttest = [19951001, 20101001]  # testing period\n",
    "TtestLst = utils.time.tRange2Array(Ttest)\n",
    "TtestLoad = [19951001, 20101001]  \n",
    "\n",
    "testbatch = 50  # forward number of \"testbatch\" basins each time to save GPU memory. You can set this even smaller to save more.\n",
    "testepoch = 50\n",
    "\n",
    "testseed = 111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory of database and saved output dir\n",
    "# Modify this based on your own location of CAMELS dataset and saved models\n",
    "rootDatabase = os.path.join(os.path.sep, 'D:\\data', 'Camels')  # CAMELS dataset root directory\n",
    "camels.initcamels(rootDatabase)  # initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict\n",
    "\n",
    "rootOut = os.path.join(os.path.sep, 'D:\\data\\model_runs', 'rnnStreamflow')  # Model output root directory\n",
    "\n",
    "# CAMLES basin info\n",
    "gageinfo = camels.gageDict\n",
    "hucinfo = gageinfo['huc']\n",
    "gageid = gageinfo['id']\n",
    "gageidLst = gageid.tolist()\n",
    "\n",
    "# same as training, load data based on ALL, PUB, PUR scenarios\n",
    "if PUOpt == 0: # for All the basins\n",
    "    puN = 'ALL'\n",
    "    tarIDLst = [gageidLst]\n",
    "\n",
    "elif PUOpt == 1: # for PUB\n",
    "    puN = 'PUB'\n",
    "    # load the subset ID\n",
    "    # splitPath saves the basin ID of random groups\n",
    "    splitPath = 'PUBsplitLst.txt'\n",
    "    with open(splitPath, 'r') as fp:\n",
    "        testIDLst=json.load(fp)\n",
    "    tarIDLst = testIDLst\n",
    "\n",
    "elif PUOpt == 2: # for PUR\n",
    "    puN = 'PUR'\n",
    "    # Divide CAMELS dataset into 7 PUR regions\n",
    "    # get the id list of each region\n",
    "    regionID = list()\n",
    "    regionNum = list()\n",
    "    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ] # seven regions\n",
    "    for ii in range(len(regionDivide)):\n",
    "        tempcomb = regionDivide[ii]\n",
    "        tempregid = list()\n",
    "        for ih in tempcomb:\n",
    "            tempid = gageid[hucinfo==ih].tolist()\n",
    "            tempregid = tempregid + tempid\n",
    "        regionID.append(tempregid)\n",
    "        regionNum.append(len(tempregid))\n",
    "    tarIDLst = regionID     # List of all basin ID's in the study (671 for full camels).\n",
    "\n",
    "# define the matrix to save results\n",
    "predtestALL = np.full([len(gageid), len(TtestLst), 5], np.nan)\n",
    "obstestALL = np.full([len(gageid), len(TtestLst), 1], np.nan)\n",
    "\n",
    "# this testsave_path should be consistent with where you save your model\n",
    "testsave_path = 'CAMELSDemo/dPLHBV/' + puN + '/Testforc/' + forType + '/BuffOpt' + str(buffOptOri) +\\\n",
    "                '/RMSE_para0.25/'+str(testseed)\n",
    "\n",
    "## load data and test the model\n",
    "nstart = 0\n",
    "logtestIDLst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifold in range(1, len(tarIDLst)+1):\n",
    "    testfold = ifold\n",
    "    TestLS = tarIDLst[testfold - 1]\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "   \n",
    "    TrainLS = gageidLst\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "    nbasin = len(TestLS) # number of basins for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldstr = 'Fold' + str(testfold)\n",
    "exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n",
    "            +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n",
    "# the final path to test with the trained model saved in\n",
    "testout = os.path.join(rootOut, testsave_path, foldstr, exp_info)\n",
    "testmodel = loadModel(testout, epoch=testepoch)\n",
    "testmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TtrainLoad = Ttrain\n",
    "TinvLoad = Tinv\n",
    "\n",
    "varF = ['prcp', 'tmean']\n",
    "varFInv = ['prcp', 'tmean']\n",
    "\n",
    "\n",
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "                   'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "                   'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "                   'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "                   'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "                   'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
    "\n",
    "dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n",
    "forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False)\n",
    "attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = camels.DataframeCamels(tRange=TtestLoad, subset=TestLS, forType=forType)\n",
    "forcTestUN = dfTest.getDataTs(varLst=varF, doNorm=False, rmNan=False)\n",
    "obsTestUN = dfTest.getDataObs(doNorm=False, rmNan=False, basinnorm=False)\n",
    "attrsTestUN = dfTest.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obsTestUN), len(attrnewLst), len(obsTestUN[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = gageinfo['area'][TestInd] # unit km2\n",
    "temparea = np.tile(areas[:, None, None], (1, obsTestUN.shape[1],1))\n",
    "obsTestUN = (obsTestUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3 \n",
    "\n",
    "varLstNL = ['PEVAP']\n",
    "usgsIdLst = gageid\n",
    "if forType == 'maurer':\n",
    "    tPETRange = [19800101, 20090101]\n",
    "else:\n",
    "    tPETRange = [19800101, 20150101]\n",
    "tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n",
    "ntime = len(tPETLst)\n",
    "PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "for k in range(len(usgsIdLst)):\n",
    "    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "    PETfull[k, :, :] = dataTemp\n",
    "\n",
    "TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "TtestLoadLst = utils.time.tRange2Array(TtestLoad)\n",
    "C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "PETUN = PETfull[:, ind2, :]\n",
    "PETUN = PETUN[TrainInd, :, :] # select basins\n",
    "C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "PETInvUN = PETfull[:, ind2inv, :]\n",
    "PETInvUN = PETInvUN[TrainInd, :, :]\n",
    "C, ind1, ind2test = np.intersect1d(TtestLoadLst, tPETLst, return_indices=True)\n",
    "PETTestUN = PETfull[:, ind2test, :]\n",
    "PETTestUN = PETTestUN[TestInd, :, :]\n",
    "\n",
    "# process data, do normalization and remove nan\n",
    "series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n",
    "seriesvarLst = varFInv + ['pet']\n",
    "# load the saved statistics\n",
    "statFile = os.path.join(testout, 'statDict.json')\n",
    "with open(statFile, 'r') as fp:\n",
    "    statDict = json.load(fp)\n",
    "\n",
    "# normalize\n",
    "attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict, toNorm=True)\n",
    "attr_norm[np.isnan(attr_norm)] = 0.0\n",
    "series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True)\n",
    "series_norm[np.isnan(series_norm)] = 0.0\n",
    "\n",
    "attrtest_norm = camels.transNormbyDic(attrsTestUN, attrnewLst, statDict, toNorm=True)\n",
    "attrtest_norm[np.isnan(attrtest_norm)] = 0.0\n",
    "seriestest_inv = np.concatenate([forcTestUN, PETTestUN], axis=2)\n",
    "seriestest_norm = camels.transNormbyDic(seriestest_inv, seriesvarLst, statDict, toNorm=True)\n",
    "seriestest_norm[np.isnan(seriestest_norm)] = 0.0\n",
    "\n",
    "# prepare the inputs\n",
    "zTrain = series_norm\n",
    "xTrain = np.concatenate([forcUN, PETUN], axis=2) # HBV forcing\n",
    "xTrain[np.isnan(xTrain)] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if buffOpt == 1: # repeat the first year for buff\n",
    "    zTrainIn = np.concatenate([zTrain[:,0:BUFFTIME,:], zTrain], axis=1)\n",
    "    xTrainIn = np.concatenate([xTrain[:,0:BUFFTIME,:], xTrain], axis=1) # Bufftime for the first year\n",
    "    # yTrainIn = np.concatenate([obsUN[:,0:BUFFTIME,:], obsUN], axis=1)\n",
    "else: # no repeat, original data\n",
    "    zTrainIn = zTrain\n",
    "    xTrainIn = xTrain\n",
    "    # yTrainIn = obsUN\n",
    "\n",
    "forcTuple = (xTrainIn, zTrainIn)\n",
    "attrs = attr_norm\n",
    "\n",
    "## Prepare the testing data and forward the trained model for testing\n",
    "# TestBuff = 365 # Use 365 days forcing to warm up the model for testing\n",
    "TestBuff = xTrain.shape[1]  # Use the whole training period to warm up the model for testing\n",
    "# TestBuff = len(TtestLoadLst) - len(TtestLst)  # use the redundantly loaded data to warm up\n",
    "\n",
    "# prepare file name to save the testing predictions\n",
    "filePathLst = master.master.namePred(\n",
    "        testout, Ttest, 'All_Buff'+str(TestBuff), epoch=testepoch, targLst=['Qr', 'Q0', 'Q1', 'Q2', 'ET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the inputs for TESTING\n",
    "if PUOpt == 0: # for ALL basins, temporal generalization test\n",
    "    zTest = series_norm  # dPL inversion\n",
    "    xTest = np.concatenate([forcTestUN, PETTestUN], axis=2)  # HBV forcing\n",
    "    # forcings to warm up the model. Here use the forcing of training period to warm up\n",
    "    xTestBuff = xTrain[:, -TestBuff:, :]\n",
    "    xTest = np.concatenate([xTestBuff, xTest], axis=1)\n",
    "    obs = obsTestUN[:, 0:, :]  # starts with 0 when not loading more data before testing period\n",
    "\n",
    "else:  # for PUB and PUR cases, different testing basins. Load more forcings to warm up.\n",
    "    zTest = seriestest_norm[:, 0:TestBuff, :]  # Use the warm-up period forcing as the gA input in zTest\n",
    "    # zTest = seriestest_norm\n",
    "    xTest = np.concatenate([forcTestUN, PETTestUN], axis=2)  # HBV forcing\n",
    "    obs = obsTestUN[:, TestBuff:, :]  # exclude loaded obs in warming up period (first TestBuff days) for evaluation\n",
    "\n",
    "# Use days of TestBuff to initialize the model\n",
    "testmodel.inittime=TestBuff\n",
    "\n",
    "# Final inputs to the test model\n",
    "xTest[np.isnan(xTest)] = 0.0\n",
    "attrtest = attrtest_norm\n",
    "cTemp = np.repeat(\n",
    "    np.reshape(attrtest, [attrtest.shape[0], 1, attrtest.shape[-1]]), zTest.shape[1], axis=1)\n",
    "zTest = np.concatenate([zTest, cTemp], 2) # Add attributes to historical forcings as the inversion part\n",
    "testTuple = (xTest, zTest) # xTest: input forcings to HBV; zTest: inputs to gA LSTM to learn parameters\n",
    "\n",
    "# forward the model and save results\n",
    "train.testModel(\n",
    "    testmodel, testTuple, c=None, batchSize=testbatch, filePathLst=filePathLst)\n",
    "\n",
    "# read out the saved forward predictions\n",
    "dataPred = np.ndarray([obs.shape[0], obs.shape[1], len(filePathLst)])\n",
    "for k in range(len(filePathLst)):\n",
    "    filePath = filePathLst[k]\n",
    "    dataPred[:, :, k] = pd.read_csv(\n",
    "        filePath, dtype=np.float, header=None).values\n",
    "# save the predictions to the big matrix\n",
    "predtestALL[nstart:nstart+nbasin, :, :] = dataPred\n",
    "obstestALL[nstart:nstart+nbasin, :, :] = obs\n",
    "nstart = nstart + nbasin\n",
    "logtestIDLst = logtestIDLst + TestLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtestALL[0], len(predtestALL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## post processing\n",
    "# calculate evaluation metrics\n",
    "evaDict = [stat.statError(predtestALL[:,:,0], obstestALL.squeeze())]  # Q0: the streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(evaDict[0]['NSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_dp_HBV import test_dp_hbv\n",
    "\n",
    "predtestALL, predtestALL = test_dp_hbv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGML_STemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
