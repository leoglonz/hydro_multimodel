{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_hbv as hbvArgs\n",
    "from config.read_configurations import config_prms as prmsArgs\n",
    "from config.read_configurations import config_sacsma as sacsmaArgs\n",
    "from config.read_configurations import config_hbv_hydrodl as hbvhyArgs\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.stats\n",
    "\n",
    "from post import plot\n",
    "from test_dp_HBV import test_dp_hbv\n",
    "\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import create_output_dirs\n",
    "from MODELS.loss_functions.get_loss_function import get_lossFun\n",
    "from core.data_processing.data_loading import loadData\n",
    "from core.data_processing.normalization import transNorm\n",
    "from core.data_processing.model import (\n",
    "    take_sample_test,\n",
    "    converting_flow_from_ft3_per_sec_to_mm_per_day\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "##-----## Multi-model Parameters ##-----##\n",
    "# Setting dictionaries to separately manage each diff model's attributes.\n",
    "models = {'hbv': None, 'SACSMA':None, 'marrmot_PRMS':None}  # 'hbv':None\n",
    "args_list = {'SACSMA': sacsmaArgs, 'marrmot_PRMS':prmsArgs,'hbv': hbvArgs}\n",
    "\n",
    "ENSEMBLE_TYPE = 'median'  # 'avg', 'max', 'softmax'\n",
    "OUT_DIR = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differentiable_model(args, diff_model):\n",
    "    \"\"\"\n",
    "    This function collects and outputs the model predictions and the corresponding\n",
    "    observations needed to run statistical analyses.\n",
    "    \n",
    "    If rerunning testing in a Jupyter environment, you will need to re-import args\n",
    "    as `batch_size` is overwritten in this function and will throw an error if the\n",
    "    overwrite is attempted a second time.\n",
    "    \"\"\"\n",
    "    warm_up = args[\"warm_up\"]\n",
    "    nmul = args[\"nmul\"]\n",
    "    diff_model.eval()\n",
    "    # read data for test time range\n",
    "    dataset_dictionary = loadData(args, trange=args[\"t_test\"])\n",
    "    np.save(os.path.join(args[\"out_dir\"], \"x.npy\"), dataset_dictionary[\"x_NN\"])  # saves with the overlap in the beginning\n",
    "    # normalizing\n",
    "    x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n",
    "    c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n",
    "    c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n",
    "    dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n",
    "    del x_NN_scaled, dataset_dictionary[\"x_NN\"]\n",
    "    # converting the numpy arrays to torch tensors:\n",
    "    for key in dataset_dictionary.keys():\n",
    "        dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n",
    "\n",
    "    # args_mod = args.copy()\n",
    "    args[\"batch_size\"] = args[\"no_basins\"] \n",
    "    nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n",
    "\n",
    "    # Making lists of the start and end indices of the basins for each batch.\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    iS = np.arange(0, ngrid, batch_size)    # Start index list.\n",
    "    iE = np.append(iS[1:], ngrid)   # End.\n",
    "    \n",
    "    list_out_diff_model = []\n",
    "    for i in tqdm(range(0, len(iS)), unit='Batch'):\n",
    "        dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n",
    "\n",
    "        out_diff_model = diff_model(dataset_dictionary_sample)\n",
    "        # Convert all tensors in the dictionary to CPU\n",
    "        out_diff_model_cpu = {key: tensor.cpu().detach() for key, tensor in out_diff_model.items()}\n",
    "        # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n",
    "        list_out_diff_model.append(out_diff_model_cpu)\n",
    "\n",
    "    # getting rid of warm-up period in observation dataset and making the dimension similar to\n",
    "    # converting numpy to tensor\n",
    "    # y_obs = torch.tensor(np.swapaxes(y_obs[:, warm_up:, :], 0, 1), dtype=torch.float32)\n",
    "    # c_hydro_model = torch.tensor(c_hydro_model, dtype=torch.float32)\n",
    "    y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args, \n",
    "                                                           dataset_dictionary[\"c_NN\"],\n",
    "                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])\n",
    "        \n",
    "    return list_out_diff_model, y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting predictions, observations for HBV.\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  22.34215998649597\n",
      "Time to read usgs streamflow:  22.323699474334717\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  22.293091773986816\n",
      "Time to read usgs streamflow:  22.706169366836548\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  22.093195915222168\n",
      "Time to read usgs streamflow:  22.04894709587097\n",
      "read usgs streamflow 18.233071327209473\n",
      "read master file D:\\data\\model_runs\\rnnStreamflow\\CAMELSDemo/dPLHBV/ALL/Testforc/daymet/BuffOpt0/RMSE_para0.25/111111\\Fold1\\T_19801001_19951001_BS_100_HS_256_RHO_365_NF_12_Buff_365_Mul_16\\master.json\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\\\\\data\\\\\\\\model_runs\\\\\\\\PGML_STemp_results\\\\\\\\models\\\\\\\\PRMS_SNTEMP\\\\\\\\671_sites_dp\\\\LSTM_SACSMA_E50_R365_B25_H256_tr1980_1995_n2_0\\\\model_Ep50.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m loss_funcs[mod] \u001b[38;5;241m=\u001b[39m get_lossFun(args_list[mod])\n\u001b[0;32m     16\u001b[0m modelFile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_Ep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m---> 17\u001b[0m models[mod] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelFile\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# Append instanced models.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting predictions, observations for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in batches of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(mod, args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_basins\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     20\u001b[0m model_output[mod], y_obs[mod] \u001b[38;5;241m=\u001b[39m test_differentiable_model(args\u001b[38;5;241m=\u001b[39margs, \n\u001b[0;32m     21\u001b[0m                                                           diff_model\u001b[38;5;241m=\u001b[39mmodels[mod])\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\\\\\data\\\\\\\\model_runs\\\\\\\\PGML_STemp_results\\\\\\\\models\\\\\\\\PRMS_SNTEMP\\\\\\\\671_sites_dp\\\\LSTM_SACSMA_E50_R365_B25_H256_tr1980_1995_n2_0\\\\model_Ep50.pt'"
     ]
    }
   ],
   "source": [
    "loss_funcs = dict()\n",
    "model_output = dict()\n",
    "y_obs = dict()\n",
    "\n",
    "for mod in models:\n",
    "    mod = str(mod)\n",
    "\n",
    "    if mod in ['SACSMA', 'marrmot_PRMS', 'farshid_HBV']:\n",
    "        randomseed_config(seed=args_list[mod][\"randomseed\"][0])\n",
    "        # Creating output directories and adding them to args.\n",
    "        args_list[mod] = create_output_dirs(args_list[mod])\n",
    "        args = args_list[mod]\n",
    "\n",
    "        loss_funcs[mod] = get_lossFun(args_list[mod])\n",
    "\n",
    "        modelFile = os.path.join(args[\"out_dir\"], \"model_Ep\" + str(args['EPOCHS']) + \".pt\")        \n",
    "        models[mod] = torch.load(modelFile)     # Append instanced models.\n",
    "\n",
    "        print(\"Collecting predictions, observations for %s in batches of %i.\" %(mod, args['no_basins']))\n",
    "        model_output[mod], y_obs[mod] = test_differentiable_model(args=args, \n",
    "                                                                  diff_model=models[mod])\n",
    "    elif mod == 'hbv':\n",
    "        print(\"Collecting predictions, observations for HBV.\")\n",
    "        model_output[mod], y_obs[mod] = test_dp_hbv()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type in `models`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calFDC(data):\n",
    "    # data = Ngrid * Nday\n",
    "    Ngrid, Nday = data.shape\n",
    "    FDC100 = np.full([Ngrid, 100], np.nan)\n",
    "    for ii in range(Ngrid):\n",
    "        tempdata0 = data[ii, :]\n",
    "        tempdata = tempdata0[~np.isnan(tempdata0)]\n",
    "        # deal with no data case for some gages\n",
    "        if len(tempdata)==0:\n",
    "            tempdata = np.full(Nday, 0)\n",
    "        # sort from large to small\n",
    "        temp_sort = np.sort(tempdata)[::-1]\n",
    "        # select 100 quantile points\n",
    "        Nlen = len(tempdata)\n",
    "        ind = (np.arange(100)/100*Nlen).astype(int)\n",
    "        FDCflow = temp_sort[ind]\n",
    "        if len(FDCflow) != 100:\n",
    "            raise Exception('unknown assimilation variable')\n",
    "        else:\n",
    "            FDC100[ii, :] = FDCflow\n",
    "\n",
    "    return FDC100\n",
    "\n",
    "\n",
    "def statError(pred, target):\n",
    "    ngrid, nt = pred.shape\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    # Bias\n",
    "        Bias = np.nanmean(pred - target, axis=1)\n",
    "        # RMSE\n",
    "        RMSE = np.sqrt(np.nanmean((pred - target)**2, axis=1))\n",
    "        # ubRMSE\n",
    "        predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n",
    "        targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n",
    "        predAnom = pred - predMean\n",
    "        targetAnom = target - targetMean\n",
    "        ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom)**2, axis=1))\n",
    "        # FDC metric\n",
    "        predFDC = calFDC(pred)\n",
    "        targetFDC = calFDC(target)\n",
    "        FDCRMSE = np.sqrt(np.nanmean((predFDC - targetFDC) ** 2, axis=1))\n",
    "    # rho R2 NSE\n",
    "        Corr = np.full(ngrid, np.nan)\n",
    "        CorrSp = np.full(ngrid, np.nan)\n",
    "        R2 = np.full(ngrid, np.nan)\n",
    "        NSE = np.full(ngrid, np.nan)\n",
    "        PBiaslow = np.full(ngrid, np.nan)\n",
    "        PBiashigh = np.full(ngrid, np.nan)\n",
    "        PBias = np.full(ngrid, np.nan)\n",
    "        PBiasother = np.full(ngrid, np.nan)\n",
    "        KGE = np.full(ngrid, np.nan)\n",
    "        KGE12 = np.full(ngrid, np.nan)\n",
    "        RMSElow = np.full(ngrid, np.nan)\n",
    "        RMSEhigh = np.full(ngrid, np.nan)\n",
    "        RMSEother = np.full(ngrid, np.nan)\n",
    "        for k in range(0, ngrid):\n",
    "            x = pred[k, :]\n",
    "            y = target[k, :]\n",
    "            ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n",
    "            if ind.shape[0] > 0:\n",
    "                xx = x[ind]\n",
    "                yy = y[ind]\n",
    "                # percent bias\n",
    "                PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n",
    "\n",
    "                # FHV the peak flows bias 2%\n",
    "                # FLV the low flows bias bottom 30%, log space\n",
    "                pred_sort = np.sort(xx)\n",
    "                target_sort = np.sort(yy)\n",
    "                indexlow = round(0.3 * len(pred_sort))\n",
    "                indexhigh = round(0.98 * len(pred_sort))\n",
    "                lowpred = pred_sort[:indexlow]\n",
    "                highpred = pred_sort[indexhigh:]\n",
    "                otherpred = pred_sort[indexlow:indexhigh]\n",
    "                lowtarget = target_sort[:indexlow]\n",
    "                hightarget = target_sort[indexhigh:]\n",
    "                othertarget = target_sort[indexlow:indexhigh]\n",
    "                PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n",
    "                PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n",
    "                PBiasother[k] = np.sum(otherpred - othertarget) / np.sum(othertarget) * 100\n",
    "                RMSElow[k] = np.sqrt(np.nanmean((lowpred - lowtarget)**2))\n",
    "                RMSEhigh[k] = np.sqrt(np.nanmean((highpred - hightarget)**2))\n",
    "                RMSEother[k] = np.sqrt(np.nanmean((otherpred - othertarget)**2))\n",
    "\n",
    "                if ind.shape[0] > 1:\n",
    "                    # Theoretically at least two points for correlation\n",
    "                    Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n",
    "                    CorrSp[k] = scipy.stats.spearmanr(xx, yy)[0]\n",
    "                    yymean = yy.mean()\n",
    "                    yystd = np.std(yy)\n",
    "                    xxmean = xx.mean()\n",
    "                    xxstd = np.std(xx)\n",
    "                    KGE[k] = 1 - np.sqrt((Corr[k]-1)**2 + (xxstd/yystd-1)**2 + (xxmean/yymean-1)**2)\n",
    "                    KGE12[k] = 1 - np.sqrt((Corr[k] - 1) ** 2 + ((xxstd*yymean)/ (yystd*xxmean) - 1) ** 2 + (xxmean / yymean - 1) ** 2)\n",
    "                    SST = np.sum((yy-yymean)**2)\n",
    "                    SSReg = np.sum((xx-yymean)**2)\n",
    "                    SSRes = np.sum((yy-xx)**2)\n",
    "                    R2[k] = 1-SSRes/SST\n",
    "                    NSE[k] = 1-SSRes/SST\n",
    "\n",
    "    outDict = dict(Bias=Bias, RMSE=RMSE, ubRMSE=ubRMSE, Corr=Corr, CorrSp=CorrSp, R2=R2, NSE=NSE,\n",
    "                   FLV=PBiaslow, FHV=PBiashigh, PBias=PBias, PBiasother=PBiasother, KGE=KGE, KGE12=KGE12, fdcRMSE=FDCRMSE,\n",
    "                   lowRMSE=RMSElow, highRMSE=RMSEhigh, midRMSE=RMSEother)\n",
    "    \n",
    "    return outDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_multi(args_list, model_outputs, y_obs_list, ensemble_type='max', out_dir=None):\n",
    "    \"\"\"\n",
    "    Calculate stats for a multimodel ensemble.\n",
    "    \"\"\"\n",
    "    stats_list = dict()\n",
    "\n",
    "    for mod in args_list:\n",
    "        args = args_list[mod]\n",
    "        mod_out = model_outputs[mod]\n",
    "        y_obs = y_obs_list[mod]\n",
    "\n",
    "        if mod in ['SACSMA', 'marrmot_PRMS', 'farshid_hbv']:\n",
    "            # Note for hydrodl HBV, calculations have already been done,\n",
    "            # so skip this step.\n",
    "            \n",
    "            # Saving data            \n",
    "            if out_dir:\n",
    "                path = os.path.join(out_dir,\"models\\\\671_sites_dp\\\\\" + mod + \"\\\\\")\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "                # Test data (obs and model results).\n",
    "                for key in mod_out[0].keys():\n",
    "                    if len(mod_out[0][key].shape) == 3:\n",
    "                        dim = 1\n",
    "                    else:\n",
    "                        dim = 0\n",
    "                    concatenated_tensor = torch.cat([d[key] for d in mod_out], dim=dim)\n",
    "                    file_name = key + \".npy\"\n",
    "                    np.save(os.path.join(path, file_name), concatenated_tensor.numpy())\n",
    "                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n",
    "\n",
    "                # Reading and flow observations.\n",
    "                print(args['target'])\n",
    "                for var in args[\"target\"]:\n",
    "                    item_obs = y_obs[:, :, args[\"target\"].index(var)]\n",
    "                    file_name = var + \".npy\"\n",
    "                    np.save(os.path.join(path, file_name), item_obs)\n",
    "                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n",
    "\n",
    "\n",
    "            ###################### calculations here ######################\n",
    "            predLst = list()\n",
    "            obsLst = list()\n",
    "            flow_sim = torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1)\n",
    "            flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n",
    "            predLst.append(flow_sim.numpy())\n",
    "            obsLst.append(np.expand_dims(flow_obs, 2))\n",
    "\n",
    "            # if args[\"temp_model_name\"] != \"None\":\n",
    "            #     temp_sim = torch.cat([d[\"temp_sim\"] for d in mod_out], dim=1)\n",
    "            #     temp_obs = y_obs[:, :, args[\"target\"].index(\"00010_Mean\")]\n",
    "            #     predLst.append(temp_sim.numpy())\n",
    "            #     obsLst.append(np.expand_dims(temp_obs, 2))\n",
    "\n",
    "            # we need to swap axes here to have [basin, days], and remove redundant \n",
    "            # dimensions with np.squeeze().\n",
    "            stats_list[mod] = [\n",
    "                statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n",
    "                for (x, y) in zip(predLst, obsLst)\n",
    "            ]\n",
    "        elif mod == 'hbv':\n",
    "            stats_list[mod] = [statError(mod_out[:,:,0], y_obs.squeeze())]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type in `models`.\")\n",
    "\n",
    "    # Calculating final statistics for the whole set of basins.\n",
    "    name_list = [\"flow\", \"temp\"]\n",
    "    for st, name in zip(stats_list[mod], name_list):\n",
    "        count = 0\n",
    "        mdstd = np.zeros([len(st), 3])\n",
    "        for key in st.keys():\n",
    "            # Find the best result (e.g., the max, avg, median) and merge from each model.\n",
    "            for i, mod in enumerate(args_list):\n",
    "                if i == 0:\n",
    "                    temp = stats_list[mod][0][key]\n",
    "                    continue\n",
    "                elif i == 1:\n",
    "                    temp = np.stack((temp, stats_list[mod][0][key]), axis=1)\n",
    "                else:\n",
    "                    temp = np.hstack((temp, stats_list[mod][0][key].reshape(-1,1)))\n",
    "\n",
    "            if len(args_list) > 1:\n",
    "                if ensemble_type == 'max':\n",
    "                    # print(temp, key)\n",
    "                    temp = np.amax(temp, axis=1)\n",
    "                    # print(temp, key)\n",
    "                elif ensemble_type == 'avg':\n",
    "                    temp = np.mean(temp, axis=1)\n",
    "                elif ensemble_type == 'median':\n",
    "                    temp = np.median(temp, axis=1)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid model ensemble type specified.\")\n",
    "                \n",
    "            median = np.nanmedian(temp)  # abs(i)\n",
    "            std = np.nanstd(temp)  # abs(i)\n",
    "            mean = np.nanmean(temp)  # abs(i)\n",
    "            k = np.array([[median, std, mean]])\n",
    "            mdstd[count] = k\n",
    "            count = count + 1\n",
    "            \n",
    "        # mdstd displays the statistics for each error measure in stats_list.\n",
    "        mdstd = pd.DataFrame(\n",
    "            mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n",
    "        )\n",
    "        # Save the data stats from the training run:\n",
    "        if out_dir and len(args_list) > 1:\n",
    "            path = os.path.join(out_dir, \"multimodels\\\\671_sites_dp\\\\n_\" + ensemble_type + \"\\\\\")\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                    \n",
    "            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + ensemble_type +\".csv\")))\n",
    "        elif out_dir:\n",
    "            path = os.path.join(out_dir, \"models\\\\671_sites_dp\\\\\" + args_list[0] + \"\\\\\")\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name +\".csv\")))\n",
    "        else: continue\n",
    "\n",
    "     # Show boxplots of the results\n",
    "    plt.rcParams[\"font.size\"] = 14\n",
    "    keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "    dataBox = list()\n",
    "    for iS in range(len(keyLst)):\n",
    "        statStr = keyLst[iS]\n",
    "        temp = list()\n",
    "        # for k in range(len(st)):\n",
    "        data = st[statStr]\n",
    "        data = data[~np.isnan(data)]\n",
    "        temp.append(data)\n",
    "        dataBox.append(temp)\n",
    "    labelname = [\n",
    "        \"Hybrid differentiable model\"\n",
    "    ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n",
    "\n",
    "    xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "    fig = plot.plotBoxFig(\n",
    "        dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n",
    "    )\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    boxPlotName = \"PGML\"\n",
    "    fig.suptitle(boxPlotName, fontsize=12)\n",
    "    plt.rcParams[\"font.size\"] = 12\n",
    "    # plt.savefig(\n",
    "    #     os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n",
    "    # )  # , dpi=500\n",
    "    # fig.show()\n",
    "    plt.close()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(\"Testing ended\")\n",
    "\n",
    "    return stats_list, mdstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00060_Mean']\n",
      "['00060_Mean']\n"
     ]
    }
   ],
   "source": [
    "# models = {'SACSMA':None, 'marrmot_PRMS':None}  # 'hbv':None\n",
    "# args_list = {'marrmot_PRMS':prmsArgs,'hbv': hbvArgs}\n",
    "\n",
    "stats_list, mtstd = calculate_metrics_multi(args_list, model_outputs=model_output, y_obs_list=y_obs, ensemble_type=ENSEMBLE_TYPE, out_dir=OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bias           0.023638\n",
       " RMSE           1.375866\n",
       " ubRMSE         1.364707\n",
       " Corr           0.766663\n",
       " CorrSp         0.789618\n",
       " R2             0.531261\n",
       " NSE            0.531261\n",
       " FLV           82.221192\n",
       " FHV          -14.811704\n",
       " PBias          2.451530\n",
       " PBiasother     4.070650\n",
       " KGE            0.633087\n",
       " KGE12          0.620712\n",
       " fdcRMSE        1.362580\n",
       " lowRMSE        0.105923\n",
       " highRMSE       3.322313\n",
       " midRMSE        0.311551\n",
       " Name: median, dtype: float64,\n",
       " 'SAC')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtstd['median'], \"SAC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bias          -0.012955\n",
       " RMSE           1.187929\n",
       " ubRMSE         1.181253\n",
       " Corr           0.845166\n",
       " CorrSp         0.808476\n",
       " R2             0.684968\n",
       " NSE            0.684968\n",
       " FLV           16.443141\n",
       " FHV           -8.544982\n",
       " PBias         -1.679518\n",
       " PBiasother    -0.010780\n",
       " KGE            0.750555\n",
       " KGE12          0.762107\n",
       " fdcRMSE        0.967566\n",
       " lowRMSE        0.056080\n",
       " highRMSE       2.605308\n",
       " midRMSE        0.207460\n",
       " Name: median, dtype: float64,\n",
       " 'PRMS')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtstd['median'], \"PRMS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bias           0.030976\n",
       " RMSE           1.186985\n",
       " ubRMSE         1.171042\n",
       " Corr           0.856491\n",
       " CorrSp         0.854078\n",
       " R2             0.713660\n",
       " NSE            0.713660\n",
       " FLV           51.890851\n",
       " FHV           -8.908644\n",
       " PBias          3.470816\n",
       " PBiasother     6.365034\n",
       " KGE            0.734564\n",
       " KGE12          0.728974\n",
       " fdcRMSE        1.057993\n",
       " lowRMSE        0.076760\n",
       " highRMSE       2.656486\n",
       " midRMSE        0.228451\n",
       " Name: median, dtype: float64,\n",
       " 'HBV')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtstd['median'], 'HBV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bias            0.091692\n",
       " RMSE            1.415431\n",
       " ubRMSE          1.406244\n",
       " Corr            0.861052\n",
       " CorrSp          0.859163\n",
       " R2              0.726301\n",
       " NSE             0.726301\n",
       " FLV           130.182993\n",
       " FHV            -2.363760\n",
       " PBias          10.018780\n",
       " PBiasother     12.789425\n",
       " KGE             0.778676\n",
       " KGE12           0.784603\n",
       " fdcRMSE         1.661377\n",
       " lowRMSE         0.171308\n",
       " highRMSE        4.083405\n",
       " midRMSE         0.357401\n",
       " Name: median, dtype: float64,\n",
       " 'all')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtstd['median'], 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bias           0.013683\n",
       " RMSE           1.184225\n",
       " ubRMSE         1.174306\n",
       " Corr           0.850140\n",
       " CorrSp         0.832750\n",
       " R2             0.698096\n",
       " NSE            0.698096\n",
       " FLV           41.489862\n",
       " FHV           -8.064903\n",
       " PBias          1.489761\n",
       " PBiasother     3.476490\n",
       " KGE            0.742377\n",
       " KGE12          0.744687\n",
       " fdcRMSE        1.058350\n",
       " lowRMSE        0.073452\n",
       " highRMSE       2.618986\n",
       " midRMSE        0.221791\n",
       " Name: median, dtype: float64,\n",
       " 'avg all')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtstd['median'], 'avg all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3000539e-02, 0.0000000e+00, 7.9583000e-03, 2.9776424e-01,\n",
       "        1.7031918e+00],\n",
       "       [5.8530558e-02, 0.0000000e+00, 1.3215601e-03, 2.7569354e-01,\n",
       "        1.6276169e+00],\n",
       "       [1.1220434e-01, 0.0000000e+00, 0.0000000e+00, 2.5547340e-01,\n",
       "        1.3423388e+00],\n",
       "       ...,\n",
       "       [1.3872437e+00, 0.0000000e+00, 8.0913790e-01, 1.4040515e+00,\n",
       "        1.8904784e+00],\n",
       "       [1.5643985e+00, 0.0000000e+00, 1.0355383e+00, 1.5936201e+00,\n",
       "        1.9796518e+00],\n",
       "       [1.8524745e+00, 8.7666190e-02, 2.0063870e+00, 1.7806802e+00,\n",
       "        2.0639358e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output['hbv'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['flow_sim', 'srflow', 'ssflow', 'gwflow', 'sink', 'PET_hydro', 'AET_hydro', 'BFI_sim'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output['marrmot_PRMS'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ts1995_2010'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"testing_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m concatenated_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m model_output], dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[0;32m      7\u001b[0m file_name \u001b[38;5;241m=\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_PATH, args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], file_name), concatenated_tensor\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m concatenated_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m model_output], dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[0;32m      7\u001b[0m file_name \u001b[38;5;241m=\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_PATH, args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], file_name), concatenated_tensor\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for key in model_output['marrmot_PRMS'][0].keys():\n",
    "    if len(model_output['marrmot_PRMS'][0][key].shape) == 3:\n",
    "        dim = 1\n",
    "    else:\n",
    "        dim = 0\n",
    "    concatenated_tensor = torch.cat([d[key] for d in model_output], dim=dim)\n",
    "    file_name = key + \".npy\"\n",
    "    np.save(os.path.join(SAVE_PATH, args[\"testing_dir\"], file_name), concatenated_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(args, list_out_diff_model, y_obs, calculate_metrics=True):\n",
    "    for key in list_out_diff_model[0].keys():\n",
    "        if len(list_out_diff_model[0][key].shape) == 3:\n",
    "            dim = 1\n",
    "        else:\n",
    "            dim = 0\n",
    "        concatenated_tensor = torch.cat([d[key] for d in list_out_diff_model], dim=dim)\n",
    "        file_name = key + \".npy\"\n",
    "        np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n",
    "\n",
    "    # Reading flow observation\n",
    "    for var in args[\"target\"]:\n",
    "        item_obs = y_obs[:, :, args[\"target\"].index(var)]\n",
    "        file_name = var + \".npy\"\n",
    "        np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if calculate_metrics == True:\n",
    "        predLst = list()\n",
    "        obsLst = list()\n",
    "        flow_sim = torch.cat([d[\"flow_sim\"] for d in list_out_diff_model], dim=1)\n",
    "        flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n",
    "        predLst.append(flow_sim.numpy())\n",
    "        obsLst.append(np.expand_dims(flow_obs, 2))\n",
    "        if args[\"temp_model_name\"] != \"None\":\n",
    "            temp_sim = torch.cat([d[\"temp_sim\"] for d in list_out_diff_model], dim=1)\n",
    "            temp_obs = y_obs[:, :, args[\"target\"].index(\"00010_Mean\")]\n",
    "            predLst.append(temp_sim.numpy())\n",
    "            obsLst.append(np.expand_dims(temp_obs, 2))\n",
    "        # we need to swap axes here to have [basin, days]\n",
    "        statDictLst = [\n",
    "            stat.statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n",
    "            for (x, y) in zip(predLst, obsLst)\n",
    "        ]\n",
    "        ### save this file\n",
    "        # median and STD calculation\n",
    "        name_list = [\"flow\", \"temp\"]\n",
    "        for st, name in zip(statDictLst, name_list):\n",
    "            count = 0\n",
    "            mdstd = np.zeros([len(st), 3])\n",
    "            for key in st.keys():\n",
    "                median = np.nanmedian(st[key])  # abs(i)\n",
    "                STD = np.nanstd(st[key])  # abs(i)\n",
    "                mean = np.nanmean(st[key])  # abs(i)\n",
    "                k = np.array([[median, STD, mean]])\n",
    "                mdstd[count] = k\n",
    "                count = count + 1\n",
    "            mdstd = pd.DataFrame(\n",
    "                mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n",
    "            )\n",
    "            mdstd.to_csv((os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"mdstd_\" + name + \".csv\")))\n",
    "\n",
    "            # Show boxplots of the results\n",
    "            plt.rcParams[\"font.size\"] = 14\n",
    "            keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "            dataBox = list()\n",
    "            for iS in range(len(keyLst)):\n",
    "                statStr = keyLst[iS]\n",
    "                temp = list()\n",
    "                # for k in range(len(st)):\n",
    "                data = st[statStr]\n",
    "                data = data[~np.isnan(data)]\n",
    "                temp.append(data)\n",
    "                dataBox.append(temp)\n",
    "            labelname = [\n",
    "                \"Hybrid differentiable model\"\n",
    "            ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n",
    "\n",
    "            xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n",
    "            fig = plot.plotBoxFig(\n",
    "                dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n",
    "            )\n",
    "            fig.patch.set_facecolor(\"white\")\n",
    "            boxPlotName = \"PGML\"\n",
    "            fig.suptitle(boxPlotName, fontsize=12)\n",
    "            plt.rcParams[\"font.size\"] = 12\n",
    "            plt.savefig(\n",
    "                os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n",
    "            )  # , dpi=500\n",
    "            # fig.show()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Bias', 'RMSE', 'ubRMSE', 'Corr', 'CorrSp', 'R2', 'NSE', 'FLV', 'FHV', 'PBias', 'PBiasother', 'KGE', 'KGE12', 'fdcRMSE', 'lowRMSE', 'highRMSE', 'midRMSE'])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_list['SACSMA'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7, 14,  7],\n",
       "        [ 8, 16,  8],\n",
       "        [ 9, 18,  9]]),\n",
       " array([14, 16, 18]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.array([[1, 2],\n",
    "                   [3, 4],\n",
    "                   [5, 6]])\n",
    "\n",
    "# Create a 1x3 array\n",
    "a2 = np.array([7, 8, 9])\n",
    "\n",
    "a = np.stack((a2, a2*2), axis=1)\n",
    "# print(a)\n",
    "\n",
    "b = np.hstack((a, a2.reshape(-1,1)))\n",
    "\n",
    "\n",
    "b, np.amax(b, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_nse(pred, target):\n",
    "#     \"\"\"\n",
    "#     Currently returns the overall nse per basin.\n",
    "\n",
    "#     Note: modify this to allow per day per basin as well.\n",
    "#     \"\"\"\n",
    "#     # ngrid: number of basins\n",
    "#     # nt: number of timesteps (in days usually)\n",
    "#     ngrid, nt = pred.shape\n",
    "#     NSE = np.full(ngrid, np.nan)\n",
    "\n",
    "#     print(len(pred[670,:]), len(pred))\n",
    "#     for k in range(0, ngrid):\n",
    "#         x = pred[k, :]\n",
    "#         y = target[k, :]\n",
    "#         ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n",
    "#         if ind.shape[0] > 0:\n",
    "#             xx = x[ind]\n",
    "#             yy = y[ind]\n",
    "\n",
    "#             if ind.shape[0] > 1:\n",
    "#                 yymean = yy.mean()\n",
    "            \n",
    "#                 SST = np.sum((yy-yymean)**2)\n",
    "#                 SSRes = np.sum((yy-xx)**2)\n",
    "#                 NSE[k] = 1-SSRes/SST\n",
    "\n",
    "#     return NSE\n",
    "\n",
    "\n",
    "\n",
    "# for i, (x,y) in enumerate(zip(preds, obs)):\n",
    "#     # print(i)\n",
    "#     # print(x.shape)\n",
    "#     nse = calc_nse(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Getting HBV Model Data\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from hydroDL import master, utils\n",
    "from hydroDL.data import camels\n",
    "from hydroDL.master import loadModel\n",
    "from hydroDL.model import train\n",
    "from hydroDL.post import plot, stat\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "## fix the random seeds\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## GPU setting\n",
    "testgpuid = 0\n",
    "torch.cuda.set_device(testgpuid)\n",
    "\n",
    "## setting options, keep the same as your training\n",
    "PUOpt = 0  # 0 for All; 1 for PUB; 2 for PUR;\n",
    "buffOptOri = 0  # original buffOpt, must be same as what you set for training\n",
    "buffOpt = 0  # control load training data 0: do nothing; 1: repeat first year; 2: load one more year\n",
    "forType = 'daymet'\n",
    "\n",
    "## Hyperparameters, keep the same as your training setup\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "Ttrain = [19801001, 19951001]  # Training period\n",
    "# Ttrain = [19891001, 19991001]  # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # dPL Inversion period\n",
    "# Tinv = [19891001, 19991001]  # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters\n",
    "BUFFTIME = 365\n",
    "routing = True\n",
    "Nmul = 16\n",
    "comprout = False\n",
    "compwts = False\n",
    "pcorr = None\n",
    "\n",
    "Ttest = [19951001, 20101001]  # testing period\n",
    "TtestLst = utils.time.tRange2Array(Ttest)\n",
    "TtestLoad = [19951001, 20101001]  \n",
    "\n",
    "testbatch = 50  # forward number of \"testbatch\" basins each time to save GPU memory. You can set this even smaller to save more.\n",
    "testepoch = 50\n",
    "\n",
    "testseed = 111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory of database and saved output dir\n",
    "# Modify this based on your own location of CAMELS dataset and saved models\n",
    "rootDatabase = os.path.join(os.path.sep, 'D:\\data', 'Camels')  # CAMELS dataset root directory\n",
    "camels.initcamels(rootDatabase)  # initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict\n",
    "\n",
    "rootOut = os.path.join(os.path.sep, 'D:\\data\\model_runs', 'rnnStreamflow')  # Model output root directory\n",
    "\n",
    "# CAMLES basin info\n",
    "gageinfo = camels.gageDict\n",
    "hucinfo = gageinfo['huc']\n",
    "gageid = gageinfo['id']\n",
    "gageidLst = gageid.tolist()\n",
    "\n",
    "# same as training, load data based on ALL, PUB, PUR scenarios\n",
    "if PUOpt == 0: # for All the basins\n",
    "    puN = 'ALL'\n",
    "    tarIDLst = [gageidLst]\n",
    "\n",
    "elif PUOpt == 1: # for PUB\n",
    "    puN = 'PUB'\n",
    "    # load the subset ID\n",
    "    # splitPath saves the basin ID of random groups\n",
    "    splitPath = 'PUBsplitLst.txt'\n",
    "    with open(splitPath, 'r') as fp:\n",
    "        testIDLst=json.load(fp)\n",
    "    tarIDLst = testIDLst\n",
    "\n",
    "elif PUOpt == 2: # for PUR\n",
    "    puN = 'PUR'\n",
    "    # Divide CAMELS dataset into 7 PUR regions\n",
    "    # get the id list of each region\n",
    "    regionID = list()\n",
    "    regionNum = list()\n",
    "    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ] # seven regions\n",
    "    for ii in range(len(regionDivide)):\n",
    "        tempcomb = regionDivide[ii]\n",
    "        tempregid = list()\n",
    "        for ih in tempcomb:\n",
    "            tempid = gageid[hucinfo==ih].tolist()\n",
    "            tempregid = tempregid + tempid\n",
    "        regionID.append(tempregid)\n",
    "        regionNum.append(len(tempregid))\n",
    "    tarIDLst = regionID     # List of all basin ID's in the study (671 for full camels).\n",
    "\n",
    "# define the matrix to save results\n",
    "predtestALL = np.full([len(gageid), len(TtestLst), 5], np.nan)\n",
    "obstestALL = np.full([len(gageid), len(TtestLst), 1], np.nan)\n",
    "\n",
    "# this testsave_path should be consistent with where you save your model\n",
    "testsave_path = 'CAMELSDemo/dPLHBV/' + puN + '/Testforc/' + forType + '/BuffOpt' + str(buffOptOri) +\\\n",
    "                '/RMSE_para0.25/'+str(testseed)\n",
    "\n",
    "## load data and test the model\n",
    "nstart = 0\n",
    "logtestIDLst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tarIDLst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ifold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtarIDLst\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     testfold \u001b[38;5;241m=\u001b[39m ifold\n\u001b[0;32m      3\u001b[0m     TestLS \u001b[38;5;241m=\u001b[39m tarIDLst[testfold \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tarIDLst' is not defined"
     ]
    }
   ],
   "source": [
    "for ifold in range(1, len(tarIDLst)+1):\n",
    "    testfold = ifold\n",
    "    TestLS = tarIDLst[testfold - 1]\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "   \n",
    "    TrainLS = gageidLst\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "    nbasin = len(TestLS) # number of basins for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiInv_HBVModel(\n",
       "  (lstminv): CudnnLstmModel(\n",
       "    (linearIn): Linear(in_features=38, out_features=256, bias=True)\n",
       "    (lstm): CudnnLstm()\n",
       "    (linearOut): Linear(in_features=256, out_features=194, bias=True)\n",
       "  )\n",
       "  (HBV): HBVMul()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldstr = 'Fold' + str(testfold)\n",
    "exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n",
    "            +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n",
    "# the final path to test with the trained model saved in\n",
    "testout = os.path.join(rootOut, testsave_path, foldstr, exp_info)\n",
    "testmodel = loadModel(testout, epoch=testepoch)\n",
    "testmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  26.232120752334595\n",
      "Time to read usgs streamflow:  20.120174407958984\n"
     ]
    }
   ],
   "source": [
    "TtrainLoad = Ttrain\n",
    "TinvLoad = Tinv\n",
    "\n",
    "varF = ['prcp', 'tmean']\n",
    "varFInv = ['prcp', 'tmean']\n",
    "\n",
    "\n",
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "                   'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "                   'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "                   'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "                   'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "                   'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
    "\n",
    "dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n",
    "forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  20.086344957351685\n",
      "Time to read usgs streamflow:  20.02952527999878\n"
     ]
    }
   ],
   "source": [
    "dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False)\n",
    "attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  20.367339611053467\n",
      "Time to read usgs streamflow:  20.11677098274231\n",
      "read usgs streamflow 21.04769468307495\n"
     ]
    }
   ],
   "source": [
    "dfTest = camels.DataframeCamels(tRange=TtestLoad, subset=TestLS, forType=forType)\n",
    "forcTestUN = dfTest.getDataTs(varLst=varF, doNorm=False, rmNan=False)\n",
    "obsTestUN = dfTest.getDataObs(doNorm=False, rmNan=False, basinnorm=False)\n",
    "attrsTestUN = dfTest.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(671, 35, 5479)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obsTestUN), len(attrnewLst), len(obsTestUN[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = gageinfo['area'][TestInd] # unit km2\n",
    "temparea = np.tile(areas[:, None, None], (1, obsTestUN.shape[1],1))\n",
    "obsTestUN = (obsTestUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3 \n",
    "\n",
    "varLstNL = ['PEVAP']\n",
    "usgsIdLst = gageid\n",
    "if forType == 'maurer':\n",
    "    tPETRange = [19800101, 20090101]\n",
    "else:\n",
    "    tPETRange = [19800101, 20150101]\n",
    "tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n",
    "ntime = len(tPETLst)\n",
    "PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "for k in range(len(usgsIdLst)):\n",
    "    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "    PETfull[k, :, :] = dataTemp\n",
    "\n",
    "TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "TtestLoadLst = utils.time.tRange2Array(TtestLoad)\n",
    "C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "PETUN = PETfull[:, ind2, :]\n",
    "PETUN = PETUN[TrainInd, :, :] # select basins\n",
    "C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "PETInvUN = PETfull[:, ind2inv, :]\n",
    "PETInvUN = PETInvUN[TrainInd, :, :]\n",
    "C, ind1, ind2test = np.intersect1d(TtestLoadLst, tPETLst, return_indices=True)\n",
    "PETTestUN = PETfull[:, ind2test, :]\n",
    "PETTestUN = PETTestUN[TestInd, :, :]\n",
    "\n",
    "# process data, do normalization and remove nan\n",
    "series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n",
    "seriesvarLst = varFInv + ['pet']\n",
    "# load the saved statistics\n",
    "statFile = os.path.join(testout, 'statDict.json')\n",
    "with open(statFile, 'r') as fp:\n",
    "    statDict = json.load(fp)\n",
    "\n",
    "# normalize\n",
    "attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict, toNorm=True)\n",
    "attr_norm[np.isnan(attr_norm)] = 0.0\n",
    "series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True)\n",
    "series_norm[np.isnan(series_norm)] = 0.0\n",
    "\n",
    "attrtest_norm = camels.transNormbyDic(attrsTestUN, attrnewLst, statDict, toNorm=True)\n",
    "attrtest_norm[np.isnan(attrtest_norm)] = 0.0\n",
    "seriestest_inv = np.concatenate([forcTestUN, PETTestUN], axis=2)\n",
    "seriestest_norm = camels.transNormbyDic(seriestest_inv, seriesvarLst, statDict, toNorm=True)\n",
    "seriestest_norm[np.isnan(seriestest_norm)] = 0.0\n",
    "\n",
    "# prepare the inputs\n",
    "zTrain = series_norm\n",
    "xTrain = np.concatenate([forcUN, PETUN], axis=2) # HBV forcing\n",
    "xTrain[np.isnan(xTrain)] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read master file D:\\data\\model_runs\\rnnStreamflow\\CAMELSDemo/dPLHBV/ALL/Testforc/daymet/BuffOpt0/RMSE_para0.25/111111\\Fold1\\T_19801001_19951001_BS_100_HS_256_RHO_365_NF_12_Buff_365_Mul_16\\master.json\n"
     ]
    }
   ],
   "source": [
    "if buffOpt == 1: # repeat the first year for buff\n",
    "    zTrainIn = np.concatenate([zTrain[:,0:BUFFTIME,:], zTrain], axis=1)\n",
    "    xTrainIn = np.concatenate([xTrain[:,0:BUFFTIME,:], xTrain], axis=1) # Bufftime for the first year\n",
    "    # yTrainIn = np.concatenate([obsUN[:,0:BUFFTIME,:], obsUN], axis=1)\n",
    "else: # no repeat, original data\n",
    "    zTrainIn = zTrain\n",
    "    xTrainIn = xTrain\n",
    "    # yTrainIn = obsUN\n",
    "\n",
    "forcTuple = (xTrainIn, zTrainIn)\n",
    "attrs = attr_norm\n",
    "\n",
    "## Prepare the testing data and forward the trained model for testing\n",
    "# TestBuff = 365 # Use 365 days forcing to warm up the model for testing\n",
    "TestBuff = xTrain.shape[1]  # Use the whole training period to warm up the model for testing\n",
    "# TestBuff = len(TtestLoadLst) - len(TtestLst)  # use the redundantly loaded data to warm up\n",
    "\n",
    "# prepare file name to save the testing predictions\n",
    "filePathLst = master.master.namePred(\n",
    "        testout, Ttest, 'All_Buff'+str(TestBuff), epoch=testepoch, targLst=['Qr', 'Q0', 'Q1', 'Q2', 'ET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code_repos\\hydro_ensemble\\hydroDL\\model\\rnn.py:321: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeoLo\\AppData\\Local\\Temp\\ipykernel_32520\\4105741686.py:36: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  filePath, dtype=np.float, header=None).values\n"
     ]
    }
   ],
   "source": [
    "# prepare the inputs for TESTING\n",
    "if PUOpt == 0: # for ALL basins, temporal generalization test\n",
    "    zTest = series_norm  # dPL inversion\n",
    "    xTest = np.concatenate([forcTestUN, PETTestUN], axis=2)  # HBV forcing\n",
    "    # forcings to warm up the model. Here use the forcing of training period to warm up\n",
    "    xTestBuff = xTrain[:, -TestBuff:, :]\n",
    "    xTest = np.concatenate([xTestBuff, xTest], axis=1)\n",
    "    obs = obsTestUN[:, 0:, :]  # starts with 0 when not loading more data before testing period\n",
    "\n",
    "else:  # for PUB and PUR cases, different testing basins. Load more forcings to warm up.\n",
    "    zTest = seriestest_norm[:, 0:TestBuff, :]  # Use the warm-up period forcing as the gA input in zTest\n",
    "    # zTest = seriestest_norm\n",
    "    xTest = np.concatenate([forcTestUN, PETTestUN], axis=2)  # HBV forcing\n",
    "    obs = obsTestUN[:, TestBuff:, :]  # exclude loaded obs in warming up period (first TestBuff days) for evaluation\n",
    "\n",
    "# Use days of TestBuff to initialize the model\n",
    "testmodel.inittime=TestBuff\n",
    "\n",
    "# Final inputs to the test model\n",
    "xTest[np.isnan(xTest)] = 0.0\n",
    "attrtest = attrtest_norm\n",
    "cTemp = np.repeat(\n",
    "    np.reshape(attrtest, [attrtest.shape[0], 1, attrtest.shape[-1]]), zTest.shape[1], axis=1)\n",
    "zTest = np.concatenate([zTest, cTemp], 2) # Add attributes to historical forcings as the inversion part\n",
    "testTuple = (xTest, zTest) # xTest: input forcings to HBV; zTest: inputs to gA LSTM to learn parameters\n",
    "\n",
    "# forward the model and save results\n",
    "train.testModel(\n",
    "    testmodel, testTuple, c=None, batchSize=testbatch, filePathLst=filePathLst)\n",
    "\n",
    "# read out the saved forward predictions\n",
    "dataPred = np.ndarray([obs.shape[0], obs.shape[1], len(filePathLst)])\n",
    "for k in range(len(filePathLst)):\n",
    "    filePath = filePathLst[k]\n",
    "    dataPred[:, :, k] = pd.read_csv(\n",
    "        filePath, dtype=np.float, header=None).values\n",
    "# save the predictions to the big matrix\n",
    "predtestALL[nstart:nstart+nbasin, :, :] = dataPred\n",
    "obstestALL[nstart:nstart+nbasin, :, :] = obs\n",
    "nstart = nstart + nbasin\n",
    "logtestIDLst = logtestIDLst + TestLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.3000539e-02, 0.0000000e+00, 7.9583000e-03, 2.9776424e-01,\n",
       "         1.7031918e+00],\n",
       "        [5.8530558e-02, 0.0000000e+00, 1.3215601e-03, 2.7569354e-01,\n",
       "         1.6276169e+00],\n",
       "        [1.1220434e-01, 0.0000000e+00, 0.0000000e+00, 2.5547340e-01,\n",
       "         1.3423388e+00],\n",
       "        ...,\n",
       "        [1.3872437e+00, 0.0000000e+00, 8.0913790e-01, 1.4040515e+00,\n",
       "         1.8904784e+00],\n",
       "        [1.5643985e+00, 0.0000000e+00, 1.0355383e+00, 1.5936201e+00,\n",
       "         1.9796518e+00],\n",
       "        [1.8524745e+00, 8.7666190e-02, 2.0063870e+00, 1.7806802e+00,\n",
       "         2.0639358e+00]]),\n",
       " 5479)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predtestALL[0], len(predtestALL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code_repos\\hydro_ensemble\\hydroDL\\post\\stat.py:60: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n"
     ]
    }
   ],
   "source": [
    "## post processing\n",
    "# calculate evaluation metrics\n",
    "evaDict = [stat.statError(predtestALL[:,:,0], obstestALL.squeeze())]  # Q0: the streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaDict[0]['NSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  20.30001163482666\n",
      "Time to read usgs streamflow:  20.10430121421814\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  20.163031578063965\n",
      "Time to read usgs streamflow:  20.467963218688965\n",
      "daymet tmean was used!\n",
      "Time to read usgs streamflow:  20.458739519119263\n",
      "Time to read usgs streamflow:  20.346842050552368\n",
      "read usgs streamflow 16.54717493057251\n",
      "read master file D:\\data\\model_runs\\rnnStreamflow\\CAMELSDemo/dPLHBV/ALL/Testforc/daymet/BuffOpt0/RMSE_para0.25/111111\\Fold1\\T_19801001_19951001_BS_100_HS_256_RHO_365_NF_12_Buff_365_Mul_16\\master.json\n",
      "batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code_repos\\hydro_ensemble\\hydroDL\\model\\rnn.py:321: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n"
     ]
    }
   ],
   "source": [
    "from test_dp_HBV import test_dp_hbv\n",
    "\n",
    "predtestALL, predtestALL = test_dp_hbv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGML_STemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
