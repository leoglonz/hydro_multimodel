{"cells":[{"cell_type":"markdown","metadata":{"id":"q1jGJFeaF0oG"},"source":["### Running bulk of multimodel testing\n","\n","This is equivalent to that present in the multimodel wrapper.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1832,"status":"ok","timestamp":1707705234766,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"2Vt-fsprDO_u","outputId":"c300c26f-3e5f-4134-c3f5-a0182475218a"},"outputs":[],"source":["from config.read_configurations import config_hbv as hbvArgs\n","from config.read_configurations import config_prms as prmsArgs\n","from config.read_configurations import config_sacsma as sacsmaArgs\n","from config.read_configurations import config_sacsma_snow as sacsmaSnowArgs\n","from config.read_configurations import config_hbv_hydrodl as hbvhyArgs_d\n","\n","\n","import torch\n","import os\n","import platform\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import scipy.stats\n","# from post import plot\n","\n","from core.utils.randomseed_config import randomseed_config\n","from core.utils.master import create_output_dirs\n","from MODELS.loss_functions.get_loss_function import get_lossFun\n","from MODELS.test_dp_HBV_dynamic import test_dp_hbv\n","from core.data_processing.data_loading import loadData\n","from core.data_processing.normalization import transNorm\n","from core.utils.randomseed_config import randomseed_config\n","from core.data_processing.model import (\n","    take_sample_test,\n","    converting_flow_from_ft3_per_sec_to_mm_per_day\n",")\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","\n","##-----## Multi-model Parameters ##-----##\n","##--------------------------------------##\n","# Setting dictionaries to separately manage each diff model's attributes.\n","models = {'hbvhy_dynamic': None, 'SACSMA_w_snow':None, 'marrmot_PRMS':None}  # 'hbv':None, 'hbvhy': None, 'SACSMA_w_snow':None, 'SACSMA':None,\n","args_list = {'hbvhy_dynamic': hbvhyArgs_d, 'SACSMA_w_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'hbv' : hbvArgs, 'SACSMA_w_snow':None, 'SACSMA': sacsmaArgs,\n","ENSEMBLE_TYPE = 'max'  # 'median', 'avg', 'max', 'softmax'\n","\n","# Set path to `hydro_multimodel_results` directory.\n","if platform.system() == 'Darwin':\n","    # For mac os\n","    OUT_DIR = '/Users/leoglonz/Desktop/water/data/model_runs/hydro_multimodel_results'\n","    # Some operations are not yet working with MPS, so we might need to set some environment variables to use CPU fall instead\n","    # %env PYTORCH_ENABLE_MPS_FALLBACK=1\n","\n","    # Load test observations and predictions from a prior run.\n","    pred_path = os.path.join(OUT_DIR, \"multimodels/671_sites_dp/output/preds_671_HBV_SACSMASnow_PRMS_dynamic.npy\")\n","    obs_path = os.path.join(OUT_DIR, \"multimodels/671_sites_dp/output/obs_671_HBV_SACSMASnow_PRMS_dynamic.npy\")\n","    preds = np.load(pred_path, allow_pickle=True).item()\n","    obs = np.load(obs_path, allow_pickle=True).item()\n","\n","    model_output = preds\n","    y_obs = obs\n","\n","elif platform.system() == 'Windows':\n","    # For windows\n","    OUT_DIR = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n","\n","    # Load test predictions from a prior run.\n","    path = os.path.join(OUT_DIR, \"multimodels\\\\671_sites_dp\\\\output\\\\preds_671_HBV_SACSMASnow_PRMS.npy\")\n","    preds = np.load(path, allow_pickle=True).item()\n","\n","elif platform.system() == 'Linux':\n","    # For Colab\n","    OUT_DIR = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results'\n","\n","    # Load test predictions from a prior run on colab.\n","    # path = os.path.join(OUT_DIR, \"multimodels\\\\671_sites_dp\\\\output\\\\preds_671_HBV_SACSMASnow_PRMS_dynamic.npy\")\n","    # preds = np.load(path, allow_pickle=True).item()\n","\n","else:\n","    raise ValueError('Unsupported operating system.')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1707705238560,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"Nddaecm9DO_x"},"outputs":[],"source":["def test_differentiable_model(args, diff_model):\n","    \"\"\"\n","    This function collects and outputs the model predictions and the corresponding\n","    observations needed to run statistical analyses.\n","\n","    If rerunning testing in a Jupyter environment, you will need to re-import args\n","    as `batch_size` is overwritten in this function and will throw an error if the\n","    overwrite is attempted a second time.\n","    \"\"\"\n","    warm_up = args[\"warm_up\"]\n","    nmul = args[\"nmul\"]\n","    diff_model.eval()\n","    # read data for test time range\n","    dataset_dictionary = loadData(args, trange=args[\"t_test\"])\n","    np.save(os.path.join(args[\"out_dir\"], \"x.npy\"), dataset_dictionary[\"x_NN\"])  # saves with the overlap in the beginning\n","    # normalizing\n","    x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n","    c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n","    c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n","    dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n","    del x_NN_scaled, dataset_dictionary[\"x_NN\"]\n","    # converting the numpy arrays to torch tensors:\n","    for key in dataset_dictionary.keys():\n","        dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n","\n","    # args_mod = args.copy()\n","    args[\"batch_size\"] = args[\"no_basins\"]\n","    nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n","\n","    # Making lists of the start and end indices of the basins for each batch.\n","    batch_size = args[\"batch_size\"]\n","    iS = np.arange(0, ngrid, batch_size)    # Start index list.\n","    iE = np.append(iS[1:], ngrid)   # End.\n","\n","    list_out_diff_model = []\n","    for i in tqdm(range(0, len(iS)), unit='Batch'):\n","        dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n","\n","        out_diff_model = diff_model(dataset_dictionary_sample)\n","        # Convert all tensors in the dictionary to CPU\n","        out_diff_model_cpu = {key: tensor.cpu().detach() for key, tensor in out_diff_model.items()}\n","        # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n","        list_out_diff_model.append(out_diff_model_cpu)\n","\n","    # getting rid of warm-up period in observation dataset and making the dimension similar to\n","    # converting numpy to tensor\n","    # y_obs = torch.tensor(np.swapaxes(y_obs[:, warm_up:, :], 0, 1), dtype=torch.float32)\n","    # c_hydro_model = torch.tensor(c_hydro_model, dtype=torch.float32)\n","    y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args,\n","                                                           dataset_dictionary[\"c_NN\"],\n","                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])\n","\n","    return list_out_diff_model, y_obs"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1389188,"status":"ok","timestamp":1707706630684,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"b2KY2pBADO_y","outputId":"d8bfeab3-ea0d-441e-9bcd-1a897e946435"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting predictions, observations for HBV (HydroDL).\n","daymet tmean was used!\n","Time to read usgs streamflow:  12.833029747009277\n","Time to read usgs streamflow:  11.846182584762573\n","daymet tmean was used!\n","Time to read usgs streamflow:  11.661510944366455\n","Time to read usgs streamflow:  11.792118072509766\n","daymet tmean was used!\n","Time to read usgs streamflow:  11.829334497451782\n","Time to read usgs streamflow:  12.348673582077026\n","read usgs streamflow 15.64434266090393\n","read master file /content/drive/MyDrive/Colab/data/model_runs/rnnStreamflow/CAMELSDemo/dPLHBV/ALL/TDTestforc/TD1_13/daymet/BuffOpt0/RMSE_para0.25/111111/Fold1/T_19801001_19951001_BS_100_HS_256_RHO_365_NF_13_Buff_365_Mul_16/master.json\n","Using device  cpu\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 23/23 [06:45<00:00, 17.63s/Batch]\n"]},{"name":"stdout","output_type":"stream","text":["Collecting predictions, observations for SACSMA_w_snow in batches of 25.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 27/27 [08:24<00:00, 18.69s/Batch]\n"]},{"name":"stdout","output_type":"stream","text":["Collecting predictions, observations for marrmot_PRMS in batches of 25.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 27/27 [04:59<00:00, 11.09s/Batch]\n"]}],"source":["######## NOTE: As of now, testing for this cudnn_rnn model cannot be run with mps or cpu on MAC m-series architecture ##########\n","\n","\n","# loss_funcs = dict()\n","# model_output = dict()\n","# y_obs = dict()\n","\n","# for mod in models:\n","#     mod = str(mod)\n","\n","#     if mod in ['SACSMA', 'SACSMA_w_snow', 'marrmot_PRMS', 'hbv']:\n","#         randomseed_config(seed=args_list[mod][\"randomseed\"][0])\n","#         # Creating output directories and adding them to args.\n","#         args_list[mod] = create_output_dirs(args_list[mod])\n","#         args = args_list[mod]\n","\n","#         loss_funcs[mod] = get_lossFun(args_list[mod])\n","\n","#         modelFile = os.path.join(args[\"out_dir\"], \"model_Ep\" + str(args['EPOCHS']) + \".pt\")\n","#         models[mod] = torch.load(modelFile)     # Append instanced models.\n","\n","#         print(\"Collecting predictions, observations for %s in batches of %i.\" %(mod, args['no_basins']))\n","#         model_output[mod], y_obs[mod] = test_differentiable_model(args=args,\n","#                                                                   diff_model=models[mod])\n","#     elif mod in ['hbvhy', 'hbvhy_dynamic']:\n","#         print(\"Collecting predictions, observations for HBV (HydroDL).\")\n","#         model_output[mod], y_obs[mod] = test_dp_hbv()\n","#     else:\n","#         raise ValueError(f\"Unsupported model type in `models`.\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3435,"status":"ok","timestamp":1707706633954,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"Nu3h24-gDO_z"},"outputs":[],"source":["# Figure out how to save the model output to a csv or npy so that we don't waste\n","# time having to recollect data every time we start up this notebook.\n","\n","path = os.path.join(OUT_DIR, \"multimodels\\\\671_sites_dp\\\\output\\\\\")\n","if not os.path.exists(path):\n","    os.makedirs(path, exist_ok=True)\n","\n","np.save(os.path.join(path, \"preds_671_HBVdynamic_SACSMASnow_PRMS.npy\"),model_output)\n","np.save(os.path.join(path, \"obs_671_HBVdynamic_SACSMASnow_PRMS.npy\"),y_obs)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":216,"status":"ok","timestamp":1707706633956,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"typgVyc9DO_z"},"outputs":[],"source":["def calFDC(data):\n","    # data = Ngrid * Nday\n","    Ngrid, Nday = data.shape\n","    FDC100 = np.full([Ngrid, 100], np.nan)\n","    for ii in range(Ngrid):\n","        tempdata0 = data[ii, :]\n","        tempdata = tempdata0[~np.isnan(tempdata0)]\n","        # deal with no data case for some gages\n","        if len(tempdata)==0:\n","            tempdata = np.full(Nday, 0)\n","        # sort from large to small\n","        temp_sort = np.sort(tempdata)[::-1]\n","        # select 100 quantile points\n","        Nlen = len(tempdata)\n","        ind = (np.arange(100)/100*Nlen).astype(int)\n","        FDCflow = temp_sort[ind]\n","        if len(FDCflow) != 100:\n","            raise Exception('unknown assimilation variable')\n","        else:\n","            FDC100[ii, :] = FDCflow\n","\n","    return FDC100\n","\n","\n","def statError(pred, target):\n","    ngrid, nt = pred.shape\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n","    # Bias\n","        Bias = np.nanmean(pred - target, axis=1)\n","        # RMSE\n","        RMSE = np.sqrt(np.nanmean((pred - target)**2, axis=1))\n","        # ubRMSE\n","        predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n","        targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n","        predAnom = pred - predMean\n","        targetAnom = target - targetMean\n","        ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom)**2, axis=1))\n","        # FDC metric\n","        predFDC = calFDC(pred)\n","        targetFDC = calFDC(target)\n","        FDCRMSE = np.sqrt(np.nanmean((predFDC - targetFDC) ** 2, axis=1))\n","    # rho R2 NSE\n","        Corr = np.full(ngrid, np.nan)\n","        CorrSp = np.full(ngrid, np.nan)\n","        R2 = np.full(ngrid, np.nan)\n","        NSE = np.full(ngrid, np.nan)\n","        PBiaslow = np.full(ngrid, np.nan)\n","        PBiashigh = np.full(ngrid, np.nan)\n","        PBias = np.full(ngrid, np.nan)\n","        PBiasother = np.full(ngrid, np.nan)\n","        KGE = np.full(ngrid, np.nan)\n","        KGE12 = np.full(ngrid, np.nan)\n","        RMSElow = np.full(ngrid, np.nan)\n","        RMSEhigh = np.full(ngrid, np.nan)\n","        RMSEother = np.full(ngrid, np.nan)\n","        for k in range(0, ngrid):\n","            x = pred[k, :]\n","            y = target[k, :]\n","            ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n","            if ind.shape[0] > 0:\n","                xx = x[ind]\n","                yy = y[ind]\n","                # percent bias\n","                PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n","\n","                # FHV the peak flows bias 2%\n","                # FLV the low flows bias bottom 30%, log space\n","                pred_sort = np.sort(xx)\n","                target_sort = np.sort(yy)\n","                indexlow = round(0.3 * len(pred_sort))\n","                indexhigh = round(0.98 * len(pred_sort))\n","                lowpred = pred_sort[:indexlow]\n","                highpred = pred_sort[indexhigh:]\n","                otherpred = pred_sort[indexlow:indexhigh]\n","                lowtarget = target_sort[:indexlow]\n","                hightarget = target_sort[indexhigh:]\n","                othertarget = target_sort[indexlow:indexhigh]\n","                PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n","                PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n","                PBiasother[k] = np.sum(otherpred - othertarget) / np.sum(othertarget) * 100\n","                RMSElow[k] = np.sqrt(np.nanmean((lowpred - lowtarget)**2))\n","                RMSEhigh[k] = np.sqrt(np.nanmean((highpred - hightarget)**2))\n","                RMSEother[k] = np.sqrt(np.nanmean((otherpred - othertarget)**2))\n","\n","                if ind.shape[0] > 1:\n","                    # Theoretically at least two points for correlation\n","                    Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n","                    CorrSp[k] = scipy.stats.spearmanr(xx, yy)[0]\n","                    yymean = yy.mean()\n","                    yystd = np.std(yy)\n","                    xxmean = xx.mean()\n","                    xxstd = np.std(xx)\n","                    KGE[k] = 1 - np.sqrt((Corr[k]-1)**2 + (xxstd/yystd-1)**2 + (xxmean/yymean-1)**2)\n","                    KGE12[k] = 1 - np.sqrt((Corr[k] - 1) ** 2 + ((xxstd*yymean)/ (yystd*xxmean) - 1) ** 2 + (xxmean / yymean - 1) ** 2)\n","                    SST = np.sum((yy-yymean)**2)\n","                    SSReg = np.sum((xx-yymean)**2)\n","                    SSRes = np.sum((yy-xx)**2)\n","                    R2[k] = 1-SSRes/SST\n","                    NSE[k] = 1-SSRes/SST\n","\n","    outDict = dict(Bias=Bias, RMSE=RMSE, ubRMSE=ubRMSE, Corr=Corr, CorrSp=CorrSp, R2=R2, NSE=NSE,\n","                   FLV=PBiaslow, FHV=PBiashigh, PBias=PBias, PBiasother=PBiasother, KGE=KGE, KGE12=KGE12, fdcRMSE=FDCRMSE,\n","                   lowRMSE=RMSElow, highRMSE=RMSEhigh, midRMSE=RMSEother)\n","\n","    return outDict"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":126,"status":"ok","timestamp":1707706633957,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"RtdVYl00DO_z"},"outputs":[],"source":["class hydroEnsemble(torch.nn.Module):\n","    # Wrapper for multiple hydrologic models.\n","    # In future, consider just passing the models you want to ensemble explicitly.\n","    def __init__(self, num_models, hidden_size, num_layers):\n","        super(hydroEnsemble, self).__init__()\n","\n","        self.lstm = torch.nn.LSTM(num_models, hidden_size, num_layers, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_size, num_models)  # Two models (modelA and modelB)\n","\n","        # self.modelA = modelA\n","        # self.modelB = modelB\n","        # self.classifier = torch.nn.Linear(4, 2)\n","\n","    def forward(self, x):\n","        # x is the input sequence tensor with shape (batch_size, sequence_length, num_models)\n","\n","        # Setting randomseed for deterministic output.\n","        randomseed_config(0)\n","\n","        # Add batch dimension to input and convert to tensor.\n","        x_exp = x.unsqueeze(0)\n","\n","        # LSTM layer\n","        lstm_out, _ = self.lstm(x_exp)\n","\n","        # Fully connected layer\n","        fc_out = self.fc(lstm_out)\n","\n","        # Apply softmax activation to obtain weights\n","        weights = torch.nn.functional.softmax(fc_out, dim=2).squeeze()\n","\n","        # Weighted combination of predictions.\n","        weighted_preds = np.multiply(weights.detach(), x)\n","\n","        # Or take the max weight and return the corresponding value.\n","        max_vals, _ = torch.max(weights, dim=1)\n","        btensor = torch.zeros_like(weights)\n","        btensor[weights==max_vals.view(-1,1)] = 1\n","        weighted_preds = np.multiply(btensor.detach(), x)\n","\n","        preds = torch.sum(weighted_preds, dim=1)\n","\n","        # All tensors\n","        # return preds, weights, weighted_preds\n","        return preds\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1707706634441,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"uJhJHca9DO_0"},"outputs":[],"source":["def calculate_metrics_multi(args_list, model_outputs, y_obs_list, ensemble_type='max', out_dir=None):\n","    \"\"\"\n","    Calculate stats for a multimodel ensemble.\n","    \"\"\"\n","    stats_list = dict()\n","\n","    for mod in args_list:\n","        args = args_list[mod]\n","        mod_out = model_outputs[mod]\n","        y_obs = y_obs_list[mod]\n","\n","        if mod in ['SACSMA', 'SACSMA_w_snow', 'marrmot_PRMS', 'hbv']:\n","            # Note for hydrodl HBV, calculations have already been done,\n","            # so skip this step.\n","\n","            # Saving data\n","            if out_dir:\n","                path = os.path.join(out_dir,\"models\\\\671_sites_dp\\\\\" + mod + \"\\\\\")\n","                if not os.path.exists(path):\n","                    os.makedirs(path, exist_ok=True)\n","\n","                # Test data (obs and model results).\n","                for key in mod_out[0].keys():\n","                    if len(mod_out[0][key].shape) == 3:\n","                        dim = 1\n","                    else:\n","                        dim = 0\n","                    concatenated_tensor = torch.cat([d[key] for d in mod_out], dim=dim)\n","                    file_name = key + \".npy\"\n","                    np.save(os.path.join(path, file_name), concatenated_tensor.numpy())\n","                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), concatenated_tensor.numpy())\n","\n","                # Reading and flow observations.\n","                print(args['target'])\n","                for var in args[\"target\"]:\n","                    item_obs = y_obs[:, :, args[\"target\"].index(var)]\n","                    file_name = var + \".npy\"\n","                    np.save(os.path.join(path, file_name), item_obs)\n","                    # np.save(os.path.join(args[\"out_dir\"], args[\"testing_dir\"], file_name), item_obs)\n","\n","\n","            ###################### calculations here ######################\n","            predLst = list()\n","            obsLst = list()\n","            flow_sim = torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1)\n","            flow_obs = y_obs[:, :, args[\"target\"].index(\"00060_Mean\")]\n","            predLst.append(flow_sim.numpy())\n","            obsLst.append(np.expand_dims(flow_obs, 2))\n","\n","            # if args[\"temp_model_name\"] != \"None\":\n","            #     temp_sim = torch.cat([d[\"temp_sim\"] for d in mod_out], dim=1)\n","            #     temp_obs = y_obs[:, :, args[\"target\"].index(\"00010_Mean\")]\n","            #     predLst.append(temp_sim.numpy())\n","            #     obsLst.append(np.expand_dims(temp_obs, 2))\n","\n","            # we need to swap axes here to have [basin, days], and remove redundant\n","            # dimensions with np.squeeze().\n","            stats_list[mod] = [\n","                statError(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))\n","                for (x, y) in zip(predLst, obsLst)\n","            ]\n","        elif mod in ['hbvhy', 'hbvhy_dynamic']:\n","            stats_list[mod] = [statError(mod_out[:,:,0], y_obs.squeeze())]\n","        else:\n","            raise ValueError(f\"Unsupported model type in `models`.\")\n","\n","    # Calculating final statistics for the whole set of basins.\n","    name_list = [\"flow\", \"temp\"]\n","    for st, name in zip(stats_list[mod], name_list):\n","        count = 0\n","        mdstd = np.zeros([len(st), 3])\n","        for key in st.keys():\n","            # st contains the statistics on a model run like NSE and KGE.\n","            # Find the best result (e.g., the max, avg, median) and merge from each model.\n","            for i, mod in enumerate(args_list):\n","                if i == 0:\n","                    # temp contains the values of key per basin.\n","                    temp = stats_list[mod][0][key]\n","                    continue\n","                elif i == 1:\n","                    temp = np.stack((temp, stats_list[mod][0][key]), axis=1)\n","                else:\n","                    temp = np.hstack((temp, stats_list[mod][0][key].reshape(-1,1)))\n","\n","            if len(args_list) > 1:\n","                if ensemble_type == 'max':\n","                    # print(temp, key)\n","                    temp = np.amax(temp, axis=1)\n","                    # print(temp, key)\n","                elif ensemble_type == 'avg':\n","                    temp = np.mean(temp, axis=1)\n","                elif ensemble_type == 'median':\n","                    temp = np.median(temp, axis=1)\n","                elif ensemble_type == 'softmax':\n","                    # # Softmax gets relative contributions of each model.\n","                    # weights = torch.nn.functional.softmax(torch.from_numpy(temp), dim=1)\n","                    # temp = np.sum(temp * weights.numpy(), axis=1)\n","\n","                    # Instantiate weighting lstm with softmax.\n","                    lstm = hydroEnsemble(num_models=len(args_list), hidden_size=192, num_layers=3)\n","                    # Forward pass through the model\n","                    temp = lstm(torch.tensor(temp, dtype=torch.float))\n","                else:\n","                    raise ValueError(\"Invalid model ensemble type specified.\")\n","\n","            median = np.nanmedian(temp)  # abs(i)\n","            std = np.nanstd(temp)  # abs(i)\n","            mean = np.nanmean(temp)  # abs(i)\n","            k = np.array([[median, std, mean]])\n","            mdstd[count] = k\n","            count = count + 1\n","\n","        # mdstd displays the statistics for each error measure in stats_list.\n","        mdstd = pd.DataFrame(\n","            mdstd, index=st.keys(), columns=[\"median\", \"STD\", \"mean\"]\n","        )\n","        # Save the data stats from the training run:\n","        if out_dir and len(args_list) > 1:\n","            path = os.path.join(out_dir, \"multimodels\\\\671_sites_dp\\\\n_\" + ensemble_type + \"\\\\\")\n","            if not os.path.exists(path):\n","                os.makedirs(path, exist_ok=True)\n","\n","            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name + ensemble_type +\".csv\")))\n","        elif out_dir:\n","            path = os.path.join(out_dir, \"models\\\\671_sites_dp\\\\\" + args_list[0] + \"\\\\\")\n","            if not os.path.exists(path):\n","                os.makedirs(path, exist_ok=True)\n","\n","            mdstd.to_csv((os.path.join(path, \"mdstd_\" + name +\".csv\")))\n","        else: continue\n","\n","    # Show boxplots of the results\n","    # plt.rcParams[\"font.size\"] = 14\n","    # keyLst = [\"Bias\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n","    # dataBox = list()\n","    # for iS in range(len(keyLst)):\n","    #     statStr = keyLst[iS]\n","    #     temp = list()\n","    #     # for k in range(len(st)):\n","    #     data = st[statStr]\n","    #     data = data[~np.isnan(data)]\n","    #     temp.append(data)\n","    #     dataBox.append(temp)\n","    # labelname = [\n","    #     \"Hybrid differentiable model\"\n","    # ]  # ['STA:316,batch158', 'STA:156,batch156', 'STA:1032,batch516']   # ['LSTM-34 Basin']\n","\n","    # xlabel = [\"Bias ($\\mathregular{deg}$C)\", \"RMSE\", \"ubRMSE\", \"NSE\", \"Corr\"]\n","    # fig = plot.plotBoxFig(\n","    #     dataBox, xlabel, label2=labelname, sharey=False, figsize=(16, 8)\n","    # )\n","    # fig.patch.set_facecolor(\"white\")\n","    # boxPlotName = \"PGML\"\n","    # fig.suptitle(boxPlotName, fontsize=12)\n","    # plt.rcParams[\"font.size\"] = 12\n","    # # plt.savefig(\n","    # #     os.path.join(args[\"out_dir\"], args[\"testing_dir\"], \"Box_\" + name + \".png\")\n","    # # )  # , dpi=500\n","    # # fig.show()\n","    # plt.close()\n","\n","    torch.cuda.empty_cache()\n","    print(\"Testing ended\")\n","\n","    return stats_list, mdstd"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35429,"status":"ok","timestamp":1707706669822,"user":{"displayName":"Leo Lonzarich","userId":"03094207546900081501"},"user_tz":360},"id":"gyETa7dZDO_0","outputId":"41921b5c-c2b8-46cc-eef1-3f0d4dd3b21d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing ended\n"]},{"data":{"text/plain":["Bias           0.023276\n","RMSE           1.154326\n","ubRMSE         1.136360\n","Corr           0.858483\n","CorrSp         0.847346\n","R2             0.707425\n","NSE            0.707425\n","FLV           45.150305\n","FHV           -6.803808\n","PBias          2.927847\n","PBiasother     5.308928\n","KGE            0.743327\n","KGE12          0.737234\n","fdcRMSE        1.024379\n","lowRMSE        0.075245\n","highRMSE       2.522294\n","midRMSE        0.237547\n","Name: median, dtype: float64"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# models = {'SACSMA':None, 'marrmot_PRMS':None}  # 'hbv':None\n","args_list = {'hbvhy_dynamic': hbvhyArgs_d, 'SACSMA_w_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'hbv' : hbvArgs, 'SACSMA_w_snow':None, 'SACSMA': sacsmaArgs,\n","ENSEMBLE_TYPE = 'avg'  # 'median', 'avg', 'max', 'softmax'\n","stats_list, mtstd = calculate_metrics_multi(args_list, model_outputs=model_output, y_obs_list=y_obs, ensemble_type=ENSEMBLE_TYPE)\n","\n","mtstd['median']"]},{"cell_type":"markdown","metadata":{"id":"nI6c4K21DO_4"},"source":["----\n","\n","### Getting HBV Model Data\n","\n","----\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jYdZ1sXDO_8"},"outputs":[],"source":["import sys\n","sys.path.append('../../')\n","from hydroDL import master, utils\n","from hydroDL.data import camels\n","from hydroDL.master import loadModel\n","from hydroDL.model import train\n","from hydroDL.post import plot, stat\n","\n","import os\n","import numpy as np\n","import torch\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","import json\n","import datetime as dt\n","\n","\n","## fix the random seeds\n","randomseed = 111111\n","random.seed(randomseed)\n","torch.manual_seed(randomseed)\n","np.random.seed(randomseed)\n","torch.cuda.manual_seed(randomseed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","## GPU setting\n","testgpuid = 0\n","torch.cuda.set_device(testgpuid)\n","\n","## setting options, keep the same as your training\n","PUOpt = 0  # 0 for All; 1 for PUB; 2 for PUR;\n","buffOptOri = 0  # original buffOpt, must be same as what you set for training\n","buffOpt = 0  # control load training data 0: do nothing; 1: repeat first year; 2: load one more year\n","forType = 'daymet'\n","\n","## Hyperparameters, keep the same as your training setup\n","BATCH_SIZE = 100\n","RHO = 365\n","HIDDENSIZE = 256\n","Ttrain = [19801001, 19951001]  # Training period\n","# Ttrain = [19891001, 19991001]  # PUB/PUR period\n","Tinv = [19801001, 19951001] # dPL Inversion period\n","# Tinv = [19891001, 19991001]  # PUB/PUR period\n","Nfea = 12 # number of HBV parameters\n","BUFFTIME = 365\n","routing = True\n","Nmul = 16\n","comprout = False\n","compwts = False\n","pcorr = None\n","\n","Ttest = [19951001, 20101001]  # testing period\n","TtestLst = utils.time.tRange2Array(Ttest)\n","TtestLoad = [19951001, 20101001]\n","\n","testbatch = 50  # forward number of \"testbatch\" basins each time to save GPU memory. You can set this even smaller to save more.\n","testepoch = 50\n","\n","testseed = 111111"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrJP5uddDO_8"},"outputs":[],"source":["# Define root directory of database and saved output dir\n","# Modify this based on your own location of CAMELS dataset and saved models\n","rootDatabase = os.path.join(os.path.sep, 'D:\\data', 'Camels')  # CAMELS dataset root directory\n","camels.initcamels(rootDatabase)  # initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict\n","\n","rootOut = os.path.join(os.path.sep, 'D:\\data\\model_runs', 'rnnStreamflow')  # Model output root directory\n","\n","# CAMLES basin info\n","gageinfo = camels.gageDict\n","hucinfo = gageinfo['huc']\n","gageid = gageinfo['id']\n","gageidLst = gageid.tolist()\n","\n","# same as training, load data based on ALL, PUB, PUR scenarios\n","if PUOpt == 0: # for All the basins\n","    puN = 'ALL'\n","    tarIDLst = [gageidLst]\n","\n","elif PUOpt == 1: # for PUB\n","    puN = 'PUB'\n","    # load the subset ID\n","    # splitPath saves the basin ID of random groups\n","    splitPath = 'PUBsplitLst.txt'\n","    with open(splitPath, 'r') as fp:\n","        testIDLst=json.load(fp)\n","    tarIDLst = testIDLst\n","\n","elif PUOpt == 2: # for PUR\n","    puN = 'PUR'\n","    # Divide CAMELS dataset into 7 PUR regions\n","    # get the id list of each region\n","    regionID = list()\n","    regionNum = list()\n","    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ] # seven regions\n","    for ii in range(len(regionDivide)):\n","        tempcomb = regionDivide[ii]\n","        tempregid = list()\n","        for ih in tempcomb:\n","            tempid = gageid[hucinfo==ih].tolist()\n","            tempregid = tempregid + tempid\n","        regionID.append(tempregid)\n","        regionNum.append(len(tempregid))\n","    tarIDLst = regionID     # List of all basin ID's in the study (671 for full camels).\n","\n","# define the matrix to save results\n","predtestALL = np.full([len(gageid), len(TtestLst), 5], np.nan)\n","obstestALL = np.full([len(gageid), len(TtestLst), 1], np.nan)\n","\n","# this testsave_path should be consistent with where you save your model\n","testsave_path = 'CAMELSDemo/dPLHBV/' + puN + '/Testforc/' + forType + '/BuffOpt' + str(buffOptOri) +\\\n","                '/RMSE_para0.25/'+str(testseed)\n","\n","## load data and test the model\n","nstart = 0\n","logtestIDLst = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnVIU9jiDO_9"},"outputs":[],"source":["for ifold in range(1, len(tarIDLst)+1):\n","    testfold = ifold\n","    TestLS = tarIDLst[testfold - 1]\n","    TestInd = [gageidLst.index(j) for j in TestLS]\n","\n","    TrainLS = gageidLst\n","    TrainInd = [gageidLst.index(j) for j in TrainLS]\n","\n","    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n","\n","    nbasin = len(TestLS) # number of basins for testing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f7HddOzDO_9"},"outputs":[],"source":["foldstr = 'Fold' + str(testfold)\n","exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n","            +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n","# the final path to test with the trained model saved in\n","testout = os.path.join(rootOut, testsave_path, foldstr, exp_info)\n","testmodel = loadModel(testout, epoch=testepoch)\n","testmodel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEZ2YSsSDO_9"},"outputs":[],"source":["TtrainLoad = Ttrain\n","TinvLoad = Tinv\n","\n","varF = ['prcp', 'tmean']\n","varFInv = ['prcp', 'tmean']\n","\n","\n","attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n","                   'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n","                   'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n","                   'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n","                   'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n","                   'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n","\n","dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n","forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DbXIn-3DO_9"},"outputs":[],"source":["dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n","forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False)\n","attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYZ9yompDO_9"},"outputs":[],"source":["dfTest = camels.DataframeCamels(tRange=TtestLoad, subset=TestLS, forType=forType)\n","forcTestUN = dfTest.getDataTs(varLst=varF, doNorm=False, rmNan=False)\n","obsTestUN = dfTest.getDataObs(doNorm=False, rmNan=False, basinnorm=False)\n","attrsTestUN = dfTest.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TgHN7MeDO_-"},"outputs":[],"source":["len(obsTestUN), len(attrnewLst), len(obsTestUN[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hL2RRSp6DO_-"},"outputs":[],"source":["areas = gageinfo['area'][TestInd] # unit km2\n","temparea = np.tile(areas[:, None, None], (1, obsTestUN.shape[1],1))\n","obsTestUN = (obsTestUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3\n","\n","varLstNL = ['PEVAP']\n","usgsIdLst = gageid\n","if forType == 'maurer':\n","    tPETRange = [19800101, 20090101]\n","else:\n","    tPETRange = [19800101, 20150101]\n","tPETLst = utils.time.tRange2Array(tPETRange)\n","PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n","ntime = len(tPETLst)\n","PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n","for k in range(len(usgsIdLst)):\n","    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n","    PETfull[k, :, :] = dataTemp\n","\n","TtrainLst = utils.time.tRange2Array(TtrainLoad)\n","TinvLst = utils.time.tRange2Array(TinvLoad)\n","TtestLoadLst = utils.time.tRange2Array(TtestLoad)\n","C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n","PETUN = PETfull[:, ind2, :]\n","PETUN = PETUN[TrainInd, :, :] # select basins\n","C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n","PETInvUN = PETfull[:, ind2inv, :]\n","PETInvUN = PETInvUN[TrainInd, :, :]\n","C, ind1, ind2test = np.intersect1d(TtestLoadLst, tPETLst, return_indices=True)\n","PETTestUN = PETfull[:, ind2test, :]\n","PETTestUN = PETTestUN[TestInd, :, :]\n","\n","# process data, do normalization and remove nan\n","series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n","seriesvarLst = varFInv + ['pet']\n","# load the saved statistics\n","statFile = os.path.join(testout, 'statDict.json')\n","with open(statFile, 'r') as fp:\n","    statDict = json.load(fp)\n","\n","# normalize\n","attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict, toNorm=True)\n","attr_norm[np.isnan(attr_norm)] = 0.0\n","series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True)\n","series_norm[np.isnan(series_norm)] = 0.0\n","\n","attrtest_norm = camels.transNormbyDic(attrsTestUN, attrnewLst, statDict, toNorm=True)\n","attrtest_norm[np.isnan(attrtest_norm)] = 0.0\n","seriestest_inv = np.concatenate([forcTestUN, PETTestUN], axis=2)\n","seriestest_norm = camels.transNormbyDic(seriestest_inv, seriesvarLst, statDict, toNorm=True)\n","seriestest_norm[np.isnan(seriestest_norm)] = 0.0\n","\n","# prepare the inputs\n","zTrain = series_norm\n","xTrain = np.concatenate([forcUN, PETUN], axis=2) # HBV forcing\n","xTrain[np.isnan(xTrain)] = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXjzgSVZDO__"},"outputs":[],"source":["if buffOpt == 1: # repeat the first year for buff\n","    zTrainIn = np.concatenate([zTrain[:,0:BUFFTIME,:], zTrain], axis=1)\n","    xTrainIn = np.concatenate([xTrain[:,0:BUFFTIME,:], xTrain], axis=1) # Bufftime for the first year\n","    # yTrainIn = np.concatenate([obsUN[:,0:BUFFTIME,:], obsUN], axis=1)\n","else: # no repeat, original data\n","    zTrainIn = zTrain\n","    xTrainIn = xTrain\n","    # yTrainIn = obsUN\n","\n","forcTuple = (xTrainIn, zTrainIn)\n","attrs = attr_norm\n","\n","## Prepare the testing data and forward the trained model for testing\n","# TestBuff = 365 # Use 365 days forcing to warm up the model for testing\n","TestBuff = xTrain.shape[1]  # Use the whole training period to warm up the model for testing\n","# TestBuff = len(TtestLoadLst) - len(TtestLst)  # use the redundantly loaded data to warm up\n","\n","# prepare file name to save the testing predictions\n","filePathLst = master.master.namePred(\n","        testout, Ttest, 'All_Buff'+str(TestBuff), epoch=testepoch, targLst=['Qr', 'Q0', 'Q1', 'Q2', 'ET'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRvmM9S7DO__"},"outputs":[],"source":["# prepare the inputs for TESTING\n","if PUOpt == 0: # for ALL basins, temporal generalization test\n","    zTest = series_norm  # dPL inversion\n","    xTest = np.concatenate([forcTestUN, PETTestUN], axis=2)  # HBV forcing\n","    # forcings to warm up the model. Here use the forcing of training period to warm up\n","    xTestBuff = xTrain[:, -TestBuff:, :]\n","    xTest = np.concatenate([xTestBuff, xTest], axis=1)\n","    obs = obsTestUN[:, 0:, :]  # starts with 0 when not loading more data before testing period\n","\n","else:  # for PUB and PUR cases, different testing basins. Load more forcings to warm up.\n","    zTest = seriestest_norm[:, 0:TestBuff, :]  # Use the warm-up period forcing as the gA input in zTest\n","    # zTest = seriestest_norm\n","    xTest = np.concatenate([forcTestUN, PETTestUN], axis=2)  # HBV forcing\n","    obs = obsTestUN[:, TestBuff:, :]  # exclude loaded obs in warming up period (first TestBuff days) for evaluation\n","\n","# Use days of TestBuff to initialize the model\n","testmodel.inittime=TestBuff\n","\n","# Final inputs to the test model\n","xTest[np.isnan(xTest)] = 0.0\n","attrtest = attrtest_norm\n","cTemp = np.repeat(\n","    np.reshape(attrtest, [attrtest.shape[0], 1, attrtest.shape[-1]]), zTest.shape[1], axis=1)\n","zTest = np.concatenate([zTest, cTemp], 2) # Add attributes to historical forcings as the inversion part\n","testTuple = (xTest, zTest) # xTest: input forcings to HBV; zTest: inputs to gA LSTM to learn parameters\n","\n","# forward the model and save results\n","train.testModel(\n","    testmodel, testTuple, c=None, batchSize=testbatch, filePathLst=filePathLst)\n","\n","# read out the saved forward predictions\n","dataPred = np.ndarray([obs.shape[0], obs.shape[1], len(filePathLst)])\n","for k in range(len(filePathLst)):\n","    filePath = filePathLst[k]\n","    dataPred[:, :, k] = pd.read_csv(\n","        filePath, dtype=np.float, header=None).values\n","# save the predictions to the big matrix\n","predtestALL[nstart:nstart+nbasin, :, :] = dataPred\n","obstestALL[nstart:nstart+nbasin, :, :] = obs\n","nstart = nstart + nbasin\n","logtestIDLst = logtestIDLst + TestLS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9daO3WgtDO__"},"outputs":[],"source":["predtestALL[0], len(predtestALL[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"er93LQwlDO__"},"outputs":[],"source":["## post processing\n","# calculate evaluation metrics\n","evaDict = [stat.statError(predtestALL[:,:,0], obstestALL.squeeze())]  # Q0: the streamflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qk7fAqUQDPAA"},"outputs":[],"source":["len(evaDict[0]['NSE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHuert8ADPAA"},"outputs":[],"source":["from test_dp_HBV import test_dp_hbv\n","\n","predtestALL, predtestALL = test_dp_hbv()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6Ygir-aDPAA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
