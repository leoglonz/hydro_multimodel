{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 4.619056224822998\n",
      "read usgs streamflow 4.002650022506714\n",
      "read usgs streamflow 5.169572114944458\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 4.245026111602783\n",
      "read usgs streamflow 4.149972915649414\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from hydroDL import master, utils\n",
    "from hydroDL.data import camels\n",
    "from hydroDL.master import default\n",
    "from hydroDL.master.master import loadModel\n",
    "from hydroDL.model import rnn, crit, train\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "## fix the random seeds for reproducibility\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## GPU setting\n",
    "# which GPU to use when having multiple\n",
    "traingpuid = 0\n",
    "# torch.cuda.set_device(traingpuid)\n",
    "\n",
    "## Setting training options here\n",
    "PUOpt = 0\n",
    "# PUOpt values and explanations:\n",
    "# 0: train and test on ALL basins;\n",
    "# 1 for PUB spatial test, randomly hold out basins;\n",
    "# 2 for PUR spatial test, hold out a continuous region;\n",
    "buffOpt = 1\n",
    "# buffOpt defines the warm-up option for the first year of training forcing data\n",
    "# 0: do nothing, the first year forcing would only be used to warm up the next year;\n",
    "# 1: repeat first year forcing to warm up the first year;\n",
    "# 2: load one more year forcing to warm up the first year\n",
    "TDOpt = True\n",
    "# TDOpt, True as using dynamic parameters and False as using static parameters\n",
    "forType = 'daymet'\n",
    "# for Type defines which forcing in CAMELS to use: 'daymet', 'nldas', 'maurer'\n",
    "\n",
    "\n",
    "# used for multimodel ensembling.\n",
    "prcp_loss_factor = 23\n",
    "smooth_loss_factor = 0\n",
    "\n",
    "## Set hyperparameters\n",
    "EPOCH = 50 # total epoches to train the mode\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "saveEPOCH = 10\n",
    "Ttrain = [19801001, 19951001] # Training period\n",
    "# Ttrain = [19891001, 19991001] # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # Inversion period for historical forcings\n",
    "# Tinv = [19891001, 19991001] # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters. 12:original HBV; 13:includes the added dynamic ET para when setting ETMod=True\n",
    "BUFFTIME = 365 # for each training sample, to use BUFFTIME days to warm up the states.\n",
    "routing = True # Whether to use the routing module for simulated runoff\n",
    "Nmul = 16 # Multi-component model. How many parallel HBV components to use. 1 means the original HBV.\n",
    "comprout = False # True is doing routing for each component\n",
    "compwts = False # True is using weighted average for components; False is the simple mean\n",
    "pcorr = None # or a list to give the range of precip correction\n",
    "\n",
    "flow_regime = 0  # 1 is high flow expert.\n",
    "\n",
    "if TDOpt is True:\n",
    "    # Below options are only for running models with dynamic parameters\n",
    "    tdRep = [1, 13] # When using dynamic parameters, this list defines which parameters to set as dynamic\n",
    "    tdRepS = [str(ix) for ix in tdRep]\n",
    "    # ETMod: if True, use the added shape parameter (index 13) for ET. Default as False.\n",
    "    # Must set below ETMod as True and Nfea=13 when including 13 index in above tdRep list for dynamic parameters\n",
    "    # If 13 not in tdRep list, set below ETMod=False and Nfea=12 to use the original HBV without ET shape para\n",
    "    ETMod = True\n",
    "    Nfea = 13 # should be 13 when setting ETMod=True. 12 when ETMod=False\n",
    "    dydrop = 0.0 # dropout possibility for those dynamic parameters: 0.0 always dynamic; 1.0 always static\n",
    "    staind = -1 # which time step to use from the learned para time series for those static parameters\n",
    "    TDN = '/TDTestforc/'+'TD'+\"_\".join(tdRepS) +'/'\n",
    "else:\n",
    "    TDN = '/Testforc/'\n",
    "\n",
    "# Define root directory of database and output\n",
    "# Modify these based on your own location of CAMELS dataset\n",
    "# Following the data download instruction in README file, you should organize the folders like\n",
    "# 'your/path/to/Camels/basin_timeseries_v1p2_metForcing_obsFlow' and 'your/path/to/Camels/camels_attributes_v2.0'\n",
    "# Then 'rootDatabase' here should be 'your/path/to/Camels';\n",
    "# 'rootOut' is the root dir where you save the trained model\n",
    "    \n",
    "\n",
    "\n",
    "# CAMELS dataset root directory\n",
    "sysroot = '/Users/leoglonz/Desktop/water/'\n",
    "rootDatabase = os.path.join(os.path.sep, sysroot, 'data', 'Camels')    # CAMELS dataset root directory\n",
    "# rootDatabase = os.path.join(os.path.sep, 'data', 'kas7897', 'dPLHBVrelease')  # CAMELS dataset root directory\n",
    "camels.initcamels(rootDatabase, forType=forType)  # initialize camels module-scope variables in camels.py (dirDB, gageDict) to read basin info\n",
    "\n",
    "# rootOut = os.path.join(os.path.sep, 'data', 'rnnStreamflow')  # Model output root directory\n",
    "rootOut = os.path.join(os.path.sep, sysroot, 'data', 'model_runs', 'hydro_multimodel_results', 'multimodels', '671_sites_dp', 'mm_ensemble_sigmoid')  # Model output root directory\n",
    "\n",
    "\n",
    "\n",
    "## If you have a checkpoint file for a previous run, set the path to directory here. \n",
    "# saved epoch is the epoch corresponding to the checkpoint. \n",
    "saved_epoch = 6\n",
    "checkpoint_file = None #'/Users/leoglonz/Desktop/water/data/model_runs/rnnStreamflow/CAMELSDemo/dPLHBV/ALL/TDTestforc/TD1_13/daymet/BuffOpt0/RMSE_para0.25/111111/Fold1/T_19801001_19951001_BS_100_HS_256_RHO_365_NF_13_Buff_365_Mul_16/'\n",
    "\n",
    "## set up different data loadings for ALL, PUB, PUR\n",
    "testfoldInd = 1\n",
    "# Which fold to hold out for PUB (10 folds, from 1 to 10) and PUR (7 folds, from 1 to 7).\n",
    "# It doesn't matter when training on ALL basins (setting PUOpt=0), could always set testfoldInd=1 for this case.\n",
    "\n",
    "# load CAMELS basin information\n",
    "gageinfo = camels.gageDict\n",
    "hucinfo = gageinfo['huc']\n",
    "gageid = gageinfo['id']\n",
    "gageidLst = gageid.tolist()\n",
    "\n",
    "if PUOpt == 0: # training on all basins without spatial hold-out\n",
    "    puN = 'ALL'\n",
    "    TrainLS = gageidLst # all basins\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    TestLS = gageidLst\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 1: # random hold out basins. hold out the fold set by testfoldInd\n",
    "    puN = 'PUB'\n",
    "    # load the PUB basin groups\n",
    "    # randomly divide CAMELS basins into 10 groups and this file contains the basin ID for each group\n",
    "    # located in splitPath\n",
    "    splitPath = 'PUBsplitLst.txt'\n",
    "    with open(splitPath, 'r') as fp:\n",
    "        testIDLst=json.load(fp)\n",
    "    # Generate training ID lists excluding the hold out fold\n",
    "    TestLS = testIDLst[testfoldInd - 1]\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS))\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 2:\n",
    "    puN = 'PUR'\n",
    "    # Divide CAMELS dataset into 7 continous PUR regions, as shown in Feng et al, 2021 GRL; 2022 HESSD\n",
    "    # get the id list of each PUR region, save to list\n",
    "    regionID = list()\n",
    "    regionNum = list()\n",
    "    # seven regions including different HUCs\n",
    "    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ]\n",
    "    for ii in range(len(regionDivide)):\n",
    "        tempcomb = regionDivide[ii]\n",
    "        tempregid = list()\n",
    "        for ih in tempcomb:\n",
    "            tempid = gageid[hucinfo==ih].tolist()\n",
    "            tempregid = tempregid + tempid\n",
    "        regionID.append(tempregid)\n",
    "        regionNum.append(len(tempregid))\n",
    "\n",
    "    iexp = testfoldInd - 1  #index\n",
    "    TestLS = regionID[iexp] # basin ID list for testing, hold out for training\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS)) # basin ID for training\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID': TrainLS, 'TestID': TestLS}\n",
    "\n",
    "\n",
    "# apply buffOPt to solve the warm-up for the first year\n",
    "if buffOpt ==2: # load more BUFFTIME data for the first year\n",
    "    sd = utils.time.t2dt(Ttrain[0]) - dt.timedelta(days=BUFFTIME)\n",
    "    sdint = int(sd.strftime(\"%Y%m%d\"))\n",
    "    TtrainLoad = [sdint, Ttrain[1]]\n",
    "    TinvLoad = [sdint, Ttrain[1]]\n",
    "else:\n",
    "    TtrainLoad = Ttrain\n",
    "    TinvLoad = Tinv\n",
    "\n",
    "## prepare input data\n",
    "## load camels dataset\n",
    "if forType == 'daymet':\n",
    "    varF = ['prcp', 'tmean']\n",
    "    varFInv = ['prcp', 'tmean']\n",
    "else:\n",
    "    varF = ['prcp', 'tmax'] # For CAMELS maurer and nldas forcings, tmax is actually tmean\n",
    "    varFInv = ['prcp', 'tmax']\n",
    "\n",
    "# the attributes used to learn parameters\n",
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "               'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "               'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "               'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "               'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
    "\n",
    "optData = default.optDataCamels # a default dictionary for logging, updated below\n",
    "# Update the training period and variables\n",
    "optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS, forType=forType)\n",
    "\n",
    "# for HBV model training inputs\n",
    "dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n",
    "forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "obsUN = dfTrain.getDataObs(doNorm=False, rmNan=False, basinnorm=False, flow_regime=flow_regime)\n",
    "\n",
    "# for dPL inversion data, inputs of gA\n",
    "dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "\n",
    "# Unit transformation, discharge obs from ft3/s to mm/day\n",
    "areas = gageinfo['area'][TrainInd] # unit km2\n",
    "temparea = np.tile(areas[:, None, None], (1, obsUN.shape[1],1))\n",
    "obsUN = (obsUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3 # transform to mm/day\n",
    "\n",
    "# load potential ET calculated by hargreaves method\n",
    "varLstNL = ['PEVAP']\n",
    "usgsIdLst = gageid\n",
    "if forType == 'maurer':\n",
    "    tPETRange = [19800101, 20090101]\n",
    "else:\n",
    "    tPETRange = [19800101, 20150101]\n",
    "tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "\n",
    "# Modify this as the directory where you put PET\n",
    "PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n",
    "ntime = len(tPETLst)\n",
    "PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "for k in range(len(usgsIdLst)):\n",
    "    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "    PETfull[k, :, :] = dataTemp\n",
    "\n",
    "TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "PETUN = PETfull[:, ind2, :]\n",
    "PETUN = PETUN[TrainInd, :, :] # select basins\n",
    "C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "PETInvUN = PETfull[:, ind2inv, :]\n",
    "PETInvUN = PETInvUN[TrainInd, :, :]\n",
    "\n",
    "# process data, do normalization and remove nan\n",
    "series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n",
    "seriesvarLst = varFInv + ['pet']\n",
    "# calculate statistics for normalization and saved to a dictionary\n",
    "statDict = camels.getStatDic(attrLst=attrnewLst, attrdata=attrsUN, seriesLst=seriesvarLst, seriesdata=series_inv, flow_regime=flow_regime)\n",
    "# normalize data\n",
    "attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict, toNorm=True, flow_regime=flow_regime)\n",
    "attr_norm[np.isnan(attr_norm)] = 0.0\n",
    "series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True, flow_regime=flow_regime)\n",
    "series_norm[np.isnan(series_norm)] = 0.0\n",
    "\n",
    "# prepare the inputs\n",
    "zTrain = series_norm # used as the inputs for dPL inversion gA along with attributes\n",
    "xTrain = np.concatenate([forcUN, PETUN], axis=2) # used as HBV forcing\n",
    "xTrain[np.isnan(xTrain)] = 0.0\n",
    "\n",
    "if buffOpt == 1: # repeat the first year warm up the first year itself\n",
    "    zTrainIn = np.concatenate([zTrain[:,0:BUFFTIME,:], zTrain], axis=1)\n",
    "    xTrainIn = np.concatenate([xTrain[:,0:BUFFTIME,:], xTrain], axis=1) # repeat forcing to warm up the first year\n",
    "    yTrainIn = np.concatenate([obsUN[:,0:BUFFTIME,:], obsUN], axis=1)\n",
    "else: # no repeat, original data, the first year data would only be used as warmup for the next following year\n",
    "    zTrainIn = zTrain\n",
    "    xTrainIn = xTrain\n",
    "    yTrainIn = obsUN\n",
    "\n",
    "forcTuple = (xTrainIn, zTrainIn)\n",
    "attrs = attr_norm\n",
    "\n",
    "## Train the model\n",
    "# define loss function\n",
    "alpha = 0.25 # a weight for RMSE loss to balance low and peak flow\n",
    "optLoss = default.update(default.optLossComb, name='hydroDL.model.crit.RmseLossComb', weight=alpha)\n",
    "lossFun = crit.RmseLossComb(alpha=alpha)\n",
    "\n",
    "# define training options\n",
    "optTrain = default.update(default.optTrainCamels, miniBatch=[BATCH_SIZE, RHO], nEpoch=EPOCH, saveEpoch=saveEPOCH)\n",
    "# define output folder to save model results\n",
    "exp_name = 'CAMELSDemo'\n",
    "exp_disp = 'dPLHBV/' + puN + TDN + forType + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "           '/Fold' + str(testfoldInd)\n",
    "exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n",
    "           +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n",
    "save_path = os.path.join(exp_name, exp_disp)\n",
    "out = os.path.join(rootOut, save_path, exp_info) # output folder to save results\n",
    "# define and load model\n",
    "Ninv = zTrain.shape[-1] + attrs.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prcp': [-1.0, 0.533597287717274, -0.5355190506932449, 0.6686731458008723],\n",
       " 'tmean': [-3.6750000000000003, 23.87, 10.222146296879702, 10.520438079586562],\n",
       " 'pet': [0.57028911, 5.60688872, 2.943688350438002, 1.8915542037135196],\n",
       " 'p_mean': [1.55055167693361,\n",
       "  4.64999041752225,\n",
       "  3.257722061222179,\n",
       "  1.4093900773162575],\n",
       " 'pet_mean': [2.1470467761807,\n",
       "  3.55741331964408,\n",
       "  2.7873652197155474,\n",
       "  0.5545586452738964],\n",
       " 'p_seasonality': [-0.836194748180532,\n",
       "  0.621476145395479,\n",
       "  -0.04127946765143021,\n",
       "  0.5270366120903908],\n",
       " 'frac_snow': [0.0030982528891208,\n",
       "  0.52956688772688,\n",
       "  0.1776040592521807,\n",
       "  0.2031358038480485],\n",
       " 'aridity': [0.543704960792406,\n",
       "  1.87330300304748,\n",
       "  1.0565155490835534,\n",
       "  0.6149898629889541],\n",
       " 'high_prec_freq': [13.75, 25.85, 20.930998509687036, 4.541832424799546],\n",
       " 'high_prec_dur': [1.16627078384798,\n",
       "  1.6517571884984,\n",
       "  1.3498370635820642,\n",
       "  0.1910832155245367],\n",
       " 'low_prec_freq': [204.35, 300.45, 254.64731743666167, 35.089201003742616],\n",
       " 'low_prec_dur': [3.45440956651719,\n",
       "  9.44347826086956,\n",
       "  5.953786629968722,\n",
       "  3.197050807990256],\n",
       " 'elev_mean': [101.5, 2085.87, 759.4218777943368, 785.4110468749818],\n",
       " 'slope_mean': [4.38699, 116.4248, 46.19530734724292, 47.08567516434056],\n",
       " 'area_gages2': [38.23, 1781.61, 792.6180625931445, 1700.6816807451587],\n",
       " 'frac_forest': [0.0201, 0.9964, 0.6395393442622951, 0.37235636613312806],\n",
       " 'lai_max': [1.16382103006498,\n",
       "  5.04213474288441,\n",
       "  3.215969887067216,\n",
       "  1.5190004763762706],\n",
       " 'lai_diff': [0.752624093190321,\n",
       "  4.28948183933672,\n",
       "  2.448587613941409,\n",
       "  1.3328168318494504],\n",
       " 'gvf_max': [0.463035338065315,\n",
       "  0.885626193492327,\n",
       "  0.7221043008882689,\n",
       "  0.16819328774329798],\n",
       " 'gvf_diff': [0.128325498977479,\n",
       "  0.504552265673554,\n",
       "  0.3227487481524666,\n",
       "  0.1484538865500057],\n",
       " 'dom_land_cover_frac': [0.539464124495951,\n",
       "  0.999999999999894,\n",
       "  0.8099687736977969,\n",
       "  0.1838098286713087],\n",
       " 'dom_land_cover': [2.0, 11.0, 6.078986587183309, 2.9260161241841307],\n",
       " 'root_depth_50': [0.1321499917652488,\n",
       "  0.22460713150891218,\n",
       "  0.17881443061069846,\n",
       "  0.03090319048356531],\n",
       " 'soil_depth_pelletier': [0.791666666666667,\n",
       "  41.9248366013072,\n",
       "  10.872827963069417,\n",
       "  16.227227941663568],\n",
       " 'soil_depth_statsgo': [0.862333787940201,\n",
       "  1.50000001490116,\n",
       "  1.2931724691699245,\n",
       "  0.26987779532688716],\n",
       " 'soil_porosity': [0.41658977506002,\n",
       "  0.467676694953845,\n",
       "  0.4426183213996412,\n",
       "  0.02356828843810459],\n",
       " 'soil_conductivity': [0.653269266776188,\n",
       "  2.91634943889388,\n",
       "  1.7405237797425586,\n",
       "  1.5212737409484445],\n",
       " 'max_water_content': [0.322782379498217,\n",
       "  0.684376076224964,\n",
       "  0.5280316787759867,\n",
       "  0.14664307119509448],\n",
       " 'sand_frac': [17.202253171098,\n",
       "  56.655488319526,\n",
       "  36.468223000026654,\n",
       "  15.616220417993468],\n",
       " 'silt_frac': [16.7882600008835,\n",
       "  51.305546712209,\n",
       "  33.85860457309613,\n",
       "  13.241899227329707],\n",
       " 'clay_frac': [8.16280077308793,\n",
       "  32.7614458754766,\n",
       "  19.885857854245483,\n",
       "  9.312995271476105],\n",
       " 'geol_1st_class': [3.0, 11.0, 7.640834575260805, 3.3924702131339743],\n",
       " 'glim_1st_class_frac': [0.492424217677858,\n",
       "  1.0,\n",
       "  0.7855461965732077,\n",
       "  0.20202002064114472],\n",
       " 'geol_2nd_class': [-1.0, 11.0, 5.426229508196721, 4.64124231157111],\n",
       " 'glim_2nd_class_frac': [0.0,\n",
       "  0.369166122526313,\n",
       "  0.15542607184978768,\n",
       "  0.14363069081266147],\n",
       " 'carbonate_rocks_frac': [0.0,\n",
       "  0.532724007641709,\n",
       "  0.11874262265363414,\n",
       "  0.26066758492904074],\n",
       " 'geol_porostiy': [0.020890000000000002,\n",
       "  0.2167,\n",
       "  0.12637290419161679,\n",
       "  0.06951330915048355],\n",
       " 'geol_permeability': [-15.4207,\n",
       "  -12.3177,\n",
       "  -13.885704619970195,\n",
       "  1.1835051170736903]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.20416251, -0.39372375, -0.8220996 ],\n",
       "        [ 1.30078058,  0.02926244, -0.56379756],\n",
       "        [ 1.49950266,  0.15330671, -0.66241314],\n",
       "        ...,\n",
       "        [ 1.17727655, -0.29629434, -0.69075544],\n",
       "        [-0.69463078, -0.51634221, -0.50557539],\n",
       "        [-0.69463078, -0.1418331 , -0.22237922]],\n",
       "\n",
       "       [[-0.69463078,  0.12716711, -0.47018761],\n",
       "        [-0.69463078,  0.25643929, -0.47469026],\n",
       "        [ 1.40663177,  0.38000829, -0.63604434],\n",
       "        ...,\n",
       "        [ 1.29264226,  0.08867061, -0.42786305],\n",
       "        [-0.69463078, -0.33621664, -0.39681128],\n",
       "        [-0.69463078, -0.24163883, -0.31861828]],\n",
       "\n",
       "       [[ 1.12832949, -0.10571293, -0.66361204],\n",
       "        [ 1.05319639,  0.0682342 , -0.59036572],\n",
       "        [ 1.57072137,  0.32392698, -0.59902711],\n",
       "        ...,\n",
       "        [ 1.31296095,  0.0078755 , -0.44710892],\n",
       "        [-0.69463078, -0.48117258, -0.49237815],\n",
       "        [-0.69463078, -0.29581908, -0.32006159]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.69463078,  0.60338302, -0.00402899],\n",
       "        [-0.69463078,  0.85099628,  0.43479032],\n",
       "        [-0.69463078,  0.74120998,  0.28747696],\n",
       "        ...,\n",
       "        [ 1.63843851,  0.03021297, -0.75589145],\n",
       "        [ 1.50646605,  0.12811764, -0.6020422 ],\n",
       "        [ 1.33600254,  0.16756467, -0.55840454]],\n",
       "\n",
       "       [[-0.69463078,  0.72742728,  0.45270796],\n",
       "        [-0.69463078,  0.92418715,  0.69476553],\n",
       "        [-0.69463078,  0.82248036,  0.53165151],\n",
       "        ...,\n",
       "        [-0.69463078,  0.0820169 , -0.47848058],\n",
       "        [ 1.25614467, -0.00875879, -0.40936916],\n",
       "        [-0.69463078,  0.09484906, -0.26052457]],\n",
       "\n",
       "       [[-0.69463078,  0.65043429,  0.09847852],\n",
       "        [-0.69463078,  0.92703874,  0.45346607],\n",
       "        [-0.69463078,  0.54207379, -0.00410316],\n",
       "        ...,\n",
       "        [ 1.39162741,  0.0682342 , -0.68991195],\n",
       "        [ 1.37064814,  0.12811764, -0.44168354],\n",
       "        [-0.69463078,  0.17231732, -0.43758405]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True, flow_regime=flow_regime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.1      ,  6.08     ,  1.3886424],\n",
       "        [ 4.24     , 10.53     ,  1.8772347],\n",
       "        [ 8.02     , 11.835    ,  1.690698 ],\n",
       "        ...,\n",
       "        [ 2.84     ,  7.105    ,  1.637087 ],\n",
       "        [ 0.       ,  4.79     ,  1.9873651],\n",
       "        [ 0.       ,  8.73     ,  2.523046 ]],\n",
       "\n",
       "       [[ 0.       , 11.56     ,  2.054303 ],\n",
       "        [ 0.       , 12.92     ,  2.045786 ],\n",
       "        [ 5.96     , 14.22     ,  1.740576 ],\n",
       "        ...,\n",
       "        [ 4.13     , 11.155    ,  2.1343622],\n",
       "        [ 0.       ,  6.685    ,  2.1930983],\n",
       "        [ 0.       ,  7.68     ,  2.3410046]],\n",
       "\n",
       "       [[ 2.42     ,  9.11     ,  1.6884302],\n",
       "        [ 1.89     , 10.94     ,  1.8269796],\n",
       "        [10.06     , 13.63     ,  1.8105961],\n",
       "        ...,\n",
       "        [ 4.41     , 10.305    ,  2.0979576],\n",
       "        [ 0.       ,  5.16     ,  2.0123284],\n",
       "        [ 0.       ,  7.11     ,  2.3382745]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.       , 16.57     ,  2.9360673],\n",
       "        [ 0.       , 19.175    ,  3.7661178],\n",
       "        [ 0.       , 18.02     ,  3.4874666],\n",
       "        ...,\n",
       "        [12.47     , 10.54     ,  1.5138787],\n",
       "        [ 8.2      , 11.57     ,  1.8048929],\n",
       "        [ 4.75     , 11.985    ,  1.8874359]],\n",
       "\n",
       "       [[ 0.       , 17.875    ,  3.80001  ],\n",
       "        [ 0.       , 19.945    ,  4.257875 ],\n",
       "        [ 0.       , 18.875    ,  3.949336 ],\n",
       "        ...,\n",
       "        [ 0.       , 11.085    ,  2.0386164],\n",
       "        [ 3.67     , 10.13     ,  2.1693444],\n",
       "        [ 0.       , 11.22     ,  2.450892 ]],\n",
       "\n",
       "       [[ 0.       , 17.065    ,  3.1299658],\n",
       "        [ 0.       , 19.975    ,  3.801444 ],\n",
       "        [ 0.       , 15.925    ,  2.935927 ],\n",
       "        ...,\n",
       "        [ 5.68     , 10.94     ,  1.6386825],\n",
       "        [ 5.31     , 11.57     ,  2.10822  ],\n",
       "        [ 0.       , 12.035    ,  2.1159744]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TDOpt is False:\n",
    "    # model with all static parameters\n",
    "    # model = rnn_new.MultiInv_HBVModel(ninv=Ninv, nfea=Nfea, nmul=Nmul, hiddeninv=HIDDENSIZE, inittime=BUFFTIME,\n",
    "    #                                   routOpt = routing, comprout=comprout, compwts=compwts, pcorr=pcorr, multiforcing=multiforcing, prcp_datatypes=len(forType))\n",
    "    model = rnn.prcp_weights(ninv=Ninv, hiddeninv=HIDDENSIZE, prcp_datatypes=len(forType))\n",
    "    # dict only for logging\n",
    "    optModel = OrderedDict(name='LSTM-dPLHBV', nx=Ninv, nfea=Nfea, nmul=Nmul, hiddenSize=HIDDENSIZE, doReLU=True,\n",
    "                           Tinv=Tinv, Trainbuff=BUFFTIME, routOpt=routing, comprout=comprout, compwts=compwts,\n",
    "                           pcorr=pcorr, buffOpt=buffOpt, TDOpt=TDOpt)\n",
    "else:\n",
    "    # model with some dynamic parameters\n",
    "    # model = rnn_new.MultiInv_HBVTDModel(ninv=Ninv, nfea=Nfea, nmul=Nmul, hiddeninv=HIDDENSIZE, inittime=BUFFTIME,\n",
    "    #                               routOpt=routing, comprout=comprout, compwts=compwts, staind=staind, tdlst=tdRep,\n",
    "    #                               dydrop=dydrop, ETMod=ETMod, multiforcing=multiforcing, prcp_datatypes=len(forType))\n",
    "    model = rnn.prcp_weights(ninv=Ninv, hiddeninv=HIDDENSIZE, prcp_datatypes=len(forType))\n",
    "\n",
    "    # dict only for logging\n",
    "    optModel = OrderedDict(name='LSTM-dPLHBV', nx=Ninv, nfea=Nfea, nmul=Nmul, hiddenSize=HIDDENSIZE, doReLU=True,\n",
    "                           Tinv=Tinv, Trainbuff=BUFFTIME, routOpt=routing, comprout=comprout, compwts=compwts,\n",
    "                           pcorr=pcorr, staind=staind, tdlst=tdRep, dydrop=dydrop,buffOpt=buffOpt, TDOpt=TDOpt, ETMod=ETMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from hydroDL.model.dropout import DropMask, createMask\n",
    "\n",
    "\n",
    "\n",
    "class CudnnLstm(nn.Module):\n",
    "    def __init__(self, *, inputSize, hiddenSize, dr=0.5, drMethod=\"drW\", gpu=0, seed=42):\n",
    "        super(CudnnLstm, self).__init__()\n",
    "        self.name = 'CudnnLstm'\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.dr = dr\n",
    "\n",
    "        self.w_ih = Parameter(torch.Tensor(hiddenSize * 4, inputSize))\n",
    "        self.w_hh = Parameter(torch.Tensor(hiddenSize * 4, hiddenSize))\n",
    "        self.b_ih = Parameter(torch.Tensor(hiddenSize * 4))\n",
    "        self.b_hh = Parameter(torch.Tensor(hiddenSize * 4))\n",
    "        self._all_weights = [['w_ih', 'w_hh', 'b_ih', 'b_hh']]\n",
    "        self.cuda()\n",
    "        self.seed = seed\n",
    "        self.is_legacy = True\n",
    "\n",
    "        self.reset_mask()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(CudnnLstm, self)._apply(fn)\n",
    "        return ret\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        super(CudnnLstm, self).__setstate__(d)\n",
    "        self.__dict__.setdefault(\"_data_ptrs\", [])\n",
    "        if \"all_weights\" in d:\n",
    "            self._all_weights = d[\"all_weights\"]\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        self._all_weights = [[\"w_ih\", \"w_hh\", \"b_ih\", \"b_hh\"]]\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.maskW_ih = createMask(self.w_ih, self.dr, self.seed)\n",
    "        self.maskW_hh = createMask(self.w_hh, self.dr, self.seed)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hiddenSize)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx=None, cx=None, doDropMC=False, dropoutFalse=False):\n",
    "        # dropoutFalse: it will ensure doDrop is false, unless doDropMC is true\n",
    "        if dropoutFalse and (not doDropMC):\n",
    "            doDrop = False\n",
    "        elif self.dr > 0 and (doDropMC is True or self.training is True):\n",
    "            doDrop = True\n",
    "        else:\n",
    "            doDrop = False\n",
    "\n",
    "        batchSize = input.size(1)\n",
    "            \n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(1, batchSize, self.hiddenSize, requires_grad=False)\n",
    "        if cx is None:\n",
    "            cx = input.new_zeros(1, batchSize, self.hiddenSize, requires_grad=False)\n",
    "\n",
    "        # cuDNN backend - disabled flat weight\n",
    "        # handle = torch.backends.cudnn.get_handle()\n",
    "        if doDrop is True:\n",
    "            self.reset_mask()\n",
    "            weight = [\n",
    "                DropMask.apply(self.w_ih, self.maskW_ih, True),\n",
    "                DropMask.apply(self.w_hh, self.maskW_hh, True), self.b_ih,\n",
    "                self.b_hh\n",
    "            ]\n",
    "        else:\n",
    "            weight = [self.w_ih, self.w_hh, self.b_ih, self.b_hh]\n",
    "\n",
    "        # output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "        # input, weight, 4, None, hx, cx, torch.backends.cudnn.CUDNN_LSTM,\n",
    "        # self.hiddenSize, 1, False, 0, self.training, False, (), None)\n",
    "        if torch.__version__ < \"1.8\":\n",
    "            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "                input,\n",
    "                weight,\n",
    "                4,\n",
    "                None,\n",
    "                hx,\n",
    "                cx,\n",
    "                2,  # 2 means LSTM\n",
    "                self.hiddenSize,\n",
    "                1,\n",
    "                False,\n",
    "                0,\n",
    "                self.training,\n",
    "                False,\n",
    "                (),\n",
    "                None,\n",
    "            )\n",
    "        else:\n",
    "            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "                input,\n",
    "                weight,\n",
    "                4,\n",
    "                None,\n",
    "                hx,\n",
    "                cx,\n",
    "                2,  # 2 means LSTM\n",
    "                self.hiddenSize,\n",
    "                0,\n",
    "                1,\n",
    "                False,\n",
    "                0,\n",
    "                self.training,\n",
    "                False,\n",
    "                (),\n",
    "                None,\n",
    "            )\n",
    "        return output, (hy, cy)\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [\n",
    "            [getattr(self, weight) for weight in weights]\n",
    "            for weights in self._all_weights\n",
    "        ]\n",
    "    \n",
    "\n",
    "\n",
    "class CudnnLstmModel(torch.nn.Module):\n",
    "    def __init__(self, *, nx, ny, hiddenSize, dr=0.5, warmUpDay=None):\n",
    "        super(CudnnLstmModel, self).__init__()\n",
    "        self.name = 'CudnnLstmModel'\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.ct = 0\n",
    "        self.nLayer = 1\n",
    "        self.linearIn = torch.nn.Linear(nx, hiddenSize)\n",
    "\n",
    "        self.lstm = CudnnLstm(\n",
    "            inputSize=hiddenSize, hiddenSize=hiddenSize, dr=dr\n",
    "        )\n",
    "        self.linearOut = torch.nn.Linear(hiddenSize, ny)\n",
    "        self.gpu = 1\n",
    "        self.is_legacy = True\n",
    "        # self.drtest = torch.nn.Dropout(p=0.4)\n",
    "        self.warmUpDay = warmUpDay\n",
    "\n",
    "    def forward(self, x, doDropMC=False, dropoutFalse=False):\n",
    "        \"\"\"\n",
    "        :param inputs: a dictionary of input data (x and potentially z data)\n",
    "        :param doDropMC:\n",
    "        :param dropoutFalse:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # if not self.warmUpDay is None:\n",
    "        #     x, warmUpDay = self.extend_day(x, warm_up_day=self.warmUpDay)\n",
    "\n",
    "        x0 = F.relu(self.linearIn(x))\n",
    "\n",
    "        outLSTM, (hn, cn) = self.lstm(\n",
    "            x0, doDropMC=doDropMC, dropoutFalse=dropoutFalse\n",
    "        )\n",
    "        # outLSTMdr = self.drtest(outLSTM)\n",
    "        out = self.linearOut(outLSTM)\n",
    "        \n",
    "        # if not self.warmUpDay is None:\n",
    "        #     out = self.reduce_day(out, warm_up_day=self.warmUpDay)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def extend_day(self, x, warm_up_day):\n",
    "        x_num_day = x.shape[0]\n",
    "        warm_up_day = min(x_num_day, warm_up_day)\n",
    "        x_select = x[:warm_up_day, :, :]\n",
    "        x = torch.cat([x_select, x], dim=0)\n",
    "        return x, warm_up_day\n",
    "\n",
    "    def reduce_day(self, x, warm_up_day):\n",
    "        x = x[warm_up_day:,:,:]\n",
    "        return x\n",
    "\n",
    "\n",
    "class RangeBoundLoss(nn.Module):\n",
    "    \"\"\"limit parameters from going out of range\"\"\"\n",
    "    def __init__(self, lb, ub):\n",
    "        super(RangeBoundLoss, self).__init__()\n",
    "        self.lb = torch.tensor(lb).cuda()\n",
    "        self.ub = torch.tensor(ub).cuda()\n",
    "        # self.factor = torch.tensor(factor).cuda()\n",
    "\n",
    "    def forward(self, params, factor):\n",
    "        factor = torch.tensor(factor).cuda()\n",
    "        loss = 0\n",
    "        for i in range(len(params)):\n",
    "            lb = self.lb[i]\n",
    "            ub = self.ub[i]\n",
    "            upper_bound_loss = factor * torch.relu(params[i] - ub).mean()\n",
    "            lower_bound_loss = factor * torch.relu(lb - params[i]).mean()\n",
    "            loss = loss + upper_bound_loss + lower_bound_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleWeights(torch.nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, nmodels=1):\n",
    "        super(EnsembleWeights, self).__init__()\n",
    "        self.name = 'EnsembleWeights'\n",
    "        self.ninv = ninv\n",
    "        self.nmodels = nmodels\n",
    "\n",
    "        ntp = nmodels\n",
    "        self.hiddeninv = hiddeninv\n",
    "\n",
    "        self.lstminv = CudnnLstmModel(\n",
    "            nx=ninv, ny=ntp, hiddenSize=hiddeninv, dr=drinv)\n",
    "        lb_prcp = [0.95]\n",
    "        ub_prcp = [1.05]\n",
    "        self.RangeBoundLoss = RangeBoundLoss(lb=lb_prcp, ub=ub_prcp)\n",
    "\n",
    "    def forward(self, x, z, prcp_loss_factor):\n",
    "        z.requires_grad = True\n",
    "\n",
    "        wghts = self.lstminv(z)\n",
    "        ntstep = wghts.shape[0]\n",
    "        ngage = wghts.shape[1]\n",
    "        wghts_scaled = torch.sigmoid(wghts)  # Change weighting method here.\n",
    "\n",
    "        prcp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        prcp_wghts_sum = torch.sum(wghts_scaled, dim=2)\n",
    "        range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)\n",
    "\n",
    "        for para in range(wghts.shape[2]):\n",
    "            prcp_wavg = prcp_wavg + wghts_scaled[:, :, para] * x[:, :, para]\n",
    "\n",
    "        x_new = torch.empty((ntstep, ngage, 3), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        x_new[:, :, 0] = prcp_wavg\n",
    "        x_new[:, :, 1] = x[:, :, self.prcp_datatypes]\n",
    "        x_new[:, :, 2] = x[:, :, -1]\n",
    "\n",
    "        # For gradient analysis.\n",
    "        # grad_daymet = autograd.grad(outputs=wghts_scaled[:, :, 0], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 0]), retain_graph=True)[0]\n",
    "        # grad_maurer = autograd.grad(outputs=wghts_scaled[:, :, 1], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 1]), retain_graph=True)[0]\n",
    "        # grad_nldas = autograd.grad(outputs=wghts_scaled[:, :, 2], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 2]), retain_graph=True)[0]\n",
    "\n",
    "        # return x_new, range_bound_loss_prcp, wghts_scaled, grad_daymet, grad_maurer, grad_nldas\n",
    "        return x_new, range_bound_loss_prcp, wghts_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import ValuesView\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def randomIndex(ngrid, nt, dimSubset, bufftime=0):\n",
    "    batchSize, rho = dimSubset\n",
    "    iGrid = np.random.randint(0, ngrid, [batchSize])\n",
    "    iT = np.random.randint(0 + bufftime, nt - rho, [batchSize])\n",
    "    return iGrid, iT\n",
    "\n",
    "\n",
    "\n",
    "def selectSubset(x, iGrid, iT, rho, *, c=None, tupleOut=False, LCopt=False, bufftime=0):\n",
    "    nx = x.shape[-1]\n",
    "    nt = x.shape[1]\n",
    "    if x.shape[0] == len(iGrid):   #hack\n",
    "        iGrid = np.arange(0,len(iGrid))  # hack\n",
    "    if nt <= rho:\n",
    "        iT.fill(0)\n",
    "\n",
    "    batchSize = iGrid.shape[0]\n",
    "    if iT is not None:\n",
    "        # batchSize = iGrid.shape[0]\n",
    "        xTensor = torch.zeros([rho+bufftime, batchSize, nx], requires_grad=False)\n",
    "        for k in range(batchSize):\n",
    "            temp = x[iGrid[k]:iGrid[k] + 1, np.arange(iT[k]-bufftime, iT[k] + rho), :]\n",
    "            xTensor[:, k:k + 1, :] = torch.from_numpy(np.swapaxes(temp, 1, 0))\n",
    "    else:\n",
    "        if LCopt is True:\n",
    "            # used for local calibration kernel: FDC, SMAP...\n",
    "            if len(x.shape) == 2:\n",
    "                # Used for local calibration kernel as FDC\n",
    "                # x = Ngrid * Ntime\n",
    "                xTensor = torch.from_numpy(x[iGrid, :]).float()\n",
    "            elif len(x.shape) == 3:\n",
    "                # used for LC-SMAP x=Ngrid*Ntime*Nvar\n",
    "                xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 2)).float()\n",
    "        else:\n",
    "            # Used for rho equal to the whole length of time series\n",
    "            xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 0)).float()\n",
    "            rho = xTensor.shape[0]\n",
    "    if c is not None:\n",
    "        nc = c.shape[-1]\n",
    "        temp = np.repeat(\n",
    "            np.reshape(c[iGrid, :], [batchSize, 1, nc]), rho+bufftime, axis=1)\n",
    "        cTensor = torch.from_numpy(np.swapaxes(temp, 1, 0)).float()\n",
    "\n",
    "        if (tupleOut):\n",
    "            if torch.cuda.is_available():\n",
    "                xTensor = xTensor.cuda()\n",
    "                cTensor = cTensor.cuda()\n",
    "            out = (xTensor, cTensor)\n",
    "        else:\n",
    "            out = torch.cat((xTensor, cTensor), 2)\n",
    "    else:\n",
    "        out = xTensor\n",
    "\n",
    "    if torch.cuda.is_available() and type(out) is not tuple:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainModel(model,\n",
    "                loaded_hbv,\n",
    "                x,\n",
    "                y,\n",
    "                c,\n",
    "                lossFun,\n",
    "                *,\n",
    "                nEpoch=500,\n",
    "                miniBatch=[100, 30],\n",
    "                saveEpoch=100,\n",
    "                saveFolder=None,\n",
    "                mode='seq2seq',\n",
    "                bufftime=0,\n",
    "                prcp_loss_factor = 15,\n",
    "                smooth_loss_factor = 0,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x - input\n",
    "    z - normalized x input\n",
    "    y - target, or observed values\n",
    "    c - constant input, attributes\n",
    "    \"\"\"\n",
    "\n",
    "    batchSize, rho = miniBatch\n",
    "    if type(x) is tuple or type(x) is list:\n",
    "        x, z = x \n",
    "\n",
    "    ngrid, nt, nx = x.shape  # ngrid= # basins, nt= # timesteps, nx= # attributes\n",
    "\n",
    "    if c is not None:\n",
    "        nx = nx + c.shape[-1]\n",
    "    if batchSize >= ngrid:\n",
    "        # Cannot have more batches than there are basins.\n",
    "        batchSize = ngrid\n",
    "\n",
    "    nIterEp = int(\n",
    "        np.ceil(np.log(0.01) / np.log(1 - batchSize * rho / ngrid / (nt-bufftime)))\n",
    "        )\n",
    "    if hasattr(model, 'ctRm'):\n",
    "        if model.ctRm is True:\n",
    "            nIterEp = int(\n",
    "                np.ceil(\n",
    "                    np.log(0.01) / np.log(1 - batchSize *\n",
    "                                          (rho - model.ct) / ngrid / (nt-bufftime))))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        lossFun = lossFun.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    optim = torch.optim.Adadelta(list(model.parameters()))\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Save file.\n",
    "    if saveFolder is not None:\n",
    "        os.makedirs(saveFolder, exist_ok=True)\n",
    "        runFile = os.path.join(saveFolder, 'run.csv')\n",
    "        rf = open(runFile, 'w+')\n",
    "\n",
    "    for iEpoch in range(1, nEpoch + 1):\n",
    "        lossEp = 0\n",
    "        loss_prcp_Ep = 0\n",
    "        loss_sf_Ep = 0\n",
    "        # loss_smooth_Ep = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        prog_str = \"Epoch \" + str(iEpoch) + \"/\" + str(nEpoch)\n",
    "\n",
    "        for iIter in tqdm(range(0, nIterEp), desc=prog_str, leave=False):\n",
    "            # training iterations\n",
    "            if type(model) in [EnsembleWeights]:\n",
    "                iGrid, iT = randomIndex(ngrid, nt, [batchSize, rho], bufftime=bufftime)\n",
    "                \n",
    "                xTrain = selectSubset(x, iGrid, iT, rho, bufftime=bufftime)\n",
    "                yTrain = selectSubset(y, iGrid, iT, rho)\n",
    "                zTrain = selectSubset(z, iGrid, iT, rho)\n",
    "                    \n",
    "                # calculate loss and weights `wt`.\n",
    "                xP, prcp_loss, prcp_weights = model(xTrain, zTrain, prcp_loss_factor)\n",
    "                yP = np.sum(xTrain * prcp_weights, axis=2)\n",
    "                \n",
    "            else:\n",
    "                Exception('unknown model')\n",
    "\n",
    "            # Consider the buff time for initialization.\n",
    "            if bufftime > 0:\n",
    "                yP = yP[bufftime:,:,:]\n",
    "\n",
    "            # get loss\n",
    "            if type(lossFun) in [crit.NSELossBatch, crit.NSESqrtLossBatch]:\n",
    "                loss_sf = lossFun(yP, yTrain, iGrid)\n",
    "                loss =  loss_sf + prcp_loss\n",
    "            else:\n",
    "                loss_sf = lossFun(yP, yTrain)\n",
    "                loss = loss_sf + prcp_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            lossEp = lossEp + loss.item()\n",
    "\n",
    "            try:\n",
    "                loss_prcp_Ep = loss_prcp_Ep + prcp_loss.item()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            loss_sf_Ep = loss_sf_Ep + loss_sf.item()\n",
    "\n",
    "            # if iIter % 100 == 0:\n",
    "            #     print('Iter {} of {}: Loss {:.3f}'.format(iIter, nIterEp, loss.item()))\n",
    "\n",
    "        # print loss\n",
    "        lossEp = lossEp / nIterEp\n",
    "        loss_sf_Ep = loss_sf_Ep / nIterEp\n",
    "        loss_prcp_Ep = loss_prcp_Ep / nIterEp\n",
    "\n",
    "        logStr = 'Epoch {} Loss {:.3f}, Streamflow Loss {:.3f}, Precipitation Loss {:.3f}, time {:.2f}'.format(\n",
    "            iEpoch, lossEp, loss_sf_Ep, loss_prcp_Ep,\n",
    "            time.time() - t0)\n",
    "        print(logStr)\n",
    "\n",
    "        # Save model and loss.\n",
    "        if saveFolder is not None:\n",
    "            rf.write(logStr + '\\n')\n",
    "            if iEpoch % saveEpoch == 0:\n",
    "                # save model\n",
    "                modelFile = os.path.join(saveFolder,\n",
    "                                         'model_Ep' + str(iEpoch) + '.pt')\n",
    "                torch.save(model, modelFile)\n",
    "\n",
    "    if saveFolder is not None:\n",
    "        rf.close()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainedModel = train.trainModel(\n",
    "    model,\n",
    "    forcTuple,\n",
    "    yTrainIn,\n",
    "    attrs,\n",
    "    lossFun,\n",
    "    nEpoch=EPOCH,\n",
    "    miniBatch=[BATCH_SIZE, RHO],\n",
    "    saveEpoch=saveEPOCH,\n",
    "    saveFolder=out,\n",
    "    bufftime=BUFFTIME,\n",
    "    prcp_loss_factor=prcp_loss_factor,\n",
    "    smooth_loss_factor=smooth_loss_factor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGML_STemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
