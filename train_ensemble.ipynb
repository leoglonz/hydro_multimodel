{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_hbv as hbvArgs\n",
    "from config.read_configurations import config_prms as prmsArgs\n",
    "from config.read_configurations import config_sacsma as sacsmaArgs\n",
    "from config.read_configurations import config_sacsma_snow as sacsmaSnowArgs\n",
    "from config.read_configurations import config_hbv_hydrodl as hbvhyArgs_d\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.stats\n",
    "# from post import plot\n",
    "\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import create_output_dirs\n",
    "from MODELS.loss_functions .get_loss_function import get_lossFun\n",
    "from MODELS.test_dp_HBV_dynamic import test_dp_hbv\n",
    "from core.data_processing.data_loading import loadData\n",
    "from core.data_processing.normalization import transNorm\n",
    "from core.data_processing.model import (\n",
    "    take_sample_test,\n",
    "    converting_flow_from_ft3_per_sec_to_mm_per_day\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "## GPU setting\n",
    "# which GPU to use when having multiple\n",
    "traingpuid = 0\n",
    "torch.cuda.set_device(traingpuid)\n",
    "\n",
    "\n",
    "\n",
    "# fix the random seeds for reproducibility\n",
    "def randomseed_config(seed):\n",
    "    if seed == None:  # args['randomseed'] is None:\n",
    "        # generate random seed\n",
    "        randomseed = int(np.random.uniform(low=0, high=1e6))\n",
    "        print(\"random seed updated!\")\n",
    "    else:\n",
    "        print(\"Setting seed 0.\")\n",
    "        # randomseed = args['randomseed']\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "randomseed_config(0)\n",
    "\n",
    "\n",
    "\n",
    "# Set path to `hydro_multimodel_results` directory.\n",
    "if platform.system() == 'Darwin':\n",
    "    # For mac os\n",
    "    out_dir = '/Users/leoglonz/Desktop/water/data/model_runs/hydro_multimodel_results'\n",
    "    # Some operations are not yet working with MPS, so we might need to set some environment variables to use CPU fall instead\n",
    "    # %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "elif platform.system() == 'Windows':\n",
    "    # For windows\n",
    "    out_dir = 'D:\\\\data\\\\model_runs\\\\hydro_multimodel_results\\\\'\n",
    "\n",
    "elif platform.system() == 'Linux':\n",
    "    # For Colab\n",
    "    out_dir = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results'\n",
    "\n",
    "else:\n",
    "    raise ValueError('Unsupported operating system.')\n",
    "\n",
    "\n",
    "##-----## Multi-model Parameters ##-----##\n",
    "##--------------------------------------##\n",
    "# Setting dictionaries to separately manage each diff model's attributes.\n",
    "models = {'dPLHBV_dyn': None,'SACSMA_snow':None, 'marrmot_PRMS':None}  # 'HBV':None, 'hbvhy': None, 'SACSMA_snow':None, 'SACSMA':None,\n",
    "args_list = {'dPLHBV_dyn': hbvhyArgs_d,'SACSMA_snow':sacsmaSnowArgs, 'marrmot_PRMS':prmsArgs}   # 'hbvhy': hbvhyArgs, 'HBV' : hbvArgs, 'SACSMA_snow':None, 'SACSMA': sacsmaArgs,\n",
    "ENSEMBLE_TYPE = 'max'  # 'median', 'avg', 'max', 'softmax'\n",
    "\n",
    "# Load test observations and predictions from a prior run.\n",
    "pred_path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_test_preds_obs', 'HBV_dynamic_SACSMA_snow_marrmot_PRMS', 'preds_multim_E50_B25_R365_BT365_H256_tr1980_1995_n16.npy')\n",
    "obs_path = os.path.join(out_dir, 'hydro_models', '671_sites_dp', 'merged_test_preds_obs', 'HBV_dynamic_SACSMA_snow_marrmot_PRMS', 'obs_multim_E50_B25_R365_BT365_H256_tr1980_1995_n16.npy')\n",
    "preds = np.load(pred_path, allow_pickle=True).item()\n",
    "obs = np.load(obs_path, allow_pickle=True).item()\n",
    "\n",
    "model_output = preds\n",
    "y_obs = obs\n",
    "model_output['dPLHBV_dyn'] = model_output.pop('HBV_dynamic')\n",
    "y_obs['dPLHBV_dyn'] = y_obs.pop('HBV_dynamic')\n",
    "\n",
    "# Initialize\n",
    "flow_preds = []\n",
    "flow_obs = None\n",
    "obs_trig = False\n",
    "\n",
    "# Concatenate individual model predictions, and observation data.\n",
    "for i, mod in enumerate(args_list):\n",
    "    args = args_list[mod]\n",
    "    mod_out = model_output[mod]\n",
    "    y_ob = y_obs[mod]\n",
    "\n",
    "    print(mod)\n",
    "\n",
    "    if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n",
    "        # Hydro models are tested in batches, so we concatenate them and select\n",
    "        # the desired flow.\n",
    "        # Note: modified HBV already has this preparation done during testing.\n",
    "\n",
    "        # Get flow predictions and swap axes to get shape [basins, days]\n",
    "        pred = np.swapaxes(torch.cat([d[\"flow_sim\"] for d in mod_out], dim=1).squeeze().numpy(), 0, 1)\n",
    "\n",
    "        if obs_trig == False:\n",
    "            # dPLHBV uses GAGES while the other hydro models use CAMELS data. This means small\n",
    "            # e-5 variation in observation data between the two. This is averaged if both models\n",
    "            # are used, but to avoid double-counting data from multiply hydro models, use a trigger.\n",
    "            obs = np.swapaxes(y_ob[:, :, args[\"target\"].index(\"00060_Mean\")].numpy(), 0, 1)\n",
    "            obs_trig = True\n",
    "            dup = False\n",
    "        else:\n",
    "            dup = True\n",
    "\n",
    "    elif mod in ['dPLHBV_dyn']:\n",
    "        pred = mod_out[:,:,0][:,365:] # Set dim2 = 0 to get streamflow Qr\n",
    "        obs = y_ob.squeeze()[:,365:]\n",
    "        dup = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type in `models`.\")\n",
    "\n",
    "    if i == 0:\n",
    "        tmp_pred = pred\n",
    "        tmp_obs = obs\n",
    "    elif i == 1:\n",
    "        tmp_pred = np.stack((tmp_pred, pred), axis=2)\n",
    "        if not dup:\n",
    "            # Avoid double-counting GAGES obs.\n",
    "            tmp_obs = np.stack((tmp_obs, obs), axis=2)\n",
    "    else:\n",
    "        # Combine outputs of >3 models.\n",
    "        tmp_pred = np.concatenate((tmp_pred,np.expand_dims(pred, 2)), axis=2)\n",
    "        if not dup:\n",
    "            # Avoid double-counting GAGES obs.\n",
    "            tmp_obs = np.concatenate((tmp_obs,np.expand_dims(obs, 2)), axis=2)\n",
    "\n",
    "preds = tmp_pred\n",
    "obs = tmp_obs\n",
    "\n",
    "# Merge observation data.\n",
    "if len(obs.shape) == 3:\n",
    "    comp_obs = np.mean(obs, axis = 2)\n",
    "elif len(obs.shape) == 2:\n",
    "    comp_obs = obs\n",
    "else:\n",
    "    raise ValueError(\"Error reading prediction data: incorrect formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 5.109021902084351\n",
      "read usgs streamflow 5.095563173294067\n",
      "read usgs streamflow 6.282549142837524\n",
      "daymet tmean was used!\n",
      "read usgs streamflow 4.992432117462158\n",
      "read usgs streamflow 4.333171844482422\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from hydroDL import master, utils\n",
    "from hydroDL.data import camels\n",
    "from hydroDL.master import default\n",
    "from hydroDL.master.master import loadModel\n",
    "from hydroDL.model import rnn, crit, train\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "## fix the random seeds for reproducibility\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## GPU setting\n",
    "# which GPU to use when having multiple\n",
    "traingpuid = 0\n",
    "# torch.cuda.set_device(traingpuid)\n",
    "\n",
    "## Setting training options here\n",
    "PUOpt = 0\n",
    "# PUOpt values and explanations:\n",
    "# 0: train and test on ALL basins;\n",
    "# 1 for PUB spatial test, randomly hold out basins;\n",
    "# 2 for PUR spatial test, hold out a continuous region;\n",
    "buffOpt = 1\n",
    "# buffOpt defines the warm-up option for the first year of training forcing data\n",
    "# 0: do nothing, the first year forcing would only be used to warm up the next year;\n",
    "# 1: repeat first year forcing to warm up the first year;\n",
    "# 2: load one more year forcing to warm up the first year\n",
    "TDOpt = True\n",
    "# TDOpt, True as using dynamic parameters and False as using static parameters\n",
    "forType = 'daymet'\n",
    "# for Type defines which forcing in CAMELS to use: 'daymet', 'nldas', 'maurer'\n",
    "\n",
    "\n",
    "# used for multimodel ensembling.\n",
    "prcp_loss_factor = 23\n",
    "smooth_loss_factor = 0\n",
    "\n",
    "## Set hyperparameters\n",
    "EPOCH = 50 # total epoches to train the mode\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "saveEPOCH = 10\n",
    "Ttrain = [19801001, 19951001] # Training period\n",
    "# Ttrain = [19891001, 19991001] # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # Inversion period for historical forcings\n",
    "# Tinv = [19891001, 19991001] # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters. 12:original HBV; 13:includes the added dynamic ET para when setting ETMod=True\n",
    "BUFFTIME = 365 # for each training sample, to use BUFFTIME days to warm up the states.\n",
    "routing = True # Whether to use the routing module for simulated runoff\n",
    "Nmul = 16 # Multi-component model. How many parallel HBV components to use. 1 means the original HBV.\n",
    "comprout = False # True is doing routing for each component\n",
    "compwts = False # True is using weighted average for components; False is the simple mean\n",
    "pcorr = None # or a list to give the range of precip correction\n",
    "\n",
    "flow_regime = 0  # 1 is high flow expert.\n",
    "\n",
    "if TDOpt is True:\n",
    "    # Below options are only for running models with dynamic parameters\n",
    "    tdRep = [1, 13] # When using dynamic parameters, this list defines which parameters to set as dynamic\n",
    "    tdRepS = [str(ix) for ix in tdRep]\n",
    "    # ETMod: if True, use the added shape parameter (index 13) for ET. Default as False.\n",
    "    # Must set below ETMod as True and Nfea=13 when including 13 index in above tdRep list for dynamic parameters\n",
    "    # If 13 not in tdRep list, set below ETMod=False and Nfea=12 to use the original HBV without ET shape para\n",
    "    ETMod = True\n",
    "    Nfea = 13 # should be 13 when setting ETMod=True. 12 when ETMod=False\n",
    "    dydrop = 0.0 # dropout possibility for those dynamic parameters: 0.0 always dynamic; 1.0 always static\n",
    "    staind = -1 # which time step to use from the learned para time series for those static parameters\n",
    "    TDN = '/TDTestforc/'+'TD'+\"_\".join(tdRepS) +'/'\n",
    "else:\n",
    "    TDN = '/Testforc/'\n",
    "\n",
    "# Define root directory of database and output\n",
    "# Modify these based on your own location of CAMELS dataset\n",
    "# Following the data download instruction in README file, you should organize the folders like\n",
    "# 'your/path/to/Camels/basin_timeseries_v1p2_metForcing_obsFlow' and 'your/path/to/Camels/camels_attributes_v2.0'\n",
    "# Then 'rootDatabase' here should be 'your/path/to/Camels';\n",
    "# 'rootOut' is the root dir where you save the trained model\n",
    "\n",
    "\n",
    "\n",
    "# CAMELS dataset root directory\n",
    "# sysroot = '/Users/leoglonz/Desktop/water/data'\n",
    "sysroot = '/content/'\n",
    "rootDatabase = os.path.join(os.path.sep, sysroot, 'Camels')    # CAMELS dataset root directory\n",
    "# rootDatabase = os.path.join(os.path.sep, 'data', 'kas7897', 'dPLHBVrelease')  # CAMELS dataset root directory\n",
    "camels.initcamels(flow_regime, forType=forType, rootDB=rootDatabase)  # initialize camels module-scope variables in camels.py (dirDB, gageDict) to read basin info\n",
    "\n",
    "# rootOut = os.path.join(os.path.sep, 'data', 'rnnStreamflow')  # Model output root directory\n",
    "rootOut = os.path.join(os.path.sep, sysroot, 'data', 'model_runs', 'hydro_multimodel_results', 'multimodels', '671_sites_dp', 'mm_ensemble_sigmoid')  # Model output root directory\n",
    "\n",
    "\n",
    "\n",
    "## If you have a checkpoint file for a previous run, set the path to directory here.\n",
    "# saved epoch is the epoch corresponding to the checkpoint.\n",
    "saved_epoch = 6\n",
    "checkpoint_file = None #'/Users/leoglonz/Desktop/water/data/model_runs/rnnStreamflow/CAMELSDemo/dPLHBV/ALL/TDTestforc/TD1_13/daymet/BuffOpt0/RMSE_para0.25/111111/Fold1/T_19801001_19951001_BS_100_HS_256_RHO_365_NF_13_Buff_365_Mul_16/'\n",
    "\n",
    "## set up different data loadings for ALL, PUB, PUR\n",
    "testfoldInd = 1\n",
    "# Which fold to hold out for PUB (10 folds, from 1 to 10) and PUR (7 folds, from 1 to 7).\n",
    "# It doesn't matter when training on ALL basins (setting PUOpt=0), could always set testfoldInd=1 for this case.\n",
    "\n",
    "# load CAMELS basin information\n",
    "gageinfo = camels.gageDict\n",
    "hucinfo = gageinfo['huc']\n",
    "gageid = gageinfo['id']\n",
    "gageidLst = gageid.tolist()\n",
    "\n",
    "if PUOpt == 0: # training on all basins without spatial hold-out\n",
    "    puN = 'ALL'\n",
    "    TrainLS = gageidLst # all basins\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    TestLS = gageidLst\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 1: # random hold out basins. hold out the fold set by testfoldInd\n",
    "    puN = 'PUB'\n",
    "    # load the PUB basin groups\n",
    "    # randomly divide CAMELS basins into 10 groups and this file contains the basin ID for each group\n",
    "    # located in splitPath\n",
    "    splitPath = 'PUBsplitLst.txt'\n",
    "    with open(splitPath, 'r') as fp:\n",
    "        testIDLst=json.load(fp)\n",
    "    # Generate training ID lists excluding the hold out fold\n",
    "    TestLS = testIDLst[testfoldInd - 1]\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS))\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID':TrainLS, 'TestID':TestLS}\n",
    "\n",
    "elif PUOpt == 2:\n",
    "    puN = 'PUR'\n",
    "    # Divide CAMELS dataset into 7 continous PUR regions, as shown in Feng et al, 2021 GRL; 2022 HESSD\n",
    "    # get the id list of each PUR region, save to list\n",
    "    regionID = list()\n",
    "    regionNum = list()\n",
    "    # seven regions including different HUCs\n",
    "    regionDivide = [ [1,2], [3,6], [4,5,7], [9,10], [8,11,12,13], [14,15,16,18], [17] ]\n",
    "    for ii in range(len(regionDivide)):\n",
    "        tempcomb = regionDivide[ii]\n",
    "        tempregid = list()\n",
    "        for ih in tempcomb:\n",
    "            tempid = gageid[hucinfo==ih].tolist()\n",
    "            tempregid = tempregid + tempid\n",
    "        regionID.append(tempregid)\n",
    "        regionNum.append(len(tempregid))\n",
    "\n",
    "    iexp = testfoldInd - 1  #index\n",
    "    TestLS = regionID[iexp] # basin ID list for testing, hold out for training\n",
    "    TestInd = [gageidLst.index(j) for j in TestLS]\n",
    "    TrainLS = list(set(gageid.tolist()) - set(TestLS)) # basin ID for training\n",
    "    TrainInd = [gageidLst.index(j) for j in TrainLS]\n",
    "    gageDic = {'TrainID': TrainLS, 'TestID': TestLS}\n",
    "\n",
    "\n",
    "# apply buffOPt to solve the warm-up for the first year\n",
    "if buffOpt ==2: # load more BUFFTIME data for the first year\n",
    "    sd = utils.time.t2dt(Ttrain[0]) - dt.timedelta(days=BUFFTIME)\n",
    "    sdint = int(sd.strftime(\"%Y%m%d\"))\n",
    "    TtrainLoad = [sdint, Ttrain[1]]\n",
    "    TinvLoad = [sdint, Ttrain[1]]\n",
    "else:\n",
    "    TtrainLoad = Ttrain\n",
    "    TinvLoad = Tinv\n",
    "\n",
    "## prepare input data\n",
    "## load camels dataset\n",
    "if forType == 'daymet':\n",
    "    varF = ['prcp', 'tmean']\n",
    "    varFInv = ['prcp', 'tmean']\n",
    "else:\n",
    "    varF = ['prcp', 'tmax'] # For CAMELS maurer and nldas forcings, tmax is actually tmean\n",
    "    varFInv = ['prcp', 'tmax']\n",
    "\n",
    "# the attributes used to learn parameters\n",
    "attrnewLst = [ 'p_mean','pet_mean','p_seasonality','frac_snow','aridity','high_prec_freq','high_prec_dur',\n",
    "               'low_prec_freq','low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
    "               'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
    "               'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
    "               'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
    "\n",
    "optData = default.optDataCamels # a default dictionary for logging, updated below\n",
    "# Update the training period and variables\n",
    "optData = default.update(optData, tRange=TtrainLoad, varT=varFInv, varC=attrnewLst, subset=TrainLS, forType=forType)\n",
    "\n",
    "# for HBV model training inputs\n",
    "dfTrain = camels.DataframeCamels(tRange=TtrainLoad, subset=TrainLS, forType=forType)\n",
    "forcUN = dfTrain.getDataTs(varLst=varF, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "obsUN = dfTrain.getDataObs(doNorm=False, rmNan=False, basinnorm=False, flow_regime=flow_regime)\n",
    "\n",
    "# for dPL inversion data, inputs of gA\n",
    "dfInv = camels.DataframeCamels(tRange=TinvLoad, subset=TrainLS, forType=forType)\n",
    "forcInvUN = dfInv.getDataTs(varLst=varFInv, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "attrsUN = dfInv.getDataConst(varLst=attrnewLst, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
    "\n",
    "# Unit transformation, discharge obs from ft3/s to mm/day\n",
    "areas = gageinfo['area'][TrainInd] # unit km2\n",
    "temparea = np.tile(areas[:, None, None], (1, obsUN.shape[1],1))\n",
    "obsUN = (obsUN * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10**3 # transform to mm/day\n",
    "\n",
    "# load potential ET calculated by hargreaves method\n",
    "varLstNL = ['PEVAP']\n",
    "usgsIdLst = gageid\n",
    "if forType == 'maurer':\n",
    "    tPETRange = [19800101, 20090101]\n",
    "else:\n",
    "    tPETRange = [19800101, 20150101]\n",
    "tPETLst = utils.time.tRange2Array(tPETRange)\n",
    "\n",
    "# Modify this as the directory where you put PET\n",
    "PETDir = rootDatabase + '/pet_harg/' + forType + '/'\n",
    "ntime = len(tPETLst)\n",
    "PETfull = np.empty([len(usgsIdLst), ntime, len(varLstNL)])\n",
    "for k in range(len(usgsIdLst)):\n",
    "    dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], varLstNL, ntime)\n",
    "    PETfull[k, :, :] = dataTemp\n",
    "\n",
    "TtrainLst = utils.time.tRange2Array(TtrainLoad)\n",
    "TinvLst = utils.time.tRange2Array(TinvLoad)\n",
    "C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
    "PETUN = PETfull[:, ind2, :]\n",
    "PETUN = PETUN[TrainInd, :, :] # select basins\n",
    "C, ind1, ind2inv = np.intersect1d(TinvLst, tPETLst, return_indices=True)\n",
    "PETInvUN = PETfull[:, ind2inv, :]\n",
    "PETInvUN = PETInvUN[TrainInd, :, :]\n",
    "\n",
    "# process data, do normalization and remove nan\n",
    "series_inv = np.concatenate([forcInvUN, PETInvUN], axis=2)\n",
    "seriesvarLst = varFInv + ['pet']\n",
    "# calculate statistics for normalization and saved to a dictionary\n",
    "statDict = camels.getStatDic(attrLst=attrnewLst, attrdata=attrsUN, seriesLst=seriesvarLst, seriesdata=series_inv, flow_regime=flow_regime)\n",
    "# normalize data\n",
    "attr_norm = camels.transNormbyDic(attrsUN, attrnewLst, statDict, toNorm=True, flow_regime=flow_regime)\n",
    "attr_norm[np.isnan(attr_norm)] = 0.0\n",
    "series_norm = camels.transNormbyDic(series_inv, seriesvarLst, statDict, toNorm=True, flow_regime=flow_regime)\n",
    "series_norm[np.isnan(series_norm)] = 0.0\n",
    "\n",
    "# prepare the inputs\n",
    "zTrain = series_norm # used as the inputs for dPL inversion gA along with attributes\n",
    "xTrain = np.concatenate([forcUN, PETUN], axis=2) # used as HBV forcing\n",
    "xTrain[np.isnan(xTrain)] = 0.0\n",
    "\n",
    "if buffOpt == 1: # repeat the first year warm up the first year itself\n",
    "    zTrainIn = np.concatenate([zTrain[:,0:BUFFTIME,:], zTrain], axis=1)\n",
    "    xTrainIn = np.concatenate([xTrain[:,0:BUFFTIME,:], xTrain], axis=1) # repeat forcing to warm up the first year\n",
    "    yTrainIn = np.concatenate([obsUN[:,0:BUFFTIME,:], obsUN], axis=1)\n",
    "else: # no repeat, original data, the first year data would only be used as warmup for the next following year\n",
    "    zTrainIn = zTrain\n",
    "    xTrainIn = xTrain\n",
    "    yTrainIn = obsUN\n",
    "\n",
    "forcTuple = (xTrainIn, zTrainIn)\n",
    "attrs = attr_norm\n",
    "\n",
    "## Train the model\n",
    "# define loss function\n",
    "alpha = 0.25 # a weight for RMSE loss to balance low and peak flow\n",
    "optLoss = default.update(default.optLossComb, name='hydroDL.model.crit.RmseLossComb', weight=alpha)\n",
    "lossFun = crit.RmseLossComb(alpha=alpha)\n",
    "\n",
    "# define training options\n",
    "optTrain = default.update(default.optTrainCamels, miniBatch=[BATCH_SIZE, RHO], nEpoch=EPOCH, saveEpoch=saveEPOCH)\n",
    "# define output folder to save model results\n",
    "exp_name = 'CAMELSDemo'\n",
    "exp_disp = 'dPLHBV/' + puN + TDN + forType + '/BuffOpt'+str(buffOpt)+'/RMSE_para'+str(alpha)+'/' + str(randomseed) + \\\n",
    "           '/Fold' + str(testfoldInd)\n",
    "exp_info = 'T_'+str(Ttrain[0])+'_'+str(Ttrain[1])+'_BS_'+str(BATCH_SIZE)+'_HS_'+str(HIDDENSIZE)\\\n",
    "           +'_RHO_'+str(RHO)+'_NF_'+str(Nfea)+'_Buff_'+str(BUFFTIME)+'_Mul_'+str(Nmul)\n",
    "save_path = os.path.join(exp_name, exp_disp)\n",
    "out = os.path.join(rootOut, save_path, exp_info) # output folder to save results\n",
    "# define and load model\n",
    "Ninv = zTrain.shape[-1] + attrs.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing streamflow predictions.\n",
    "# Apply the standard log normalization to the streamflow predictions, same as used\n",
    "# for forcings and attributes.\n",
    "tmp = preds\n",
    "\n",
    "modelVarLst = list(models.keys())\n",
    "modelStatDict = camels.getStatDic(seriesLst=modelVarLst, seriesdata=tmp, flow_regime=flow_regime)\n",
    "sf_norm = camels.transNormbyDic(tmp, modelVarLst, modelStatDict, toNorm=True, flow_regime=flow_regime)\n",
    "\n",
    "forcTuple = (tmp, sf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "#########################\n",
    "#### DESTINATION: rnn.py\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from hydroDL.model.dropout import DropMask, createMask\n",
    "\n",
    "\n",
    "\n",
    "class CudnnLstm(nn.Module):\n",
    "    def __init__(self, *, inputSize, hiddenSize, dr=0.5, drMethod=\"drW\", gpu=0, seed=42):\n",
    "        super(CudnnLstm, self).__init__()\n",
    "        self.name = 'CudnnLstm'\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.dr = dr\n",
    "\n",
    "        self.w_ih = Parameter(torch.Tensor(hiddenSize * 4, inputSize))\n",
    "        self.w_hh = Parameter(torch.Tensor(hiddenSize * 4, hiddenSize))\n",
    "        self.b_ih = Parameter(torch.Tensor(hiddenSize * 4))\n",
    "        self.b_hh = Parameter(torch.Tensor(hiddenSize * 4))\n",
    "        self._all_weights = [['w_ih', 'w_hh', 'b_ih', 'b_hh']]\n",
    "        self.cuda()\n",
    "        self.seed = seed\n",
    "        self.is_legacy = True\n",
    "\n",
    "        self.reset_mask()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(CudnnLstm, self)._apply(fn)\n",
    "        return ret\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        super(CudnnLstm, self).__setstate__(d)\n",
    "        self.__dict__.setdefault(\"_data_ptrs\", [])\n",
    "        if \"all_weights\" in d:\n",
    "            self._all_weights = d[\"all_weights\"]\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        self._all_weights = [[\"w_ih\", \"w_hh\", \"b_ih\", \"b_hh\"]]\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.maskW_ih = createMask(self.w_ih, self.dr, self.seed)\n",
    "        self.maskW_hh = createMask(self.w_hh, self.dr, self.seed)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hiddenSize)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx=None, cx=None, doDropMC=False, dropoutFalse=False):\n",
    "        # dropoutFalse: it will ensure doDrop is false, unless doDropMC is true\n",
    "        if dropoutFalse and (not doDropMC):\n",
    "            doDrop = False\n",
    "        elif self.dr > 0 and (doDropMC is True or self.training is True):\n",
    "            doDrop = True\n",
    "        else:\n",
    "            doDrop = False\n",
    "\n",
    "        batchSize = input.size(1)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(1, batchSize, self.hiddenSize, requires_grad=False)\n",
    "        if cx is None:\n",
    "            cx = input.new_zeros(1, batchSize, self.hiddenSize, requires_grad=False)\n",
    "\n",
    "        # cuDNN backend - disabled flat weight\n",
    "        # handle = torch.backends.cudnn.get_handle()\n",
    "        if doDrop is True:\n",
    "            self.reset_mask()\n",
    "            weight = [\n",
    "                DropMask.apply(self.w_ih, self.maskW_ih, True),\n",
    "                DropMask.apply(self.w_hh, self.maskW_hh, True), self.b_ih,\n",
    "                self.b_hh\n",
    "            ]\n",
    "        else:\n",
    "            weight = [self.w_ih, self.w_hh, self.b_ih, self.b_hh]\n",
    "\n",
    "        # output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "        # input, weight, 4, None, hx, cx, torch.backends.cudnn.CUDNN_LSTM,\n",
    "        # self.hiddenSize, 1, False, 0, self.training, False, (), None)\n",
    "        if torch.__version__ < \"1.8\":\n",
    "            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "                input,\n",
    "                weight,\n",
    "                4,\n",
    "                None,\n",
    "                hx,\n",
    "                cx,\n",
    "                2,  # 2 means LSTM\n",
    "                self.hiddenSize,\n",
    "                1,\n",
    "                False,\n",
    "                0,\n",
    "                self.training,\n",
    "                False,\n",
    "                (),\n",
    "                None,\n",
    "            )\n",
    "        else:\n",
    "            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
    "                input,\n",
    "                weight,\n",
    "                4,\n",
    "                None,\n",
    "                hx,\n",
    "                cx,\n",
    "                2,  # 2 means LSTM\n",
    "                self.hiddenSize,\n",
    "                0,\n",
    "                1,\n",
    "                False,\n",
    "                0,\n",
    "                self.training,\n",
    "                False,\n",
    "                (),\n",
    "                None,\n",
    "            )\n",
    "        return output, (hy, cy)\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [\n",
    "            [getattr(self, weight) for weight in weights]\n",
    "            for weights in self._all_weights\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "class CudnnLstmModel(torch.nn.Module):\n",
    "    def __init__(self, *, nx, ny, hiddenSize, dr=0.5, warmUpDay=None):\n",
    "        super(CudnnLstmModel, self).__init__()\n",
    "        self.name = 'CudnnLstmModel'\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.ct = 0\n",
    "        self.nLayer = 1\n",
    "        self.linearIn = torch.nn.Linear(nx, hiddenSize)\n",
    "\n",
    "        self.lstm = CudnnLstm(\n",
    "            inputSize=hiddenSize, hiddenSize=hiddenSize, dr=dr\n",
    "        )\n",
    "        self.linearOut = torch.nn.Linear(hiddenSize, ny)\n",
    "        self.gpu = 1\n",
    "        self.is_legacy = True\n",
    "        # self.drtest = torch.nn.Dropout(p=0.4)\n",
    "        self.warmUpDay = warmUpDay\n",
    "\n",
    "    def forward(self, x, doDropMC=False, dropoutFalse=False):\n",
    "        \"\"\"\n",
    "        :param inputs: a dictionary of input data (x and potentially z data)\n",
    "        :param doDropMC:\n",
    "        :param dropoutFalse:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # if not self.warmUpDay is None:\n",
    "        #     x, warmUpDay = self.extend_day(x, warm_up_day=self.warmUpDay)\n",
    "\n",
    "        x0 = F.relu(self.linearIn(x))\n",
    "\n",
    "        outLSTM, (hn, cn) = self.lstm(\n",
    "            x0, doDropMC=doDropMC, dropoutFalse=dropoutFalse\n",
    "        )\n",
    "        # outLSTMdr = self.drtest(outLSTM)\n",
    "        out = self.linearOut(outLSTM)\n",
    "\n",
    "        # if not self.warmUpDay is None:\n",
    "        #     out = self.reduce_day(out, warm_up_day=self.warmUpDay)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def extend_day(self, x, warm_up_day):\n",
    "        x_num_day = x.shape[0]\n",
    "        warm_up_day = min(x_num_day, warm_up_day)\n",
    "        x_select = x[:warm_up_day, :, :]\n",
    "        x = torch.cat([x_select, x], dim=0)\n",
    "        return x, warm_up_day\n",
    "\n",
    "    def reduce_day(self, x, warm_up_day):\n",
    "        x = x[warm_up_day:,:,:]\n",
    "        return x\n",
    "\n",
    "\n",
    "class RangeBoundLoss(nn.Module):\n",
    "    \"\"\"limit parameters from going out of range\"\"\"\n",
    "    def __init__(self, lb, ub):\n",
    "        super(RangeBoundLoss, self).__init__()\n",
    "        self.lb = torch.tensor(lb).cuda()\n",
    "        self.ub = torch.tensor(ub).cuda()\n",
    "        # self.factor = torch.tensor(factor).cuda()\n",
    "\n",
    "    def forward(self, params, factor):\n",
    "        factor = torch.tensor(factor).cuda()\n",
    "        loss = 0\n",
    "        for i in range(len(params)):\n",
    "            lb = self.lb[i]\n",
    "            ub = self.ub[i]\n",
    "            upper_bound_loss = factor * torch.relu(params[i] - ub).mean()\n",
    "            lower_bound_loss = factor * torch.relu(lb - params[i]).mean()\n",
    "            loss = loss + upper_bound_loss + lower_bound_loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently training LSTM on streamflow preds from the hydro models.\n",
    "# what we really want is to make EnsembleWeights use the forcings and attributes\n",
    "# just as done precipitatin fusion. This means most of the structure can be\n",
    "# pulled from that code.\n",
    "\n",
    "\n",
    "# Wrapper:\n",
    "##########################\n",
    "#### DESTINATION: rnn.py\n",
    "# Modified prcp_weights\n",
    "\n",
    "class EnsembleWeights(torch.nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, nmodels=1):\n",
    "        super(EnsembleWeights, self).__init__()\n",
    "        self.name = 'EnsembleWeights'\n",
    "        self.ninv = ninv\n",
    "        self.nmodels = nmodels\n",
    "\n",
    "        ntp = nmodels\n",
    "        self.hiddeninv = hiddeninv\n",
    "\n",
    "        self.lstminv = CudnnLstmModel(\n",
    "            nx=ninv, ny=ntp, hiddenSize=hiddeninv, dr=drinv)\n",
    "        lb_prcp = [0.95]\n",
    "        ub_prcp = [1.05]\n",
    "        self.RangeBoundLoss = RangeBoundLoss(lb=lb_prcp, ub=ub_prcp)\n",
    "\n",
    "    def forward(self, x, z, prcp_loss_factor):\n",
    "        z.requires_grad = True\n",
    "\n",
    "        wghts = self.lstminv(z)\n",
    "        ntstep = wghts.shape[0]\n",
    "        ngage = wghts.shape[1]\n",
    "        wghts_scaled = torch.sigmoid(wghts)  # Change weighting method here.\n",
    "\n",
    "        prcp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        prcp_wghts_sum = torch.sum(wghts_scaled, dim=2)\n",
    "        range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)\n",
    "\n",
    "        # print(prcp_wavg.shape, wghts_scaled.shape, x.shape, z.shape)\n",
    "\n",
    "        for para in range(wghts.shape[2]):\n",
    "            prcp_wavg = prcp_wavg + wghts_scaled[:, :, para] * x[:, :, para]\n",
    "\n",
    "        x_new = torch.empty((ntstep, ngage, 3), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        # x_new[:, :, 0] = prcp_wavg\n",
    "        # x_new[:, :, 1] = x[:, :, self.nmodels]\n",
    "        # x_new[:, :, 2] = x[:, :, -1]\n",
    "\n",
    "        # For gradient analysis.\n",
    "        # grad_daymet = autograd.grad(outputs=wghts_scaled[:, :, 0], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 0]), retain_graph=True)[0]\n",
    "        # grad_maurer = autograd.grad(outputs=wghts_scaled[:, :, 1], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 1]), retain_graph=True)[0]\n",
    "        # grad_nldas = autograd.grad(outputs=wghts_scaled[:, :, 2], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 2]), retain_graph=True)[0]\n",
    "\n",
    "        # return x_new, range_bound_loss_prcp, wghts_scaled, grad_daymet, grad_maurer, grad_nldas\n",
    "        return x_new, range_bound_loss_prcp, wghts_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model dependencies:\n",
    "##########################\n",
    "#### DESTINATION: train.py\n",
    "\n",
    "import time\n",
    "from hydroDL.model import rnn, cnn, crit\n",
    "\n",
    "\n",
    "\n",
    "def randomIndex(ngrid, nt, dimSubset, bufftime=0):\n",
    "    batchSize, rho = dimSubset\n",
    "    iGrid = np.random.randint(0, ngrid, [batchSize])\n",
    "    iT = np.random.randint(0 + bufftime, nt - rho, [batchSize])\n",
    "    return iGrid, iT\n",
    "\n",
    "\n",
    "\n",
    "def selectSubset(x, iGrid, iT, rho, *, c=None, tupleOut=False, LCopt=False, bufftime=0):\n",
    "    nx = x.shape[-1]\n",
    "    nt = x.shape[1]\n",
    "    if x.shape[0] == len(iGrid):   #hack\n",
    "        iGrid = np.arange(0,len(iGrid))  # hack\n",
    "    if nt <= rho:\n",
    "        iT.fill(0)\n",
    "\n",
    "    batchSize = iGrid.shape[0]\n",
    "    if iT is not None:\n",
    "        # batchSize = iGrid.shape[0]\n",
    "        xTensor = torch.zeros([rho+bufftime, batchSize, nx], requires_grad=False)\n",
    "        for k in range(batchSize):\n",
    "            temp = x[iGrid[k]:iGrid[k] + 1, np.arange(iT[k]-bufftime, iT[k] + rho), :]\n",
    "            xTensor[:, k:k + 1, :] = torch.from_numpy(np.swapaxes(temp, 1, 0))\n",
    "    else:\n",
    "        if LCopt is True:\n",
    "            # used for local calibration kernel: FDC, SMAP...\n",
    "            if len(x.shape) == 2:\n",
    "                # Used for local calibration kernel as FDC\n",
    "                # x = Ngrid * Ntime\n",
    "                xTensor = torch.from_numpy(x[iGrid, :]).float()\n",
    "            elif len(x.shape) == 3:\n",
    "                # used for LC-SMAP x=Ngrid*Ntime*Nvar\n",
    "                xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 2)).float()\n",
    "        else:\n",
    "            # Used for rho equal to the whole length of time series\n",
    "            xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 0)).float()\n",
    "            rho = xTensor.shape[0]\n",
    "    if c is not None:\n",
    "        nc = c.shape[-1]\n",
    "        temp = np.repeat(\n",
    "            np.reshape(c[iGrid, :], [batchSize, 1, nc]), rho+bufftime, axis=1)\n",
    "        cTensor = torch.from_numpy(np.swapaxes(temp, 1, 0)).float()\n",
    "\n",
    "        if (tupleOut):\n",
    "            if torch.cuda.is_available():\n",
    "                xTensor = xTensor.cuda()\n",
    "                cTensor = cTensor.cuda()\n",
    "            out = (xTensor, cTensor)\n",
    "        else:\n",
    "            out = torch.cat((xTensor, cTensor), 2)\n",
    "    else:\n",
    "        out = xTensor\n",
    "\n",
    "    if torch.cuda.is_available() and type(out) is not tuple:\n",
    "        out = out.cuda()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import ValuesView\n",
    "from tqdm import tqdm\n",
    "from hydroDL.master.master import loadModel\n",
    "\n",
    "\n",
    "\n",
    "def trainEnsemble(model,\n",
    "               x,\n",
    "               y,\n",
    "               c,\n",
    "               lossFun,\n",
    "               *,\n",
    "               nEpoch=500,\n",
    "               startEpoch=1,\n",
    "               miniBatch=[100, 30],\n",
    "               saveEpoch=100,\n",
    "               saveFolder=None,\n",
    "               mode='seq2seq',\n",
    "               bufftime=0,\n",
    "               prcp_loss_factor = 15,\n",
    "               smooth_loss_factor = 0,\n",
    "               ):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x - input\n",
    "    z - normalized x input\n",
    "    y - target, or observed values\n",
    "    c - constant input, attributes\n",
    "    \"\"\"\n",
    "\n",
    "    batchSize, rho = miniBatch\n",
    "    if type(x) is tuple or type(x) is list:\n",
    "        x, z = x\n",
    "\n",
    "    ngrid, nt, nx = x.shape  # ngrid= # basins, nt= # timesteps, nx= # attributes\n",
    "\n",
    "    if c is not None:\n",
    "        nx = nx + c.shape[-1]\n",
    "    if batchSize >= ngrid:\n",
    "        # Cannot have more batches than there are basins.\n",
    "        batchSize = ngrid\n",
    "\n",
    "    nIterEp = int(\n",
    "        np.ceil(np.log(0.01) / np.log(1 - batchSize * rho / ngrid / (nt-bufftime)))\n",
    "        )\n",
    "    if hasattr(model, 'ctRm'):\n",
    "        if model.ctRm is True:\n",
    "            nIterEp = int(\n",
    "                np.ceil(\n",
    "                    np.log(0.01) / np.log(1 - batchSize *\n",
    "                                          (rho - model.ct) / ngrid / (nt-bufftime))))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        lossFun = lossFun.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    optim = torch.optim.Adadelta(list(model.parameters()))\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Save file.\n",
    "    if saveFolder is not None:\n",
    "        os.makedirs(saveFolder, exist_ok=True)\n",
    "        runFile = os.path.join(saveFolder, 'run.csv')\n",
    "        rf = open(runFile, 'w+')\n",
    "\n",
    "    for iEpoch in range(startEpoch, nEpoch + 1):\n",
    "        lossEp = 0\n",
    "        loss_prcp_Ep = 0\n",
    "        loss_sf_Ep = 0\n",
    "        # loss_smooth_Ep = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        prog_str = \"Epoch \" + str(iEpoch) + \"/\" + str(nEpoch)\n",
    "\n",
    "        for iIter in tqdm(range(0, nIterEp), desc=prog_str, leave=False):\n",
    "            # training iterations\n",
    "            if type(model) in [EnsembleWeights]:\n",
    "                iGrid, iT = randomIndex(ngrid, nt, [batchSize, rho], bufftime=bufftime)\n",
    "\n",
    "                xTrain = selectSubset(x, iGrid, iT, rho, bufftime=bufftime)\n",
    "                yTrain = selectSubset(y, iGrid, iT, rho)\n",
    "                # zTrain = selectSubset(z, iGrid, iT, rho, c=c, bufftime=bufftime)\n",
    "                zTrain = selectSubset(z, iGrid, iT, rho, bufftime=bufftime)\n",
    "\n",
    "                # calculate loss and weights `wt`.\n",
    "                xP, prcp_loss, prcp_weights = model(xTrain, zTrain, prcp_loss_factor)\n",
    "                yP = torch.sum(xTrain * prcp_weights, dim=2).unsqueeze(2)\n",
    "\n",
    "            else:\n",
    "                Exception('unknown model')\n",
    "\n",
    "            # Consider the buff time for initialization.\n",
    "            if bufftime > 0:\n",
    "                yP = yP[bufftime:,:,:]\n",
    "\n",
    "            # get loss\n",
    "            if type(lossFun) in [crit.NSELossBatch, crit.NSESqrtLossBatch]:\n",
    "                loss_sf = lossFun(yP, yTrain, iGrid)\n",
    "                loss =  loss_sf + prcp_loss\n",
    "            else:\n",
    "                loss_sf = lossFun(yP, yTrain)\n",
    "                loss = loss_sf + prcp_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            lossEp = lossEp + loss.item()\n",
    "\n",
    "            try:\n",
    "                loss_prcp_Ep = loss_prcp_Ep + prcp_loss.item()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            loss_sf_Ep = loss_sf_Ep + loss_sf.item()\n",
    "\n",
    "            # if iIter % 100 == 0:\n",
    "            #     print('Iter {} of {}: Loss {:.3f}'.format(iIter, nIterEp, loss.item()))\n",
    "\n",
    "        # print loss\n",
    "        lossEp = lossEp / nIterEp\n",
    "        loss_sf_Ep = loss_sf_Ep / nIterEp\n",
    "        loss_prcp_Ep = loss_prcp_Ep / nIterEp\n",
    "\n",
    "        logStr = 'Epoch {} Loss {:.3f}, Streamflow Loss {:.3f}, Precipitation Loss {:.3f}, time {:.2f}'.format(\n",
    "            iEpoch, lossEp, loss_sf_Ep, loss_prcp_Ep,\n",
    "            time.time() - t0)\n",
    "        print(logStr)\n",
    "\n",
    "        # Save model and loss.\n",
    "        if saveFolder is not None:\n",
    "            rf.write(logStr + '\\n')\n",
    "            if iEpoch % saveEpoch == 0:\n",
    "                # save model\n",
    "                modelFile = os.path.join(saveFolder,\n",
    "                                         'model_Ep' + str(iEpoch) + '.pt')\n",
    "                torch.save(model, modelFile)\n",
    "\n",
    "    if saveFolder is not None:\n",
    "        rf.close()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "BATCHSIZE = 100\n",
    "RHO = 365\n",
    "saveEPOCH = 10\n",
    "BUFFTIME = 365\n",
    "LOSS_FACTOR = 23\n",
    "SMOOTH_LOSS_FACTOR = 0\n",
    "ALPHA = 0.25  # A weight for RMSE loss to balance low and peak flow.\n",
    "LOSSFUNC = crit.RmseLossComb(alpha=ALPHA)\n",
    "HIDDENSIZE = 256\n",
    "\n",
    "# model = EnsembleWeights(ninv=3, hiddeninv=HIDDENSIZE, nmodels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting save path for model.\n",
    "rootdir = '/content/drive/MyDrive/Colab/data/model_runs/hydro_multimodel_results/multimodels/671_sites_dp/mm_ensemble_sigmoid/'\n",
    "save_path = rootdir + 'HBV_SAC_wSnow_PRMS_v2' + '_E' + str(EPOCH) + '_B' + str(BATCHSIZE) + '_R' + str(RHO) + '_BT' + str(BUFFTIME) + '_L' + str(LOSS_FACTOR) + '_SmL' + str(SMOOTH_LOSS_FACTOR) + '_H' + str(HIDDENSIZE)\n",
    "\n",
    "# Load previous model to continue training:\n",
    "train_epoch = 0\n",
    "# model = loadModel(save_path, epoch=train_epoch)\n",
    "\n",
    "model = trainEnsemble(model,\n",
    "                      forcTuple,\n",
    "                      comp_obs[:,:, np.newaxis],\n",
    "                      attrs,\n",
    "                      LOSSFUNC,\n",
    "                      nEpoch=EPOCH,\n",
    "                      startEpoch = train_epoch+1,\n",
    "                      miniBatch=[BATCH_SIZE, RHO],\n",
    "                      saveEpoch=saveEPOCH,\n",
    "                      saveFolder=save_path,\n",
    "                      mode='seq2seq',\n",
    "                      bufftime=BUFFTIME,\n",
    "                      prcp_loss_factor = LOSS_FACTOR,\n",
    "                      smooth_loss_factor = SMOOTH_LOSS_FACTOR\n",
    "                      )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGML_STemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
