{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(2)\n",
    "b = np.arange(8)\n",
    "\n",
    "a = a.reshape((2,2,2))\n",
    "b = b.reshape((2,2,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 1],\n",
       "         [2, 3]],\n",
       " \n",
       "        [[4, 5],\n",
       "         [6, 7]]]),\n",
       " array([[[0, 1],\n",
       "         [2, 3]],\n",
       " \n",
       "        [[4, 5],\n",
       "         [6, 7]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5, 2.5],\n",
       "        [4.5, 6.5]],\n",
       "\n",
       "       [[0.5, 2.5],\n",
       "        [4.5, 6.5]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([a,b],axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_PRMS_SNTEMP as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def m_softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00626879, 0.01704033, 0.04632042, 0.93037047]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([[1, 2, 3, 6]])\n",
    "m_softmax(x1) #, m_softmax(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[1, 2, 3, 6]])\n",
    "\n",
    "m = m_softmax(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = []\n",
    "for i in range(len(x1)):\n",
    "    n.append(x1[i]*m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.00626879, 0.03408066, 0.13896125, 5.58222279])], 5.761533493695909)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n, np.sum(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataTs(self, args, varLst, doNorm=True, rmNan=True):\n",
    "        if type(varLst) is str:\n",
    "            varLst = [varLst]\n",
    "        inputfile = os.path.join(os.path.realpath(args[\"forcing_path\"]))\n",
    "        inputfile_attr = os.path.join(os.path.realpath(args[\"attr_path\"]))\n",
    "        if inputfile.endswith(\".csv\"):\n",
    "            dfMain = pd.read_csv(inputfile)\n",
    "            dfMain_attr = pd.read_csv(inputfile_attr)\n",
    "        elif inputfile.endswith(\".feather\"):\n",
    "            dfMain = pd.read_feather(inputfile)\n",
    "            dfMain_attr = pd.read_feather(inputfile_attr)\n",
    "        else:\n",
    "            print(\"data type is not supported\")\n",
    "            exit()\n",
    "        sites = dfMain[\"site_no\"].unique()\n",
    "        tLst = tRange2Array(args[\"tRange\"])\n",
    "        tLstobs = tRange2Array(args[\"tRange\"])\n",
    "        # nt = len(tLst)\n",
    "        ntobs = len(tLstobs)\n",
    "        nNodes = len(sites)\n",
    "\n",
    "        varLst_forcing = []\n",
    "        varLst_attr = []\n",
    "        for var in varLst:\n",
    "            if var in dfMain.columns:\n",
    "                varLst_forcing.append(var)\n",
    "            elif var in dfMain_attr.columns:\n",
    "                varLst_attr.append(var)\n",
    "            else:\n",
    "                print(var, \"the var is not in forcing file nor in attr file\")\n",
    "        xt = dfMain.loc[:, varLst_forcing].values\n",
    "        g = dfMain.reset_index(drop=True).groupby(\"site_no\")\n",
    "        xtg = [xt[i.values, :] for k, i in g.groups.items()]\n",
    "        x = np.array(xtg)\n",
    "\n",
    "        ## for attr\n",
    "        if len(varLst_attr) > 0:\n",
    "            x_attr_t = dfMain_attr.loc[:, varLst_attr].values\n",
    "            x_attr_t = np.expand_dims(x_attr_t, axis=2)\n",
    "            xattr = np.repeat(x_attr_t, x.shape[1], axis=2)\n",
    "            xattr = np.transpose(xattr, (0, 2, 1))\n",
    "            x = np.concatenate((x, xattr), axis=2)\n",
    "\n",
    "        data = x\n",
    "        C, ind1, ind2 = np.intersect1d(self.time, tLst, return_indices=True)\n",
    "        data = data[:, ind2, :]\n",
    "        # if os.path.isdir(out):\n",
    "        #     pass\n",
    "        # else:\n",
    "        #     os.makedirs(out)\n",
    "        # np.save(os.path.join(out, 'x.npy'), data)\n",
    "        # if doNorm is True:\n",
    "        #     data = transNorm(data, varLst, toNorm=True)\n",
    "        # if rmNan is True:\n",
    "        #     data[np.where(np.isnan(data))] = 0\n",
    "        return np.swapaxes(data, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "def loadData(args, trange):\n",
    "    out_dict = dict()\n",
    "    df = DataFrame_dataset(tRange=trange)\n",
    "    # getting inputs for NN model:\n",
    "    out_dict[\"x_NN\"] = df.getDataTs(args, varLst=args[\"varT_NN\"])\n",
    "    out_dict[\"c_NN\"] = df.getDataConst(args, varLst=args[\"varC_NN\"])\n",
    "    out_dict[\"obs\"] = df.getDataTs(args, varLst=args[\"target\"])\n",
    "    if args[\"hydro_model_name\"] != \"None\":\n",
    "        out_dict[\"x_hydro_model\"] = df.getDataTs(args, varLst=args[\"varT_hydro_model\"])\n",
    "        out_dict[\"c_hydro_model\"] = df.getDataConst(args, varLst=args[\"varC_hydro_model\"])\n",
    "    if args[\"temp_model_name\"] != \"None\":\n",
    "        out_dict[\"x_temp_model\"] = df.getDataTs(args, varLst=args[\"varT_temp_model\"])\n",
    "        out_dict[\"c_temp_model\"] = df.getDataConst(args, varLst=args[\"varC_temp_model\"])\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Walking Through PRMS SAC-SMA testing in PGML here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_sacsma as args\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import loadModel, create_output_dirs\n",
    "from MODELS.loss_functions.get_loss_function import get_lossFun\n",
    "from MODELS.train_test import test_differentiable_model\n",
    "from core.data_processing.data_loading import loadData\n",
    "\n",
    "\n",
    "from core.utils import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomseed_config(seed=args[\"randomseed\"][0])\n",
    "# Creating output directories and adding it to args\n",
    "args = create_output_dirs(args)\n",
    "\n",
    "lossFun = get_lossFun(args)\n",
    "\n",
    "modelFile = os.path.join(args[\"out_dir\"], \"model_Ep\" + str(args[\"EPOCHS\"]) + \".pt\")\n",
    "diff_model = torch.load(modelFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_hydro_temp_model(\n",
       "  (hydro_model): SACSMAMul()\n",
       "  (NN_model): CudnnLstmModel(\n",
       "    (linearIn): Linear(in_features=37, out_features=256, bias=True)\n",
       "    (lstm): CudnnLstm()\n",
       "    (linearOut): Linear(in_features=256, out_features=24, bias=True)\n",
       "    (activation_sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up = args[\"warm_up\"]\n",
    "nmul = args[\"nmul\"]\n",
    "diff_model.eval()\n",
    "# read data for test time range\n",
    "dataset_dictionary = loadData(args, trange=args[\"t_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4.80e+01],\n",
       "        [3.90e+01],\n",
       "        [3.60e+01],\n",
       "        ...,\n",
       "        [4.10e+01],\n",
       "        [5.20e-01],\n",
       "        [1.14e+02]],\n",
       "\n",
       "       [[4.60e+01],\n",
       "        [3.90e+01],\n",
       "        [3.60e+01],\n",
       "        ...,\n",
       "        [3.60e+01],\n",
       "        [4.60e-01],\n",
       "        [1.03e+02]],\n",
       "\n",
       "       [[4.30e+01],\n",
       "        [3.90e+01],\n",
       "        [3.50e+01],\n",
       "        ...,\n",
       "        [3.40e+01],\n",
       "        [4.40e-01],\n",
       "        [9.80e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[6.39e+02],\n",
       "        [6.30e+01],\n",
       "        [5.75e+02],\n",
       "        ...,\n",
       "        [3.50e+01],\n",
       "        [9.00e-02],\n",
       "        [1.24e+02]],\n",
       "\n",
       "       [[7.52e+02],\n",
       "        [7.20e+01],\n",
       "        [5.99e+02],\n",
       "        ...,\n",
       "        [3.40e+01],\n",
       "        [9.00e-02],\n",
       "        [1.19e+02]],\n",
       "\n",
       "       [[8.61e+02],\n",
       "        [7.50e+01],\n",
       "        [6.23e+02],\n",
       "        ...,\n",
       "        [3.40e+01],\n",
       "        [8.00e-02],\n",
       "        [1.15e+02]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary['obs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_processing.normalization import transNorm\n",
    "\n",
    "x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n",
    "c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n",
    "c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n",
    "dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n",
    "del x_NN_scaled, dataset_dictionary[\"x_NN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_dictionary.keys():\n",
    "    dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n",
    "\n",
    "args[\"batch_size\"] = args[\"no_basins\"]\n",
    "nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n",
    "rho = args[\"rho\"]\n",
    "\n",
    "batch_size = args[\"batch_size\"]\n",
    "iS = np.arange(0, ngrid, batch_size)\n",
    "iE = np.append(iS[1:], ngrid)\n",
    "list_out_diff_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]d:\\code_repos\\hydro_ensemble\\MODELS\\NN_models\\LSTM_models.py:99: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
      "  7%|▋         | 2/27 [00:28<05:55, 14.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(iS))):\n\u001b[0;32m     11\u001b[0m     dataset_dictionary_sample \u001b[38;5;241m=\u001b[39m take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n\u001b[1;32m---> 12\u001b[0m     out_diff_model \u001b[38;5;241m=\u001b[39m \u001b[43mdiff_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dictionary_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     out_diff_model_cpu \u001b[38;5;241m=\u001b[39m {key: tensor\u001b[38;5;241m.\u001b[39mcpu\n\u001b[0;32m     14\u001b[0m     ()\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m out_diff_model\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\code_repos\\hydro_ensemble\\MODELS\\Differentiable_models.py:141\u001b[0m, in \u001b[0;36mdiff_hydro_temp_model.forward\u001b[1;34m(self, dataset_dictionary_sample)\u001b[0m\n\u001b[0;32m    138\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreakdown_params(params_all)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhydro_model_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# hydro model\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     flow_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydro_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dictionary_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_hydro_model_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dictionary_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc_hydro_model_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhydro_params_raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# PET_param=params_dict[\"params_PET_model\"],  # PET is in both temp and flow model\u001b[39;49;00m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarm_up\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouting_hydro_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_params_hydro\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv_params_hydro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# Todo: send this to  a function\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# source flow calculation and converting mm/day to m3/ day\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     srflow, ssflow, gwflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhydro_model\u001b[38;5;241m.\u001b[39msource_flow_calculation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, flow_out,\n\u001b[0;32m    155\u001b[0m                                                                       dataset_dictionary_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_NN_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\code_repos\\hydro_ensemble\\MODELS\\hydro_models\\SACSMA\\SACSMAmul.py:384\u001b[0m, in \u001b[0;36mSACSMAMul.forward\u001b[1;34m(self, x_hydro_model, c_hydro_model, SACSMA_params_raw, args, muwts, warm_up, init, routing, comprout, conv_params_hydro)\u001b[0m\n\u001b[0;32m    382\u001b[0m flux_twexlp \u001b[38;5;241m=\u001b[39m ((lzfwpm \u001b[38;5;241m-\u001b[39m LZFWP_storage) \u001b[38;5;241m/\u001b[39m (lzfwpm \u001b[38;5;241m*\u001b[39m ((lzfwpm \u001b[38;5;241m-\u001b[39m LZFWP_storage) \u001b[38;5;241m/\u001b[39m lzfwpm) \u001b[38;5;241m+\u001b[39m ((lzfwsm \u001b[38;5;241m-\u001b[39m LZFWS_storage) \u001b[38;5;241m/\u001b[39m lzfwsm))) \u001b[38;5;241m*\u001b[39m flux_twexl\n\u001b[0;32m    383\u001b[0m LZFWP_storage \u001b[38;5;241m=\u001b[39m LZFWP_storage \u001b[38;5;241m+\u001b[39m flux_Pcfwp \u001b[38;5;241m+\u001b[39m flux_twexlp\n\u001b[1;32m--> 384\u001b[0m extra_LZFWP \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLZFWP_storage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlzfwpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m LZFWP_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(LZFWP_storage \u001b[38;5;241m-\u001b[39m extra_LZFWP, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)  \u001b[38;5;66;03m# I added this to make the storage not to exceed the max\u001b[39;00m\n\u001b[0;32m    386\u001b[0m flux_Qbfp \u001b[38;5;241m=\u001b[39m params_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklzp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m LZFWP_storage\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from core.data_processing.model import (\n",
    "    No_iter_nt_ngrid,\n",
    "    take_sample_train,\n",
    "    take_sample_test,\n",
    "    converting_flow_from_ft3_per_sec_to_mm_per_day\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(iS))):\n",
    "    dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n",
    "    out_diff_model = diff_model(dataset_dictionary_sample)\n",
    "    out_diff_model_cpu = {key: tensor.cpu\n",
    "    ().detach() for key, tensor in out_diff_model.items()}\n",
    "    # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n",
    "    list_out_diff_model.append(out_diff_model_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['c_NN', 'obs', 'x_hydro_model', 'c_hydro_model', 'inputs_NN_scaled']),\n",
       " dict_keys(['c_NN_sample', 'obs_sample', 'x_hydro_model_sample', 'c_hydro_model_sample', 'inputs_NN_scaled_sample']),\n",
       " dict_keys(['flow_sim', 'srflow', 'ssflow', 'gwflow', 'PET_hydro', 'AET_hydro', 'BFI_sim']))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary.keys(), dataset_dictionary_sample.keys(), out_diff_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5479"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dictionary['inputs_NN_scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args, dataset_dictionary[\"c_NN\"],\n",
    "                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6701],\n",
       "         [0.9128],\n",
       "         [0.3254],\n",
       "         ...,\n",
       "         [0.1160],\n",
       "         [0.0201],\n",
       "         [0.2646]],\n",
       "\n",
       "        [[0.6353],\n",
       "         [0.8275],\n",
       "         [0.3527],\n",
       "         ...,\n",
       "         [0.1160],\n",
       "         [0.0213],\n",
       "         [0.2646]],\n",
       "\n",
       "        [[0.6245],\n",
       "         [0.7507],\n",
       "         [0.3620],\n",
       "         ...,\n",
       "         [0.1160],\n",
       "         [0.0224],\n",
       "         [0.2646]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6940],\n",
       "         [0.2687],\n",
       "         [0.3827],\n",
       "         ...,\n",
       "         [0.1933],\n",
       "         [0.0053],\n",
       "         [0.4318]],\n",
       "\n",
       "        [[0.8167],\n",
       "         [0.3071],\n",
       "         [0.3986],\n",
       "         ...,\n",
       "         [0.1877],\n",
       "         [0.0053],\n",
       "         [0.4144]],\n",
       "\n",
       "        [[0.9351],\n",
       "         [0.3199],\n",
       "         [0.4146],\n",
       "         ...,\n",
       "         [0.1877],\n",
       "         [0.0047],\n",
       "         [0.4004]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differentiable_model(args, diff_model):\n",
    "    warm_up = args[\"warm_up\"]\n",
    "    nmul = args[\"nmul\"]\n",
    "    diff_model.eval()\n",
    "    # read data for test time range\n",
    "    dataset_dictionary = loadData(args, trange=args[\"t_test\"])\n",
    "    np.save(os.path.join(args[\"out_dir\"], \"x.npy\"), dataset_dictionary[\"x_NN\"])  # saves with the overlap in the beginning\n",
    "    # normalizing\n",
    "    x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n",
    "    c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n",
    "    c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n",
    "    dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n",
    "    del x_NN_scaled, dataset_dictionary[\"x_NN\"]\n",
    "    # converting the numpy arrays to torch tensors:\n",
    "    for key in dataset_dictionary.keys():\n",
    "        dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n",
    "\n",
    "    # args_mod = args.copy()\n",
    "    args[\"batch_size\"] = args[\"no_basins\"]\n",
    "    nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n",
    "    rho = args[\"rho\"]\n",
    "\n",
    "    # Making lists of the start and end indices of the basins for each batch.\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    iS = np.arange(0, ngrid, batch_size)    # Start index list.\n",
    "    iE = np.append(iS[1:], ngrid)   # End.\n",
    "    \n",
    "    list_out_diff_model = []\n",
    "    for i in tqdm(range(0, len(iS)), unit='Batch'):\n",
    "        dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n",
    "\n",
    "        out_diff_model = diff_model(dataset_dictionary_sample)\n",
    "        # Convert all tensors in the dictionary to CPU\n",
    "        out_diff_model_cpu = {key: tensor.cpu().detach() for key, tensor in out_diff_model.items()}\n",
    "        # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n",
    "        list_out_diff_model.append(out_diff_model_cpu)\n",
    "\n",
    "    # getting rid of warm-up period in observation dataset and making the dimension similar to\n",
    "    # converting numpy to tensor\n",
    "    # y_obs = torch.tensor(np.swapaxes(y_obs[:, warm_up:, :], 0, 1), dtype=torch.float32)\n",
    "    # c_hydro_model = torch.tensor(c_hydro_model, dtype=torch.float32)\n",
    "    y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args, dataset_dictionary[\"c_NN\"],\n",
    "                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])\n",
    "\n",
    "    return list_out_diff_model, y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5479"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dictionary['x_NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Walking Through HBV testing in dPLHBV here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "## fix the random seeds\n",
    "import torch\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from hydroDL import master, utils\n",
    "\n",
    "\n",
    "## GPU setting\n",
    "testgpuid = 0\n",
    "torch.cuda.set_device(testgpuid)\n",
    "\n",
    "## setting options, keep the same as your training\n",
    "PUOpt = 0  # 0 for All; 1 for PUB; 2 for PUR;\n",
    "buffOptOri = 0  # original buffOpt, must be same as what you set for training\n",
    "buffOpt = 0  # control load training data 0: do nothing; 1: repeat first year; 2: load one more year\n",
    "forType = 'daymet'\n",
    "\n",
    "## Hyperparameters, keep the same as your training setup\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "Ttrain = [19801001, 19951001]  # Training period\n",
    "# Ttrain = [19891001, 19991001]  # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # dPL Inversion period\n",
    "# Tinv = [19891001, 19991001]  # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters\n",
    "BUFFTIME = 365\n",
    "routing = True\n",
    "Nmul = 16\n",
    "comprout = False\n",
    "compwts = False\n",
    "pcorr = None\n",
    "\n",
    "Ttest = [19951001, 20101001]  # testing period\n",
    "TtestLst = utils.time.tRange2Array(Ttest)\n",
    "TtestLoad = [19951001, 20101001]  \n",
    "\n",
    "testbatch = 50  # forward number of \"testbatch\" basins each time to save GPU memory. You can set this even smaller to save more.\n",
    "testepoch = 50\n",
    "\n",
    "testseed = 111111\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'camels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define root directory of database and saved output dir\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Modify this based on your own location of CAMELS dataset and saved models\u001b[39;00m\n\u001b[0;32m      3\u001b[0m rootDatabase \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCamels\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# CAMELS dataset root directory\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcamels\u001b[49m\u001b[38;5;241m.\u001b[39minitcamels(rootDatabase)  \u001b[38;5;66;03m# initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'camels' is not defined"
     ]
    }
   ],
   "source": [
    "# Define root directory of database and saved output dir\n",
    "# Modify this based on your own location of CAMELS dataset and saved models\n",
    "rootDatabase = os.path.join(os.path.sep, 'D:\\data', 'Camels')  # CAMELS dataset root directory\n",
    "camels.initcamels(rootDatabase)  # initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From testMulti.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model\n",
    "# lstm = hydroEnsemble(num_models=3, hidden_size=256, num_layers=1)\n",
    "\n",
    "\n",
    "# # Create a sample input tensor\n",
    "# batch_size = 1\n",
    "# sequence_length = 671\n",
    "# num_models = 3\n",
    "# input_tensor = torch.randn(sequence_length, num_models)\n",
    "# input_array = np.random.rand(sequence_length, num_models)\n",
    "\n",
    "# # Forward pass through the model\n",
    "# preds= lstm(torch.tensor(input_tensor, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_output['marrmot_PRMS'][0].keys():\n",
    "    if len(model_output['marrmot_PRMS'][0][key].shape) == 3:\n",
    "        dim = 1\n",
    "    else:\n",
    "        dim = 0\n",
    "    concatenated_tensor = torch.cat([d[key] for d in model_output], dim=dim)\n",
    "    file_name = key + \".npy\"\n",
    "    np.save(os.path.join(SAVE_PATH, args[\"testing_dir\"], file_name), concatenated_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([[1, 2],\n",
    "                   [3, 4],\n",
    "                   [5, 6]])\n",
    "\n",
    "# Create a 1x3 array\n",
    "a2 = np.array([7, 8, 9])\n",
    "\n",
    "a = np.stack((a2, a2*2), axis=1)\n",
    "# print(a)\n",
    "\n",
    "b = np.hstack((a, a2.reshape(-1,1)))\n",
    "\n",
    "\n",
    "b, np.amax(b, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_nse(pred, target):\n",
    "#     \"\"\"\n",
    "#     Currently returns the overall nse per basin.\n",
    "\n",
    "#     Note: modify this to allow per day per basin as well.\n",
    "#     \"\"\"\n",
    "#     # ngrid: number of basins\n",
    "#     # nt: number of timesteps (in days usually)\n",
    "#     ngrid, nt = pred.shape\n",
    "#     NSE = np.full(ngrid, np.nan)\n",
    "\n",
    "#     print(len(pred[670,:]), len(pred))\n",
    "#     for k in range(0, ngrid):\n",
    "#         x = pred[k, :]\n",
    "#         y = target[k, :]\n",
    "#         ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n",
    "#         if ind.shape[0] > 0:\n",
    "#             xx = x[ind]\n",
    "#             yy = y[ind]\n",
    "\n",
    "#             if ind.shape[0] > 1:\n",
    "#                 yymean = yy.mean()\n",
    "\n",
    "#                 SST = np.sum((yy-yymean)**2)\n",
    "#                 SSRes = np.sum((yy-xx)**2)\n",
    "#                 NSE[k] = 1-SSRes/SST\n",
    "\n",
    "#     return NSE\n",
    "\n",
    "\n",
    "\n",
    "# for i, (x,y) in enumerate(zip(preds, obs)):\n",
    "#     # print(i)\n",
    "#     # print(x.shape)\n",
    "#     nse = calc_nse(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD HYDRO MULTI MOSAIC MODEL\n",
    "\n",
    "class hydroEnsemble(torch.nn.Module):\n",
    "    # Wrapper for multiple hydrologic models.\n",
    "    # In future, consider just passing the models you want to ensemble explicitly.\n",
    "    def __init__(self, num_models, hidden_size, num_layers):\n",
    "        super(hydroEnsemble, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(num_models, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_models)  # Two models (modelA and modelB)\n",
    "\n",
    "        # self.modelA = modelA\n",
    "        # self.modelB = modelB\n",
    "        # self.classifier = torch.nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence tensor with shape (batch_size, sequence_length, num_models)\n",
    "\n",
    "        # Setting randomseed for deterministic output.\n",
    "        randomseed_config(0)\n",
    "\n",
    "        # Add batch dimension to input and convert to tensor.\n",
    "        x_exp = x.unsqueeze(0)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x_exp)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc_out = self.fc(lstm_out)\n",
    "\n",
    "        # Apply softmax activation to obtain weights\n",
    "        weights = torch.nn.functional.softmax(fc_out, dim=2).squeeze()\n",
    "\n",
    "        # Weighted combination of predictions.\n",
    "        weighted_preds = np.multiply(weights.detach(), x)\n",
    "\n",
    "        # Or take the max weight and return the corresponding value.\n",
    "        max_vals, _ = torch.max(weights, dim=1)\n",
    "        btensor = torch.zeros_like(weights)\n",
    "        btensor[weights==max_vals.view(-1,1)] = 1\n",
    "        weighted_preds = np.multiply(btensor.detach(), x)\n",
    "\n",
    "        preds = torch.sum(weighted_preds, dim=1)\n",
    "\n",
    "        # All tensors\n",
    "        # return preds, weights, weighted_preds\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (6160166.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    from config.read_configurations import config_hbv as 'hbvArgs'\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from config.read_configurations import config_hbv as hbvArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prcp_weights(torch.nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, prcp_datatypes=1):\n",
    "        super(prcp_weights, self).__init__()\n",
    "        self.ninv = ninv\n",
    "        self.prcp_datatypes = prcp_datatypes\n",
    "\n",
    "        ntp = prcp_datatypes*3\n",
    "        self.hiddeninv = hiddeninv\n",
    "\n",
    "        self.lstminv = CudnnLstmModel(\n",
    "            nx=ninv, ny=ntp, hiddenSize=hiddeninv, dr=drinv)\n",
    "        lb_prcp = [0.95]\n",
    "        ub_prcp = [1.05]\n",
    "        self.RangeBoundLoss = RangeBoundLoss(lb=lb_prcp, ub=ub_prcp)\n",
    "\n",
    "    def forward(self, x, z, prcp_loss_factor):\n",
    "        z.requires_grad = True\n",
    "\n",
    "        wghts = self.lstminv(z)\n",
    "        ntstep = wghts.shape[0]\n",
    "        ngage = wghts.shape[1]\n",
    "        wghts_scaled = torch.sigmoid(wghts)\n",
    "        prcp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        # prcp_wghts_sum = torch.sum(wghts_scaled, dim=2)\n",
    "\n",
    "\n",
    "        temp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        pet_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        prcp_wghts_sum = torch.sum(wghts_scaled[:,:,:3], dim=2)\n",
    "        temp_wghts_sum = torch.sum(wghts_scaled[:,:,3:6], dim=2)\n",
    "        pet_wghts_sum = torch.sum(wghts_scaled[:,:,6:], dim=2)\n",
    "\n",
    "\n",
    "\n",
    "        for para in range(wghts.shape[2]):\n",
    "            if para<3:\n",
    "                prcp_wavg = prcp_wavg + wghts_scaled[:, :, para] * x[:, :, para]\n",
    "            elif para<6:\n",
    "                temp_wavg = temp_wavg + wghts_scaled[:, :, para] * x[:, :, para]\n",
    "            elif para>=6:\n",
    "                pet_wavg = pet_wavg + wghts_scaled[:, :, para] * x[:, :, para]\n",
    "\n",
    "        x_new = torch.empty((ntstep, ngage, 3), requires_grad=True, dtype=torch.float32).cuda()\n",
    "        x_new[:, :, 0] = prcp_wavg\n",
    "        # x_new[:, :, 1] = x[:, :, self.prcp_datatypes]\n",
    "        # x_new[:, :, 2] = x[:, :, -1]\n",
    "        x_new[:, :, 1] = temp_wavg\n",
    "        x_new[:, :, 2] = pet_wavg\n",
    "        # range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)+self.RangeBoundLoss([temp_wghts_sum], factor=prcp_loss_factor)+self.RangeBoundLoss([pet_wghts_sum], factor=prcp_loss_factor)\n",
    "        range_bound_loss_prcp = self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)\n",
    "\n",
    "        # range_bound_loss_prcp = 0\n",
    "\n",
    "        grad_daymet = autograd.grad(outputs=wghts_scaled[:, :, 0], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 0]), retain_graph=True)[0]\n",
    "        grad_maurer = autograd.grad(outputs=wghts_scaled[:, :, 1], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 1]), retain_graph=True)[0]\n",
    "        grad_nldas = autograd.grad(outputs=wghts_scaled[:, :, 2], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 2]), retain_graph=True)[0]\n",
    "\n",
    "        return x_new, range_bound_loss_prcp, wghts_scaled, grad_daymet, grad_maurer, grad_nldas\n",
    "        # return x_new, range_bound_loss_prcp, wghts_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming CudnnLstmModel and RangeBoundLoss are defined elsewhere\n",
    "# from your_cudnn_lstm_model_file import CudnnLstmModel\n",
    "# from your_range_bound_loss_file import RangeBoundLoss\n",
    "\n",
    "class prcp_weights(nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, prcp_datatypes=1):\n",
    "        super(prcp_weights, self).__init__()\n",
    "        self.ninv = ninv\n",
    "        self.hiddeninv = hiddeninv\n",
    "        self.prcp_datatypes = prcp_datatypes\n",
    "        self.drinv = drinv\n",
    "\n",
    "        # Initialize LSTM and loss model\n",
    "        self.initialize_models()\n",
    "\n",
    "    def initialize_models(self):\n",
    "        ntp = self.prcp_datatypes * 3\n",
    "        self.lstminv = CudnnLstmModel(nx=self.ninv, ny=ntp, hiddenSize=self.hiddeninv, dr=self.drinv)\n",
    "        lb_prcp = [0.95]\n",
    "        ub_prcp = [1.05]\n",
    "        self.RangeBoundLoss = RangeBoundLoss(lb=lb_prcp, ub=ub_prcp)\n",
    "\n",
    "    def forward(self, x, z, prcp_loss_factor):\n",
    "        z.requires_grad = True\n",
    "        wghts = self.lstminv(z)\n",
    "        wghts_scaled = torch.sigmoid(wghts)\n",
    "\n",
    "        # Calculate weighted averages\n",
    "        prcp_wavg, temp_wavg, pet_wavg = self.calculate_weighted_averages(x, wghts_scaled)\n",
    "\n",
    "        # Construct new x with weighted averages\n",
    "        x_new = self.construct_x_new(prcp_wavg, temp_wavg, pet_wavg)\n",
    "\n",
    "        # Compute range bound loss for precipitation\n",
    "        range_bound_loss_prcp = self.compute_range_bound_loss(wghts_scaled, prcp_loss_factor)\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_daymet, grad_maurer, grad_nldas = self.compute_gradients(z, wghts_scaled)\n",
    "\n",
    "        return x_new, range_bound_loss_prcp, wghts_scaled, grad_daymet, grad_maurer, grad_nldas\n",
    "\n",
    "    def calculate_weighted_averages(self, x, wghts_scaled):\n",
    "        ntstep, ngage, _ = wghts_scaled.shape\n",
    "        device = wghts_scaled.device\n",
    "        prcp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32, device=device)\n",
    "        temp_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32, device=device)\n",
    "        pet_wavg = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32, device=device)\n",
    "\n",
    "        for para in range(wghts_scaled.shape[2]):\n",
    "            if para < 3:\n",
    "                prcp_wavg += wghts_scaled[:, :, para] * x[:, :, para]\n",
    "            elif para < 6:\n",
    "                temp_wavg += wghts_scaled[:, :, para] * x[:, :, para]\n",
    "            else:\n",
    "                pet_wavg += wghts_scaled[:, :, para] * x[:, :, para]\n",
    "        return prcp_wavg, temp_wavg, pet_wavg\n",
    "\n",
    "    def construct_x_new(self, prcp_wavg, temp_wavg, pet_wavg):\n",
    "        ntstep, ngage = prcp_wavg.shape\n",
    "        device = prcp_wavg.device\n",
    "        x_new = torch.empty((ntstep, ngage, 3), requires_grad=True, dtype=torch.float32, device=device)\n",
    "        x_new[:, :, 0] = prcp_wavg\n",
    "        x_new[:, :, 1] = temp_wavg\n",
    "        x_new[:, :, 2] = pet_wavg\n",
    "        return x_new\n",
    "\n",
    "    def compute_range_bound_loss(self, wghts_scaled, prcp_loss_factor):\n",
    "        prcp_wghts_sum = torch.sum(wghts_scaled[:, :, :3], dim=2)\n",
    "        return self.RangeBoundLoss([prcp_wghts_sum], factor=prcp_loss_factor)\n",
    "\n",
    "    def compute_gradients(self, z, wghts_scaled):\n",
    "        grad_daymet = autograd.grad(outputs=wghts_scaled[:, :, 0], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 0]), retain_graph=True)[0]\n",
    "        grad_maurer = autograd.grad(outputs=wghts_scaled[:, :, 1], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 1]), retain_graph=True)[0]\n",
    "        grad_nldas = autograd.grad(outputs=wghts_scaled[:, :, 2], inputs=z, grad_outputs=torch.ones_like(wghts_scaled[:, :, 2]), retain_graph=True)[0]\n",
    "        return grad_daymet, grad_maurer, grad_nldas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "\n",
    "class prcp_weights(torch.nn.Module):\n",
    "    def __init__(self, *, ninv, hiddeninv, drinv=0.5, prcp_datatypes=1):\n",
    "        super(prcp_weights, self).__init__()\n",
    "        self.ninv = ninv\n",
    "        self.hiddeninv = hiddeninv\n",
    "        self.drinv = drinv\n",
    "        self.prcp_datatypes = prcp_datatypes\n",
    "\n",
    "        # Initialize keys for dictionaries\n",
    "        self.defaultKeys = {\n",
    "            'hyperparams': ['lowerb_loss', 'upperb_loss', 'prcp_loss_factor'],\n",
    "            'states': ['weights', 'weights_scaled', 'prcp_wavg', 'temp_wavg', 'pet_wavg', 'prcp_wghts_sum', 'temp_wghts_sum', 'pet_wghts_sum'],\n",
    "        }\n",
    "\n",
    "        # Initialize dictionaries\n",
    "        self.hparams = {}\n",
    "        self.states = {}\n",
    "\n",
    "        # Model initialization\n",
    "        self.initModel()\n",
    "        self.initParams()\n",
    "\n",
    "    def initModel(self):\n",
    "        \"\"\"Initialize LSTM model and other required models.\"\"\"\n",
    "        ntp = self.prcp_datatypes * 3\n",
    "        self.lstminv = CudnnLstmModel(nx=self.ninv, ny=ntp, hiddenSize=self.hiddeninv, dr=self.drinv)\n",
    "\n",
    "    def initParams(self):\n",
    "        \"\"\"Initialize parameters and states.\"\"\"\n",
    "        self.hparams['lowerb_loss'] = [0.95]\n",
    "        self.hparams['upperb_loss'] = [1.05]\n",
    "        self.hparams['prcp_loss_factor'] = 23  # Default value, can be updated in forward call\n",
    "\n",
    "        # States initialization with zeros might be done in preRun based on actual input sizes\n",
    "\n",
    "    def preRun(self, x):\n",
    "        \"\"\"Prepare or reset states before running the forward pass.\"\"\"\n",
    "        ntstep, ngage, _ = x.shape\n",
    "        self.states['weights_scaled'] = torch.zeros((ntstep, ngage, self.prcp_datatypes * 3), requires_grad=True, dtype=torch.float32).cuda()\n",
    "\n",
    "        # Initialize other states as required, for example:\n",
    "        self.states['prcp_wavg'] = torch.zeros((ntstep, ngage), requires_grad=True, dtype=torch.float32).cuda()\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        self.preRun(x)  # Prepare states\n",
    "\n",
    "        # Assuming prcp_loss_factor is passed here or has been set during initialization\n",
    "        prcp_loss_factor = self.hparams['prcp_loss_factor']\n",
    "\n",
    "        z.requires_grad = True\n",
    "        self.states['weights'] = self.lstminv(z)\n",
    "        self.states['weights_scaled'] = torch.sigmoid(self.states['weights'])\n",
    "\n",
    "        # Calculations as per your logic...\n",
    "        # Update states['prcp_wavg'], etc.\n",
    "\n",
    "        # Calculate range bound loss\n",
    "        range_bound_loss_prcp = self.calculate_range_bound_loss(prcp_loss_factor)\n",
    "\n",
    "        # Return results along with any calculated losses and gradients\n",
    "        # You need to adapt this to match your specific return expectations\n",
    "        return self.states['weights_scaled'], range_bound_loss_prcp\n",
    "\n",
    "    def calculate_range_bound_loss(self, factor):\n",
    "        \"\"\"Calculate range bound loss based on current weights sum and predefined bounds.\"\"\"\n",
    "        prcp_wghts_sum = torch.sum(self.states['weights_scaled'][:, :, :3], dim=2)\n",
    "        # Implement the actual loss calculation using self.hparams['lowerb_loss'] and self.hparams['upperb_loss']\n",
    "        # Placeholder for actual loss calculation\n",
    "        loss = torch.tensor(0.0)  # This should be replaced with the actual loss calculation logic\n",
    "        return loss\n",
    "\n",
    "# Note: You'll need to adjust the implementation of `calculate_range_bound_loss` and other parts of the class\n",
    "# to match your exact requirements, including proper loss calculation and updating states as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s = np.array([[3,3],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhpihydrodl_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
