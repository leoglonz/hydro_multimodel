{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(2)\n",
    "b = np.arange(8)\n",
    "\n",
    "a = a.reshape((2,2,2))\n",
    "b = b.reshape((2,2,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 1],\n",
       "         [2, 3]],\n",
       " \n",
       "        [[4, 5],\n",
       "         [6, 7]]]),\n",
       " array([[[0, 1],\n",
       "         [2, 3]],\n",
       " \n",
       "        [[4, 5],\n",
       "         [6, 7]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5, 2.5],\n",
       "        [4.5, 6.5]],\n",
       "\n",
       "       [[0.5, 2.5],\n",
       "        [4.5, 6.5]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([a,b],axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_PRMS_SNTEMP as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def m_softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00626879, 0.01704033, 0.04632042, 0.93037047]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([[1, 2, 3, 6]])\n",
    "m_softmax(x1) #, m_softmax(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[1, 2, 3, 6]])\n",
    "\n",
    "m = m_softmax(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = []\n",
    "for i in range(len(x1)):\n",
    "    n.append(x1[i]*m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.00626879, 0.03408066, 0.13896125, 5.58222279])], 5.761533493695909)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n, np.sum(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataTs(self, args, varLst, doNorm=True, rmNan=True):\n",
    "        if type(varLst) is str:\n",
    "            varLst = [varLst]\n",
    "        inputfile = os.path.join(os.path.realpath(args[\"forcing_path\"]))\n",
    "        inputfile_attr = os.path.join(os.path.realpath(args[\"attr_path\"]))\n",
    "        if inputfile.endswith(\".csv\"):\n",
    "            dfMain = pd.read_csv(inputfile)\n",
    "            dfMain_attr = pd.read_csv(inputfile_attr)\n",
    "        elif inputfile.endswith(\".feather\"):\n",
    "            dfMain = pd.read_feather(inputfile)\n",
    "            dfMain_attr = pd.read_feather(inputfile_attr)\n",
    "        else:\n",
    "            print(\"data type is not supported\")\n",
    "            exit()\n",
    "        sites = dfMain[\"site_no\"].unique()\n",
    "        tLst = tRange2Array(args[\"tRange\"])\n",
    "        tLstobs = tRange2Array(args[\"tRange\"])\n",
    "        # nt = len(tLst)\n",
    "        ntobs = len(tLstobs)\n",
    "        nNodes = len(sites)\n",
    "\n",
    "        varLst_forcing = []\n",
    "        varLst_attr = []\n",
    "        for var in varLst:\n",
    "            if var in dfMain.columns:\n",
    "                varLst_forcing.append(var)\n",
    "            elif var in dfMain_attr.columns:\n",
    "                varLst_attr.append(var)\n",
    "            else:\n",
    "                print(var, \"the var is not in forcing file nor in attr file\")\n",
    "        xt = dfMain.loc[:, varLst_forcing].values\n",
    "        g = dfMain.reset_index(drop=True).groupby(\"site_no\")\n",
    "        xtg = [xt[i.values, :] for k, i in g.groups.items()]\n",
    "        x = np.array(xtg)\n",
    "\n",
    "        ## for attr\n",
    "        if len(varLst_attr) > 0:\n",
    "            x_attr_t = dfMain_attr.loc[:, varLst_attr].values\n",
    "            x_attr_t = np.expand_dims(x_attr_t, axis=2)\n",
    "            xattr = np.repeat(x_attr_t, x.shape[1], axis=2)\n",
    "            xattr = np.transpose(xattr, (0, 2, 1))\n",
    "            x = np.concatenate((x, xattr), axis=2)\n",
    "\n",
    "        data = x\n",
    "        C, ind1, ind2 = np.intersect1d(self.time, tLst, return_indices=True)\n",
    "        data = data[:, ind2, :]\n",
    "        # if os.path.isdir(out):\n",
    "        #     pass\n",
    "        # else:\n",
    "        #     os.makedirs(out)\n",
    "        # np.save(os.path.join(out, 'x.npy'), data)\n",
    "        # if doNorm is True:\n",
    "        #     data = transNorm(data, varLst, toNorm=True)\n",
    "        # if rmNan is True:\n",
    "        #     data[np.where(np.isnan(data))] = 0\n",
    "        return np.swapaxes(data, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "def loadData(args, trange):\n",
    "    out_dict = dict()\n",
    "    df = DataFrame_dataset(tRange=trange)\n",
    "    # getting inputs for NN model:\n",
    "    out_dict[\"x_NN\"] = df.getDataTs(args, varLst=args[\"varT_NN\"])\n",
    "    out_dict[\"c_NN\"] = df.getDataConst(args, varLst=args[\"varC_NN\"])\n",
    "    out_dict[\"obs\"] = df.getDataTs(args, varLst=args[\"target\"])\n",
    "    if args[\"hydro_model_name\"] != \"None\":\n",
    "        out_dict[\"x_hydro_model\"] = df.getDataTs(args, varLst=args[\"varT_hydro_model\"])\n",
    "        out_dict[\"c_hydro_model\"] = df.getDataConst(args, varLst=args[\"varC_hydro_model\"])\n",
    "    if args[\"temp_model_name\"] != \"None\":\n",
    "        out_dict[\"x_temp_model\"] = df.getDataTs(args, varLst=args[\"varT_temp_model\"])\n",
    "        out_dict[\"c_temp_model\"] = df.getDataConst(args, varLst=args[\"varC_temp_model\"])\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Walking Through PRMS SAC-SMA testing in PGML here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.read_configurations import config_sacsma as args\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from core.utils.randomseed_config import randomseed_config\n",
    "from core.utils.master import loadModel, create_output_dirs\n",
    "from MODELS.loss_functions.get_loss_function import get_lossFun\n",
    "from MODELS.train_test import test_differentiable_model\n",
    "from core.data_processing.data_loading import loadData\n",
    "\n",
    "\n",
    "from core.utils import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomseed_config(seed=args[\"randomseed\"][0])\n",
    "# Creating output directories and adding it to args\n",
    "args = create_output_dirs(args)\n",
    "\n",
    "lossFun = get_lossFun(args)\n",
    "\n",
    "modelFile = os.path.join(args[\"out_dir\"], \"model_Ep\" + str(args[\"EPOCHS\"]) + \".pt\")\n",
    "diff_model = torch.load(modelFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_hydro_temp_model(\n",
       "  (hydro_model): SACSMAMul()\n",
       "  (NN_model): CudnnLstmModel(\n",
       "    (linearIn): Linear(in_features=37, out_features=256, bias=True)\n",
       "    (lstm): CudnnLstm()\n",
       "    (linearOut): Linear(in_features=256, out_features=24, bias=True)\n",
       "    (activation_sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up = args[\"warm_up\"]\n",
    "nmul = args[\"nmul\"]\n",
    "diff_model.eval()\n",
    "# read data for test time range\n",
    "dataset_dictionary = loadData(args, trange=args[\"t_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4.80e+01],\n",
       "        [3.90e+01],\n",
       "        [3.60e+01],\n",
       "        ...,\n",
       "        [4.10e+01],\n",
       "        [5.20e-01],\n",
       "        [1.14e+02]],\n",
       "\n",
       "       [[4.60e+01],\n",
       "        [3.90e+01],\n",
       "        [3.60e+01],\n",
       "        ...,\n",
       "        [3.60e+01],\n",
       "        [4.60e-01],\n",
       "        [1.03e+02]],\n",
       "\n",
       "       [[4.30e+01],\n",
       "        [3.90e+01],\n",
       "        [3.50e+01],\n",
       "        ...,\n",
       "        [3.40e+01],\n",
       "        [4.40e-01],\n",
       "        [9.80e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[6.39e+02],\n",
       "        [6.30e+01],\n",
       "        [5.75e+02],\n",
       "        ...,\n",
       "        [3.50e+01],\n",
       "        [9.00e-02],\n",
       "        [1.24e+02]],\n",
       "\n",
       "       [[7.52e+02],\n",
       "        [7.20e+01],\n",
       "        [5.99e+02],\n",
       "        ...,\n",
       "        [3.40e+01],\n",
       "        [9.00e-02],\n",
       "        [1.19e+02]],\n",
       "\n",
       "       [[8.61e+02],\n",
       "        [7.50e+01],\n",
       "        [6.23e+02],\n",
       "        ...,\n",
       "        [3.40e+01],\n",
       "        [8.00e-02],\n",
       "        [1.15e+02]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary['obs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_processing.normalization import transNorm\n",
    "\n",
    "x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n",
    "c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n",
    "c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n",
    "dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n",
    "del x_NN_scaled, dataset_dictionary[\"x_NN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_dictionary.keys():\n",
    "    dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n",
    "\n",
    "args[\"batch_size\"] = args[\"no_basins\"]\n",
    "nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n",
    "rho = args[\"rho\"]\n",
    "\n",
    "batch_size = args[\"batch_size\"]\n",
    "iS = np.arange(0, ngrid, batch_size)\n",
    "iE = np.append(iS[1:], ngrid)\n",
    "list_out_diff_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]d:\\code_repos\\hydro_ensemble\\MODELS\\NN_models\\LSTM_models.py:99: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
      "  7%|▋         | 2/27 [00:28<05:55, 14.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(iS))):\n\u001b[0;32m     11\u001b[0m     dataset_dictionary_sample \u001b[38;5;241m=\u001b[39m take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n\u001b[1;32m---> 12\u001b[0m     out_diff_model \u001b[38;5;241m=\u001b[39m \u001b[43mdiff_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dictionary_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     out_diff_model_cpu \u001b[38;5;241m=\u001b[39m {key: tensor\u001b[38;5;241m.\u001b[39mcpu\n\u001b[0;32m     14\u001b[0m     ()\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m out_diff_model\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\code_repos\\hydro_ensemble\\MODELS\\Differentiable_models.py:141\u001b[0m, in \u001b[0;36mdiff_hydro_temp_model.forward\u001b[1;34m(self, dataset_dictionary_sample)\u001b[0m\n\u001b[0;32m    138\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreakdown_params(params_all)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhydro_model_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# hydro model\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     flow_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydro_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dictionary_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_hydro_model_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dictionary_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc_hydro_model_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhydro_params_raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# PET_param=params_dict[\"params_PET_model\"],  # PET is in both temp and flow model\u001b[39;49;00m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarm_up\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouting_hydro_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_params_hydro\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv_params_hydro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# Todo: send this to  a function\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# source flow calculation and converting mm/day to m3/ day\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     srflow, ssflow, gwflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhydro_model\u001b[38;5;241m.\u001b[39msource_flow_calculation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, flow_out,\n\u001b[0;32m    155\u001b[0m                                                                       dataset_dictionary_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_NN_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LeoLo\\miniconda3\\envs\\PGML_STemp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\code_repos\\hydro_ensemble\\MODELS\\hydro_models\\SACSMA\\SACSMAmul.py:384\u001b[0m, in \u001b[0;36mSACSMAMul.forward\u001b[1;34m(self, x_hydro_model, c_hydro_model, SACSMA_params_raw, args, muwts, warm_up, init, routing, comprout, conv_params_hydro)\u001b[0m\n\u001b[0;32m    382\u001b[0m flux_twexlp \u001b[38;5;241m=\u001b[39m ((lzfwpm \u001b[38;5;241m-\u001b[39m LZFWP_storage) \u001b[38;5;241m/\u001b[39m (lzfwpm \u001b[38;5;241m*\u001b[39m ((lzfwpm \u001b[38;5;241m-\u001b[39m LZFWP_storage) \u001b[38;5;241m/\u001b[39m lzfwpm) \u001b[38;5;241m+\u001b[39m ((lzfwsm \u001b[38;5;241m-\u001b[39m LZFWS_storage) \u001b[38;5;241m/\u001b[39m lzfwsm))) \u001b[38;5;241m*\u001b[39m flux_twexl\n\u001b[0;32m    383\u001b[0m LZFWP_storage \u001b[38;5;241m=\u001b[39m LZFWP_storage \u001b[38;5;241m+\u001b[39m flux_Pcfwp \u001b[38;5;241m+\u001b[39m flux_twexlp\n\u001b[1;32m--> 384\u001b[0m extra_LZFWP \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLZFWP_storage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlzfwpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m LZFWP_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(LZFWP_storage \u001b[38;5;241m-\u001b[39m extra_LZFWP, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)  \u001b[38;5;66;03m# I added this to make the storage not to exceed the max\u001b[39;00m\n\u001b[0;32m    386\u001b[0m flux_Qbfp \u001b[38;5;241m=\u001b[39m params_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklzp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m LZFWP_storage\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from core.data_processing.model import (\n",
    "    No_iter_nt_ngrid,\n",
    "    take_sample_train,\n",
    "    take_sample_test,\n",
    "    converting_flow_from_ft3_per_sec_to_mm_per_day\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(iS))):\n",
    "    dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n",
    "    out_diff_model = diff_model(dataset_dictionary_sample)\n",
    "    out_diff_model_cpu = {key: tensor.cpu\n",
    "    ().detach() for key, tensor in out_diff_model.items()}\n",
    "    # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n",
    "    list_out_diff_model.append(out_diff_model_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['c_NN', 'obs', 'x_hydro_model', 'c_hydro_model', 'inputs_NN_scaled']),\n",
       " dict_keys(['c_NN_sample', 'obs_sample', 'x_hydro_model_sample', 'c_hydro_model_sample', 'inputs_NN_scaled_sample']),\n",
       " dict_keys(['flow_sim', 'srflow', 'ssflow', 'gwflow', 'PET_hydro', 'AET_hydro', 'BFI_sim']))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary.keys(), dataset_dictionary_sample.keys(), out_diff_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5479"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dictionary['inputs_NN_scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args, dataset_dictionary[\"c_NN\"],\n",
    "                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6701],\n",
       "         [0.9128],\n",
       "         [0.3254],\n",
       "         ...,\n",
       "         [0.1160],\n",
       "         [0.0201],\n",
       "         [0.2646]],\n",
       "\n",
       "        [[0.6353],\n",
       "         [0.8275],\n",
       "         [0.3527],\n",
       "         ...,\n",
       "         [0.1160],\n",
       "         [0.0213],\n",
       "         [0.2646]],\n",
       "\n",
       "        [[0.6245],\n",
       "         [0.7507],\n",
       "         [0.3620],\n",
       "         ...,\n",
       "         [0.1160],\n",
       "         [0.0224],\n",
       "         [0.2646]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6940],\n",
       "         [0.2687],\n",
       "         [0.3827],\n",
       "         ...,\n",
       "         [0.1933],\n",
       "         [0.0053],\n",
       "         [0.4318]],\n",
       "\n",
       "        [[0.8167],\n",
       "         [0.3071],\n",
       "         [0.3986],\n",
       "         ...,\n",
       "         [0.1877],\n",
       "         [0.0053],\n",
       "         [0.4144]],\n",
       "\n",
       "        [[0.9351],\n",
       "         [0.3199],\n",
       "         [0.4146],\n",
       "         ...,\n",
       "         [0.1877],\n",
       "         [0.0047],\n",
       "         [0.4004]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_differentiable_model(args, diff_model):\n",
    "    warm_up = args[\"warm_up\"]\n",
    "    nmul = args[\"nmul\"]\n",
    "    diff_model.eval()\n",
    "    # read data for test time range\n",
    "    dataset_dictionary = loadData(args, trange=args[\"t_test\"])\n",
    "    np.save(os.path.join(args[\"out_dir\"], \"x.npy\"), dataset_dictionary[\"x_NN\"])  # saves with the overlap in the beginning\n",
    "    # normalizing\n",
    "    x_NN_scaled = transNorm(args, dataset_dictionary[\"x_NN\"], varLst=args[\"varT_NN\"], toNorm=True)\n",
    "    c_NN_scaled = transNorm(args, dataset_dictionary[\"c_NN\"], varLst=args[\"varC_NN\"], toNorm=True)\n",
    "    c_NN_scaled = np.repeat(np.expand_dims(c_NN_scaled, 0), x_NN_scaled.shape[0], axis=0)\n",
    "    dataset_dictionary[\"inputs_NN_scaled\"] = np.concatenate((x_NN_scaled, c_NN_scaled), axis=2)\n",
    "    del x_NN_scaled, dataset_dictionary[\"x_NN\"]\n",
    "    # converting the numpy arrays to torch tensors:\n",
    "    for key in dataset_dictionary.keys():\n",
    "        dataset_dictionary[key] = torch.from_numpy(dataset_dictionary[key]).float()\n",
    "\n",
    "    # args_mod = args.copy()\n",
    "    args[\"batch_size\"] = args[\"no_basins\"]\n",
    "    nt, ngrid, nx = dataset_dictionary[\"inputs_NN_scaled\"].shape\n",
    "    rho = args[\"rho\"]\n",
    "\n",
    "    # Making lists of the start and end indices of the basins for each batch.\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    iS = np.arange(0, ngrid, batch_size)    # Start index list.\n",
    "    iE = np.append(iS[1:], ngrid)   # End.\n",
    "    \n",
    "    list_out_diff_model = []\n",
    "    for i in tqdm(range(0, len(iS)), unit='Batch'):\n",
    "        dataset_dictionary_sample = take_sample_test(args, dataset_dictionary, iS[i], iE[i])\n",
    "\n",
    "        out_diff_model = diff_model(dataset_dictionary_sample)\n",
    "        # Convert all tensors in the dictionary to CPU\n",
    "        out_diff_model_cpu = {key: tensor.cpu().detach() for key, tensor in out_diff_model.items()}\n",
    "        # out_diff_model_cpu = tuple(outs.cpu().detach() for outs in out_diff_model)\n",
    "        list_out_diff_model.append(out_diff_model_cpu)\n",
    "\n",
    "    # getting rid of warm-up period in observation dataset and making the dimension similar to\n",
    "    # converting numpy to tensor\n",
    "    # y_obs = torch.tensor(np.swapaxes(y_obs[:, warm_up:, :], 0, 1), dtype=torch.float32)\n",
    "    # c_hydro_model = torch.tensor(c_hydro_model, dtype=torch.float32)\n",
    "    y_obs = converting_flow_from_ft3_per_sec_to_mm_per_day(args, dataset_dictionary[\"c_NN\"],\n",
    "                                                           dataset_dictionary[\"obs\"][warm_up:, :, :])\n",
    "\n",
    "    return list_out_diff_model, y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5479"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dictionary['x_NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Walking Through HBV testing in dPLHBV here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "## fix the random seeds\n",
    "import torch\n",
    "randomseed = 111111\n",
    "random.seed(randomseed)\n",
    "torch.manual_seed(randomseed)\n",
    "np.random.seed(randomseed)\n",
    "torch.cuda.manual_seed(randomseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from hydroDL import master, utils\n",
    "\n",
    "\n",
    "## GPU setting\n",
    "testgpuid = 0\n",
    "torch.cuda.set_device(testgpuid)\n",
    "\n",
    "## setting options, keep the same as your training\n",
    "PUOpt = 0  # 0 for All; 1 for PUB; 2 for PUR;\n",
    "buffOptOri = 0  # original buffOpt, must be same as what you set for training\n",
    "buffOpt = 0  # control load training data 0: do nothing; 1: repeat first year; 2: load one more year\n",
    "forType = 'daymet'\n",
    "\n",
    "## Hyperparameters, keep the same as your training setup\n",
    "BATCH_SIZE = 100\n",
    "RHO = 365\n",
    "HIDDENSIZE = 256\n",
    "Ttrain = [19801001, 19951001]  # Training period\n",
    "# Ttrain = [19891001, 19991001]  # PUB/PUR period\n",
    "Tinv = [19801001, 19951001] # dPL Inversion period\n",
    "# Tinv = [19891001, 19991001]  # PUB/PUR period\n",
    "Nfea = 12 # number of HBV parameters\n",
    "BUFFTIME = 365\n",
    "routing = True\n",
    "Nmul = 16\n",
    "comprout = False\n",
    "compwts = False\n",
    "pcorr = None\n",
    "\n",
    "Ttest = [19951001, 20101001]  # testing period\n",
    "TtestLst = utils.time.tRange2Array(Ttest)\n",
    "TtestLoad = [19951001, 20101001]  \n",
    "\n",
    "testbatch = 50  # forward number of \"testbatch\" basins each time to save GPU memory. You can set this even smaller to save more.\n",
    "testepoch = 50\n",
    "\n",
    "testseed = 111111\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'camels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define root directory of database and saved output dir\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Modify this based on your own location of CAMELS dataset and saved models\u001b[39;00m\n\u001b[0;32m      3\u001b[0m rootDatabase \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCamels\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# CAMELS dataset root directory\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcamels\u001b[49m\u001b[38;5;241m.\u001b[39minitcamels(rootDatabase)  \u001b[38;5;66;03m# initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'camels' is not defined"
     ]
    }
   ],
   "source": [
    "# Define root directory of database and saved output dir\n",
    "# Modify this based on your own location of CAMELS dataset and saved models\n",
    "rootDatabase = os.path.join(os.path.sep, 'D:\\data', 'Camels')  # CAMELS dataset root directory\n",
    "camels.initcamels(rootDatabase)  # initialize three camels module-scope variables in camels.py: dirDB, gageDict, statDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From testMulti.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model\n",
    "# lstm = hydroEnsemble(num_models=3, hidden_size=256, num_layers=1)\n",
    "\n",
    "\n",
    "# # Create a sample input tensor\n",
    "# batch_size = 1\n",
    "# sequence_length = 671\n",
    "# num_models = 3\n",
    "# input_tensor = torch.randn(sequence_length, num_models)\n",
    "# input_array = np.random.rand(sequence_length, num_models)\n",
    "\n",
    "# # Forward pass through the model\n",
    "# preds= lstm(torch.tensor(input_tensor, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_output['marrmot_PRMS'][0].keys():\n",
    "    if len(model_output['marrmot_PRMS'][0][key].shape) == 3:\n",
    "        dim = 1\n",
    "    else:\n",
    "        dim = 0\n",
    "    concatenated_tensor = torch.cat([d[key] for d in model_output], dim=dim)\n",
    "    file_name = key + \".npy\"\n",
    "    np.save(os.path.join(SAVE_PATH, args[\"testing_dir\"], file_name), concatenated_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([[1, 2],\n",
    "                   [3, 4],\n",
    "                   [5, 6]])\n",
    "\n",
    "# Create a 1x3 array\n",
    "a2 = np.array([7, 8, 9])\n",
    "\n",
    "a = np.stack((a2, a2*2), axis=1)\n",
    "# print(a)\n",
    "\n",
    "b = np.hstack((a, a2.reshape(-1,1)))\n",
    "\n",
    "\n",
    "b, np.amax(b, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_nse(pred, target):\n",
    "#     \"\"\"\n",
    "#     Currently returns the overall nse per basin.\n",
    "\n",
    "#     Note: modify this to allow per day per basin as well.\n",
    "#     \"\"\"\n",
    "#     # ngrid: number of basins\n",
    "#     # nt: number of timesteps (in days usually)\n",
    "#     ngrid, nt = pred.shape\n",
    "#     NSE = np.full(ngrid, np.nan)\n",
    "\n",
    "#     print(len(pred[670,:]), len(pred))\n",
    "#     for k in range(0, ngrid):\n",
    "#         x = pred[k, :]\n",
    "#         y = target[k, :]\n",
    "#         ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n",
    "#         if ind.shape[0] > 0:\n",
    "#             xx = x[ind]\n",
    "#             yy = y[ind]\n",
    "\n",
    "#             if ind.shape[0] > 1:\n",
    "#                 yymean = yy.mean()\n",
    "\n",
    "#                 SST = np.sum((yy-yymean)**2)\n",
    "#                 SSRes = np.sum((yy-xx)**2)\n",
    "#                 NSE[k] = 1-SSRes/SST\n",
    "\n",
    "#     return NSE\n",
    "\n",
    "\n",
    "\n",
    "# for i, (x,y) in enumerate(zip(preds, obs)):\n",
    "#     # print(i)\n",
    "#     # print(x.shape)\n",
    "#     nse = calc_nse(np.swapaxes(x.squeeze(), 1, 0), np.swapaxes(y.squeeze(), 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD HYDRO MULTI MOSAIC MODEL\n",
    "\n",
    "class hydroEnsemble(torch.nn.Module):\n",
    "    # Wrapper for multiple hydrologic models.\n",
    "    # In future, consider just passing the models you want to ensemble explicitly.\n",
    "    def __init__(self, num_models, hidden_size, num_layers):\n",
    "        super(hydroEnsemble, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(num_models, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_models)  # Two models (modelA and modelB)\n",
    "\n",
    "        # self.modelA = modelA\n",
    "        # self.modelB = modelB\n",
    "        # self.classifier = torch.nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence tensor with shape (batch_size, sequence_length, num_models)\n",
    "\n",
    "        # Setting randomseed for deterministic output.\n",
    "        randomseed_config(0)\n",
    "\n",
    "        # Add batch dimension to input and convert to tensor.\n",
    "        x_exp = x.unsqueeze(0)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x_exp)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc_out = self.fc(lstm_out)\n",
    "\n",
    "        # Apply softmax activation to obtain weights\n",
    "        weights = torch.nn.functional.softmax(fc_out, dim=2).squeeze()\n",
    "\n",
    "        # Weighted combination of predictions.\n",
    "        weighted_preds = np.multiply(weights.detach(), x)\n",
    "\n",
    "        # Or take the max weight and return the corresponding value.\n",
    "        max_vals, _ = torch.max(weights, dim=1)\n",
    "        btensor = torch.zeros_like(weights)\n",
    "        btensor[weights==max_vals.view(-1,1)] = 1\n",
    "        weighted_preds = np.multiply(btensor.detach(), x)\n",
    "\n",
    "        preds = torch.sum(weighted_preds, dim=1)\n",
    "\n",
    "        # All tensors\n",
    "        # return preds, weights, weighted_preds\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PGMLHydroModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable hydro model code from F. Rahmani PGML_STemp_with_Snow.\n",
    "    Use for PRMS, SAC-SMA, and unmodified HBV.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(diff_hydro_temp_model, self).__init__()\n",
    "        self.args = args\n",
    "        self.get_model()\n",
    "\n",
    "    def get_NN_model_dim(self) -> None:\n",
    "        self.nx = len(self.args[\"varT_NN\"] + self.args[\"varC_NN\"])\n",
    "\n",
    "        # output size of NN\n",
    "        if self.args[\"hydro_model_name\"] != \"None\":\n",
    "            if self.args[\"routing_hydro_model\"] == True:  # needs a and b for routing with conv method\n",
    "                self.ny_hydro = self.args[\"nmul\"] * (len(self.hydro_model.parameters_bound)) + len(\n",
    "                    self.hydro_model.conv_routing_hydro_model_bound)\n",
    "            else:\n",
    "                self.ny_hydro = self.args[\"nmul\"] * len(self.hydro_model.parameters_bound)\n",
    "        else:\n",
    "            self.ny_hydro = 0\n",
    "\n",
    "        # SNTEMP  # needs a and b for calculating different source flow temperatures with conv method\n",
    "        if self.args[\"temp_model_name\"] != \"None\":\n",
    "            if self.args[\"routing_temp_model\"] == True:\n",
    "                self.ny_temp = self.args[\"nmul\"] * (len(self.temp_model.parameters_bound)) + len(\n",
    "                    self.temp_model.conv_temp_model_bound)\n",
    "            else:\n",
    "                self.ny_temp = self.args[\"nmul\"] * len(self.temp_model.parameters_bound)\n",
    "            if self.args[\"lat_temp_adj\"] == True:\n",
    "                self.ny_temp = self.ny_temp + self.args[\"nmul\"]\n",
    "        else:\n",
    "            self.ny_temp = 0\n",
    "        # if self.args[\"hydro_model_name\"] == \"HBV\":   # no need to have a PET to AET coef\n",
    "        #     self.ny_PET = 0\n",
    "        # elif self.args[\"hydro_model_name\"] == \"marrmot_PRMS\":   # need a PET to AET coef\n",
    "        #     self.ny_PET = self.args[\"nmul\"]\n",
    "        # if self.args[\"potet_module\"] in [\"potet_hargreaves\", \"potet_hamon\", \"dataset\"]:\n",
    "        #     self.ny_PET = self.args[\"nmul\"]\n",
    "        self.ny = self.ny_hydro + self.ny_temp # + self.ny_PET\n",
    "\n",
    "    def get_model(self) -> None:\n",
    "        # hydro_model_initialization\n",
    "        if self.args[\"hydro_model_name\"] != \"None\":\n",
    "            if self.args[\"hydro_model_name\"] == \"marrmot_PRMS\":\n",
    "                self.hydro_model = prms_marrmot()\n",
    "            elif self.args[\"hydro_model_name\"] == \"marrmot_PRMS_gw0\":\n",
    "                self.hydro_model = prms_marrmot_gw0()\n",
    "            elif self.args[\"hydro_model_name\"] == \"HBV\":\n",
    "                self.hydro_model = HBVMul()\n",
    "            elif self.args[\"hydro_model_name\"] == \"SACSMA\":\n",
    "                self.hydro_model = SACSMAMul()\n",
    "            elif self.args[\"hydro_model_name\"] == \"SACSMA_with_snow\":\n",
    "                self.hydro_model = SACSMA_snow_Mul()\n",
    "            elif self.args[\"hydro_model_name\"] != \"None\":\n",
    "                print(\"hydrology (streamflow) model type has not been defined\")\n",
    "                exit()\n",
    "            # temp_model_initialization\n",
    "        if self.args[\"temp_model_name\"] != \"None\":\n",
    "            if self.args[\"temp_model_name\"] == \"SNTEMP\":\n",
    "                self.temp_model = SNTEMP_flowSim()  # this model needs a hydrology model as backbone\n",
    "            elif self.args[\"temp_model_name\"] == \"SNTEMP_gw0\":\n",
    "                self.temp_model = SNTEMP_flowSim_gw0()  # this model needs a hydrology model as backbone, and 4 outflow\n",
    "            elif self.args[\"temp_model_name\"] != \"None\":\n",
    "                print(\"temp model type has not been defined\")\n",
    "                exit()\n",
    "        # get the dimensions of NN model based on hydro modela and temp model\n",
    "        self.get_NN_model_dim()\n",
    "        # NN_model_initialization\n",
    "        if self.args[\"NN_model_name\"] == \"LSTM\":\n",
    "            self.NN_model = CudnnLstmModel(nx=self.nx,\n",
    "                                           ny=self.ny,\n",
    "                                           hiddenSize=self.args[\"hidden_size\"],\n",
    "                                           dr=self.args[\"dropout\"])\n",
    "        elif self.args[\"NN_model_name\"] == \"MLP\":\n",
    "            self.NN_model = MLPmul(self.args, nx=self.nx, ny=self.ny)\n",
    "        else:\n",
    "            print(\"NN model type has not been defined\")\n",
    "            exit()\n",
    "\n",
    "    def breakdown_params(self, params_all):\n",
    "        params_dict = dict()\n",
    "        params_hydro_model = params_all[-1, :, :self.ny_hydro]\n",
    "        params_temp_model = params_all[-1, :, self.ny_hydro: (self.ny_hydro + self.ny_temp)]\n",
    "        # if self.ny_PET > 0:\n",
    "        #     params_dict[\"params_PET_model\"] = torch.sigmoid(params_all[-1, :, (self.ny_hydro + self.ny_temp):])\n",
    "        # else:\n",
    "        #     params_dict[\"params_PET_model\"] = None\n",
    "\n",
    "\n",
    "        # Todo: I should separate PET model output from hydro_model and temp_model.\n",
    "        #  For now, evap is calculated in both models individually (with same method)\n",
    "\n",
    "        if self.args['hydro_model_name'] != \"None\":\n",
    "            # hydro params\n",
    "            params_dict[\"hydro_params_raw\"] = torch.sigmoid(\n",
    "                params_hydro_model[:, :len(self.hydro_model.parameters_bound) * self.args[\"nmul\"]]).view(\n",
    "                params_hydro_model.shape[0], len(self.hydro_model.parameters_bound),\n",
    "                self.args[\"nmul\"])\n",
    "            # routing params\n",
    "            if self.args[\"routing_hydro_model\"] == True:\n",
    "                params_dict[\"conv_params_hydro\"] = torch.sigmoid(\n",
    "                    params_hydro_model[:, len(self.hydro_model.parameters_bound) * self.args[\"nmul\"]:])\n",
    "            else:\n",
    "                params_dict[\"conv_params_hydro\"] = None\n",
    "\n",
    "        if self.args['temp_model_name'] != \"None\":\n",
    "            # hydro params\n",
    "            params_dict[\"temp_params_raw\"] = torch.sigmoid(\n",
    "                params_temp_model[:, :len(self.temp_model.parameters_bound) * self.args[\"nmul\"]]).view(\n",
    "                params_temp_model.shape[0], len(self.temp_model.parameters_bound),\n",
    "                self.args[\"nmul\"])\n",
    "            # convolution parameters for ss and gw temp calculation\n",
    "            if self.args[\"routing_temp_model\"] == True:\n",
    "                params_dict[\"conv_params_temp\"] = torch.sigmoid(params_temp_model[:, -len(self.temp_model.conv_temp_model_bound):])\n",
    "            else:\n",
    "                print(\"it has not been defined yet what approach should be taken in place of conv\")\n",
    "                exit()\n",
    "        return params_dict\n",
    "\n",
    "\n",
    "    def forward(self, dataset_dictionary_sample):\n",
    "        params_all = self.NN_model(dataset_dictionary_sample[\"inputs_NN_scaled_sample\"][self.args[\"warm_up\"]:, :, :])\n",
    "        # breaking down the parameters to different pieces for different models (PET, hydro, temp)\n",
    "        params_dict = self.breakdown_params(params_all)\n",
    "        if self.args['hydro_model_name'] != \"None\":\n",
    "            # hydro model\n",
    "            flow_out = self.hydro_model(\n",
    "                dataset_dictionary_sample[\"x_hydro_model_sample\"],\n",
    "                dataset_dictionary_sample[\"c_hydro_model_sample\"],\n",
    "                params_dict['hydro_params_raw'],\n",
    "                self.args,\n",
    "                # PET_param=params_dict[\"params_PET_model\"],  # PET is in both temp and flow model\n",
    "                warm_up=self.args[\"warm_up\"],\n",
    "                routing=self.args[\"routing_hydro_model\"],\n",
    "                conv_params_hydro=params_dict[\"conv_params_hydro\"]\n",
    "            )\n",
    "            # baseflow index percentage\n",
    "            flow_out[\"BFI_sim\"] = 100 * (torch.sum(flow_out[\"gwflow\"], dim=0) / (\n",
    "                    torch.sum(flow_out[\"flow_sim\"], dim=0) + 0.00001))[:, 0]\n",
    "\n",
    "            if self.args['temp_model_name'] != \"None\":\n",
    "                # source flow calculation and converting mm/day to m3/ day\n",
    "                source_flows_dict = source_flow_calculation(self.args, flow_out,\n",
    "                                                              dataset_dictionary_sample[\n",
    "                                                                  \"c_NN_sample\"],\n",
    "                                                              after_routing=True)\n",
    "                # temperature model\n",
    "                temp_out = self.temp_model.forward(dataset_dictionary_sample[\"x_temp_model_sample\"],\n",
    "                                                   dataset_dictionary_sample[\"c_temp_model_sample\"],\n",
    "                                                   params_dict[\"temp_params_raw\"],\n",
    "                                                   conv_params_temp=params_dict[\"conv_params_temp\"],\n",
    "                                                   args=self.args,\n",
    "                                                   PET=flow_out[\"PET_hydro\"] * (1 / (1000 * 86400)),   # converting mm/day to m/sec,\n",
    "                                                   source_flows=source_flows_dict)\n",
    "\n",
    "                return {**flow_out, **temp_out}   # combining both dictionaries\n",
    "            else:\n",
    "                return flow_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HydroMultimodel(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for managing a collection of (trained) hydromodels, \n",
    "    and applyinf different ensembling methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, modelList, argList):\n",
    "        super(HydroMultimodel, self).__init__()\n",
    "        \"\"\"\n",
    "        Instantiate hydro models to be ensembled.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ensemble_mtd = None  # Ensemble method \n",
    "        # self.defaultKeys = {}\n",
    "        # self.defaultKeys['models'] = ['HBV', 'dPLHBV', 'dPLHBV_dp', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']\n",
    "        # self.defaultKeys['ensemble_mtds'] = {}  \n",
    "          \n",
    "        self.modelDict = m.createDictFromKeys(modelList)\n",
    "        self.argDict = m.createDictFromKeys(modelList, mtd='ref', dat=argList)\n",
    "        self.initModel()\n",
    "\n",
    "\n",
    "    def initModel(self, mode='all', *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate hydro models to be ensembled.\n",
    "        \"\"\"\n",
    "        if mode == 'all':\n",
    "            for mod in self.modelsDict:\n",
    "                if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n",
    "                    self.modelDict[mod] = PGMLHydroModel(self.argDict[mod])\n",
    "                elif mod in ['dPLHBV']:\n",
    "                    self.modelDict[mod] = rnn.MultiInv_HBVModel(*args, **kwargs)\n",
    "                elif mod in ['dPLHBV_dp']:\n",
    "                    self.modelDict[mod] = rnn.MultiInv_HBVTDModel(*args, **kwargs)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid hydrology model specified.\")     \n",
    "                 \n",
    "        elif mode in self.modelDict:\n",
    "            if mod in ['HBV', 'SACSMA', 'SACSMA_snow', 'marrmot_PRMS']:\n",
    "                self.modelDict[mod] = PGMLHydroModel(self.argDict[mod])\n",
    "            elif mod in ['dPLHBV_dp']:\n",
    "                self.modelDict[mod] = rnn.MultiInv_HBVModel(*args, **kwargs)\n",
    "            else:\n",
    "                self.modelDict[mod] = rnn.MultiInv_HBVTDModel(*args, **kwargs) \n",
    "        else:\n",
    "            raise ValueError(\"Invalid hydrology model specified.\")\n",
    "        \n",
    "\n",
    "    def multimodel_ensemble():\n",
    "        mm_ensemble = F.MultiModelEnsemble()\n",
    "\n",
    "\n",
    "    def fuse_on_avg():\n",
    "        # fuse with average of streamflows here.\n",
    "        x = 1\n",
    "\n",
    "\n",
    "    def fuse_on_md():\n",
    "        # Fuse with median of streamflows here.\n",
    "        x = 1\n",
    "\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        for key in self.modelsDict:\n",
    "            self.modelDict[key](*args, **kwargs)\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhpihydrodl_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
